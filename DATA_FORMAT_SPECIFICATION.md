# OpenPI-RIPT æ•°æ®æ ¼å¼è§„èŒƒ

## ğŸ¯ **å®Œæ•´æ•°æ®æ ¼å¼è¯´æ˜**

### **ç¬¬ä¸€å±‚ï¼šOpenPIæ ‡å‡†æ ¼å¼**

```python
# æ ‡å‡†OpenPIæ ·æœ¬æ ¼å¼ï¼ˆåŸºäºso100_train.pyï¼‰
openpi_sample = {
    "image": {
        "base_0_rgb": torch.Tensor,        # [3, 224, 224] uint8, ä¸»è§†è§’RGBå›¾åƒ
        "left_wrist_0_rgb": torch.Tensor,  # [3, 224, 224] uint8, æ‰‹è…•è§†è§’RGBå›¾åƒ
    },
    "state": torch.Tensor,                 # [state_dim] float32, æœºå™¨äººçŠ¶æ€
    "action": torch.Tensor,                # [50, 7] float32, 50æ­¥åŠ¨ä½œåºåˆ—chunk
    "action_is_pad": torch.Tensor,         # [50] bool, åŠ¨ä½œå¡«å……æ©ç 
    "prompt": str,                         # ä»»åŠ¡æè¿°å­—ç¬¦ä¸²
}
```

**å…³é”®ç‰¹å¾:**
- **Action Chunking**: 50æ­¥æœªæ¥åŠ¨ä½œåºåˆ— (1.67ç§’ @ 30fps)
- **å¤šè§†è§’å›¾åƒ**: ä¸»è§†è§’ + æ‰‹è…•è§†è§’
- **å½’ä¸€åŒ–çŠ¶æ€**: meanstdå½’ä¸€åŒ–çš„æœºå™¨äººçŠ¶æ€
- **ç›¸å¯¹åŠ¨ä½œ**: `action = target_action - current_state`
- **å¡«å……æ©ç **: æ ‡è®°æœ‰æ•ˆåŠ¨ä½œvså¡«å……åŠ¨ä½œ

### **ç¬¬äºŒå±‚ï¼šRIPTæ‰©å±•æ ¼å¼**

```python
# RIPTæ‰©å±•æ ·æœ¬æ ¼å¼
ript_extended_sample = {
    # === OpenPIæ ‡å‡†å­—æ®µ ===
    "image": {
        "base_0_rgb": torch.Tensor,        # [3, 224, 224] uint8
        "left_wrist_0_rgb": torch.Tensor,  # [3, 224, 224] uint8
    },
    "state": torch.Tensor,                 # [target_state_dim] float32, ç»´åº¦å¯¹é½å
    "action": torch.Tensor,                # [50, 7] float32
    "action_is_pad": torch.Tensor,         # [50] bool
    "prompt": str,                         # ä»»åŠ¡æè¿°
    
    # === RIPTæ‰©å±•å­—æ®µ ===  
    "advantages": torch.Tensor,            # [50] float32, RLOOä¼˜åŠ¿å€¼
    "init_hash": str,                      # åˆå§‹çŠ¶æ€å“ˆå¸Œï¼ˆç”¨äºçŠ¶æ€è·³è¿‡ï¼‰
    
    # === å¯é€‰ç»Ÿè®¡å­—æ®µ ===
    "rollout_success": bool,               # Rolloutæ˜¯å¦æˆåŠŸ
    "rollout_reward": float,               # Rolloutæ€»å¥–åŠ±
    "episode_length": int,                 # Episodeé•¿åº¦
}
```

### **ç¬¬ä¸‰å±‚ï¼šè®­ç»ƒæ‰¹æ¬¡æ ¼å¼**

```python
# GPUè®­ç»ƒæ‰¹æ¬¡æ ¼å¼
training_batch = {
    "image": {
        "base_0_rgb": torch.Tensor,        # [batch_size, 3, 224, 224] uint8, device=cuda
        "left_wrist_0_rgb": torch.Tensor,  # [batch_size, 3, 224, 224] uint8, device=cuda  
    },
    "state": torch.Tensor,                 # [batch_size, target_state_dim] float32, device=cuda
    "action": torch.Tensor,                # [batch_size, 50, 7] float32, device=cuda
    "action_is_pad": torch.Tensor,         # [batch_size, 50] bool, device=cuda
    "prompt": List[str],                   # [batch_size] ä»»åŠ¡æè¿°åˆ—è¡¨
    "advantages": torch.Tensor,            # [batch_size] float32, å¤„ç†åçš„ä¼˜åŠ¿å€¼, device=cuda
}
```

### **ç¬¬å››å±‚ï¼šCFGæ¨¡å‹è¾“å…¥æ ¼å¼**

```python
# CFGé€‚é…å™¨æœŸæœ›çš„Episodeæ ¼å¼
episode_format = {
    'observations': [{
        'agentview_image': np.ndarray,           # [224, 224, 3] uint8, HWCæ ¼å¼  
        'robot0_eye_in_hand_image': np.ndarray,  # [224, 224, 3] uint8, HWCæ ¼å¼
        'robot0_eef_pos': np.ndarray,            # [3] float32, æœ«ç«¯ä½ç½®
        'robot0_eef_quat': List[float],          # [4] å››å…ƒæ•° [0,0,0,1]
        'robot0_gripper_qpos': np.ndarray,       # [2] float32, å¤¹çˆªä½ç½®
    }],
    'actions': [np.ndarray],                     # [50, 7] float32, åŠ¨ä½œåºåˆ—
    'task': str,                                 # ä»»åŠ¡æè¿°
    'success': bool,                             # EpisodeæˆåŠŸæ ‡å¿—  
    'total_reward': float,                       # æ€»å¥–åŠ±ï¼ˆæ¥è‡ªä¼˜åŠ¿å€¼ï¼‰
}
```

## ğŸ”„ **æ•°æ®è½¬æ¢æµç¨‹**

### **1. åŸå§‹æ•°æ®åŠ è½½** (`LeRobotDataset`)

```python
# delta_timestamps å®šä¹‰åŠ¨ä½œchunk
delta_timestamps = {
    "observation.images.base": [0],          # å½“å‰æ—¶åˆ»å›¾åƒ
    "observation.images.wrist": [0],         # å½“å‰æ—¶åˆ»å›¾åƒ  
    "observation.state": [0],                # å½“å‰æ—¶åˆ»çŠ¶æ€
    "action": [i / 30 for i in range(50)],  # æœªæ¥50æ­¥åŠ¨ä½œ (1.67ç§’)
}

raw_item = {
    "observation.images.base": tensor([3, 224, 224]),    # 0-1å½’ä¸€åŒ–
    "observation.images.wrist": tensor([3, 224, 224]),   # 0-1å½’ä¸€åŒ–
    "observation.state": tensor([state_dim]),            # åŸå§‹çŠ¶æ€
    "action": tensor([50, action_dim]),                  # ç»å¯¹åŠ¨ä½œ
    "action_is_pad": tensor([50], dtype=bool),           # å¡«å……æ©ç 
    "task": "grab screwdriver",                          # ä»»åŠ¡æè¿°
}
```

### **2. OpenPIæ ‡å‡†åŒ–å¤„ç†**

```python
# å›¾åƒå¤„ç†ï¼š0-1 â†’ 0-255
base_image = (normalized_item["observation.images.base"] * 255).to(torch.uint8)
wrist_image = (normalized_item["observation.images.wrist"] * 255).to(torch.uint8)

# ç›¸å¯¹åŠ¨ä½œè®¡ç®—  
relative_action = item["action"] - item["observation.state"]  # é‡è¦ï¼

# çŠ¶æ€å½’ä¸€åŒ–
normalized_state = normalizer.normalize(item["observation.state"])

# æ ‡å‡†OpenPIæ ¼å¼
openpi_sample = {
    "image": {"base_0_rgb": base_image, "left_wrist_0_rgb": wrist_image},
    "state": normalized_state[0],          # å–ç¬¬ä¸€ä¸ªæ—¶åˆ»
    "action": relative_action,             # ç›¸å¯¹åŠ¨ä½œ
    "action_is_pad": normalized_item["action_is_pad"],
    "prompt": item["task"],
}
```

### **3. RIPTå­—æ®µæ‰©å±•**

```python
# ä¼˜åŠ¿å€¼è®¡ç®—ï¼ˆRLOOï¼‰
rewards = [sample.rollout_reward for sample in samples]
advantages = compute_rloo_advantages(rewards)  # [batch_size] 

# ä¼˜åŠ¿å€¼å¤„ç†ï¼ˆæ•°å€¼ç¨³å®šï¼‰
processed_advantages = advantage_processor.process_advantages(
    advantages,
    normalization="standard",     # (adv - mean) / std
    clipping="symmetric",         # [-clip_value, clip_value] 
    clip_value=3.0,
    negative_handling="softplus"  # log(1 + exp(x))
)

# æ‰©å±•RIPTå­—æ®µ
for i, sample in enumerate(openpi_samples):
    sample["advantages"] = torch.full_like(
        sample["action"][:, 0], 
        processed_advantages[i].item()
    )  # [50] æ¯ä¸ªåŠ¨ä½œéƒ½æœ‰ç›¸åŒä¼˜åŠ¿å€¼
    sample["init_hash"] = compute_hash(initial_state)
```

### **4. æ‰¹æ¬¡åŒ–å’Œè®¾å¤‡è½¬ç§»**

```python
# æ‰¹é‡ç»„è£…
training_batch = {
    "image": {
        "base_0_rgb": torch.stack([s["image"]["base_0_rgb"] for s in samples]).to(device),
        "left_wrist_0_rgb": torch.stack([s["image"]["left_wrist_0_rgb"] for s in samples]).to(device)
    },
    "state": torch.stack([s["state"] for s in samples]).to(device),
    "action": torch.stack([s["action"] for s in samples]).to(device), 
    "action_is_pad": torch.stack([s["action_is_pad"] for s in samples]).to(device),
    "prompt": [s["prompt"] for s in samples],
    "advantages": processed_advantages.to(device)  # [batch_size]
}
```

### **5. CFGåŒåˆ†æ”¯å¤„ç†**

```python
# CFGé€‚é…å™¨ä¸­çš„å®‰å…¨æ‹·è´å’ŒåŒåˆ†æ”¯å¤„ç†
batch_positive = safe_copy_batch(batch, "positive")  # æ·±æ‹·è´
batch_positive["is_positive"] = torch.ones(batch_size, device=device, dtype=torch.long)

batch_uncond = safe_copy_batch(batch, "negative")    # ç‹¬ç«‹æ·±æ‹·è´  
batch_uncond["is_positive"] = torch.zeros(batch_size, device=device, dtype=torch.long)

# è½¬æ¢ä¸ºEpisodeæ ¼å¼
episodes_positive = batch_to_episodes(batch_positive)
episodes_uncond = batch_to_episodes(batch_uncond)

# æ¨¡å‹å‰å‘ä¼ æ’­
out_positive = policy.forward(episodes_positive)  # æ¡ä»¶åˆ†æ”¯
out_uncond = policy.forward(episodes_uncond)      # æ— æ¡ä»¶åˆ†æ”¯

# CFGæŸå¤±è®¡ç®—
cfg_loss = compute_cfg_loss(out_positive, out_uncond, advantages)
```

## ğŸ“Š **æ•°æ®ç»´åº¦æ€»ç»“**

| æ•°æ®ç±»å‹ | ç»´åº¦ | æ•°æ®ç±»å‹ | è®¾å¤‡ | è¯´æ˜ |
|----------|------|----------|------|------|
| **base_0_rgb** | `[B, 3, 224, 224]` | `uint8` | GPU | ä¸»è§†è§’RGBå›¾åƒ |
| **left_wrist_0_rgb** | `[B, 3, 224, 224]` | `uint8` | GPU | æ‰‹è…•è§†è§’RGBå›¾åƒ |  
| **state** | `[B, target_state_dim]` | `float32` | GPU | å½’ä¸€åŒ–æœºå™¨äººçŠ¶æ€ |
| **action** | `[B, 50, 7]` | `float32` | GPU | 50æ­¥ç›¸å¯¹åŠ¨ä½œchunk |
| **action_is_pad** | `[B, 50]` | `bool` | GPU | åŠ¨ä½œå¡«å……æ©ç  |
| **prompt** | `[B]` | `str` | CPU | ä»»åŠ¡æè¿°å­—ç¬¦ä¸² |
| **advantages** | `[B]` | `float32` | GPU | å¤„ç†åçš„ä¼˜åŠ¿å€¼ |

**B = batch_size** (é€šå¸¸æ˜¯4-16)

## ğŸ”§ **å…³é”®è®¾è®¡å†³ç­–**

### **1. Action Chunking (50æ­¥)**
- **åŸå› **: å‡å°‘é‡è§„åˆ’é¢‘ç‡ï¼Œæå‡æ‰§è¡Œç¨³å®šæ€§
- **æ—¶é•¿**: 1.67ç§’ @ 30fps
- **ä¼˜åŠ¿**: å¼€ç¯æ‰§è¡Œï¼Œé¿å…ç´¯ç§¯è¯¯å·®

### **2. ç›¸å¯¹åŠ¨ä½œ**  
```python
action = target_action - current_state  # ç›¸å¯¹åŠ¨ä½œï¼Œæ›´ç¨³å®š
```

### **3. çŠ¶æ€ç»´åº¦é€‚é…**
- **Panda**: 14ç»´ â†’ ä¿æŒä¸å˜
- **UR5e**: 6ç»´ â†’ é›¶å¡«å……åˆ°14ç»´  
- **å…¶ä»–**: ä»»æ„ç»´ â†’ æˆªæ–­/å¡«å……åˆ°ç›®æ ‡ç»´åº¦

### **4. ä¼˜åŠ¿å€¼å¹¿æ’­**
```python
# æ¯ä¸ªæ—¶åˆ»åŠ¨ä½œéƒ½ä½¿ç”¨ç›¸åŒçš„episodeçº§ä¼˜åŠ¿å€¼
sample["advantages"] = torch.full_like(sample["action"][:, 0], advantage_value)  # [50]
```

### **5. CFGå®‰å…¨æ‹·è´**
- **é—®é¢˜**: æµ…æ‹·è´å¯¼è‡´æ­£/è´Ÿåˆ†æ”¯å†…å­˜å…±äº«
- **è§£å†³**: æ·±åº¦æ‹·è´ç¡®ä¿åˆ†æ”¯ç‹¬ç«‹æ€§

## ğŸš¨ **æ³¨æ„äº‹é¡¹**

### **æ•°æ®é¢„å¤„ç†**
1. **å›¾åƒ**: å¿…é¡»æ˜¯`uint8`æ ¼å¼ï¼ŒèŒƒå›´[0,255]
2. **çŠ¶æ€**: å¿…é¡»ç»è¿‡meanstdå½’ä¸€åŒ–
3. **åŠ¨ä½œ**: å¿…é¡»æ˜¯ç›¸å¯¹åŠ¨ä½œï¼Œä¸æ˜¯ç»å¯¹åŠ¨ä½œ
4. **æ©ç **: `action_is_pad`å¿…é¡»æ­£ç¡®æ ‡è®°æœ‰æ•ˆé•¿åº¦

### **å†…å­˜ç®¡ç†**  
1. **å¼ é‡æ‹·è´**: CFGè®­ç»ƒéœ€è¦ç‹¬ç«‹çš„batchæ‹·è´
2. **è®¾å¤‡ä¸€è‡´**: æ‰€æœ‰å¼ é‡å¿…é¡»åœ¨ç›¸åŒè®¾å¤‡ä¸Š
3. **æ•°æ®ç±»å‹**: ä¸¥æ ¼åŒ¹é…æ¨¡å‹æœŸæœ›çš„æ•°æ®ç±»å‹

### **æ•°å€¼ç¨³å®šæ€§**
1. **ä¼˜åŠ¿å€¼**: å¿…é¡»å¤„ç†NaNã€Infã€æç«¯å€¼
2. **æ¢¯åº¦**: éœ€è¦æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
3. **æŸå¤±**: CFGæŸå¤±éœ€è¦æ•°å€¼ç¨³å®šçš„è®¡ç®—

---

*æ­¤æ ¼å¼è§„èŒƒç¡®ä¿äº†OpenPIæ ‡å‡†å…¼å®¹æ€§å’ŒRIPTè®­ç»ƒéœ€æ±‚çš„å®Œç¾ç»“åˆ* ğŸ¯
