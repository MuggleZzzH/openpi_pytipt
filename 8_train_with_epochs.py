#!/usr/bin/env python3
"""
ç¬¬2é˜¶æ®µï¼šæ™ºèƒ½é‡‡æ ·å’Œæ‰¹å¤„ç†è®­ç»ƒ (8_train_with_epochs.py)
åŸºäº7_train_with_rl_core.pyï¼Œæ·»åŠ å®Œæ•´çš„RIPTæ™ºèƒ½é‡‡æ ·å’Œæ¢¯åº¦ç´¯ç§¯é€»è¾‘

æ ¸å¿ƒæ–°å¢åŠŸèƒ½ï¼š
1. åˆå§‹çŠ¶æ€æ•°æ®é›†å’Œæ™ºèƒ½é‡‡æ ·
2. å¤šè½®è½¨è¿¹æ”¶é›†ä¸è¿‡æ»¤ï¼ˆè·³è¿‡å…¨0/å…¨1çŠ¶æ€ï¼‰
3. æ¢¯åº¦ç´¯ç§¯æœºåˆ¶ï¼ˆNä¸ªbatchåæ‰æ›´æ–°ï¼‰
4. å¤šè®­ç»ƒè¿­ä»£å¾ªç¯
5. ç»Ÿè®¡è·Ÿè¸ªå’ŒçŠ¶æ€ç®¡ç†

ä½¿ç”¨æ–¹æ³•:
    cd /zhaohan/ZJH/openpi_pytorch
    python 8_train_with_epochs.py
    
    # è‡ªå®šä¹‰é…ç½®
    python 8_train_with_epochs.py --training_iterations 3 --rollouts_per_batch 4 --gradient_accumulation_steps 4
"""

import os
import sys
import json
import numpy as np
import torch
from pathlib import Path
from datetime import datetime
import argparse
import hashlib
from typing import List, Dict, Any, Tuple

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from pi0 import PI0Policy
from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
from pi0.ript.reward_function import BinarySuccessReward
from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter

class AdvancedTrainingConfig:
    """é«˜çº§è®­ç»ƒé…ç½®ç±»ï¼Œæ”¯æŒå®Œæ•´çš„RIPTåŠŸèƒ½"""
    def __init__(self):
        # æ¨¡å‹é…ç½®
        self.model_path = "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch"
        self.norm_stats_path = "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json"
        
        # ç¯å¢ƒé…ç½®
        self.benchmark_name = "libero_spatial"
        self.task_id = 8
        self.num_parallel_envs = 1
        self.max_episode_length = 200
        
        # ğŸ¯ å®Œæ•´RIPTè®­ç»ƒé…ç½®
        self.training_iterations = 2    # ä¸»è®­ç»ƒå¾ªç¯æ•°
        self.rollouts_per_batch = 2     # æ¯ä¸ªåˆå§‹çŠ¶æ€æ”¶é›†çš„è½¨è¿¹æ•°
        self.target_batches_per_iteration = 2  # æ¯è½®è®­ç»ƒæ”¶é›†çš„æœ‰æ•ˆbatchæ•°
        self.gradient_accumulation_steps = 2   # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        self.num_epochs = 2             # æ¯è½®æ”¶é›†æ•°æ®çš„ä¼˜åŒ–epochæ•°
        
        # å¼ºåŒ–å­¦ä¹ é…ç½®
        self.learning_rate = 1e-5
        self.optimizer_type = "AdamW"
        self.weight_decay = 0.01
        self.max_grad_norm = 1.0
        
        # ğŸ§  æ™ºèƒ½é‡‡æ ·é…ç½®
        self.enable_smart_filtering = True      # å¯ç”¨æ™ºèƒ½è¿‡æ»¤
        self.skip_uniform_results = True        # è·³è¿‡å…¨0æˆ–å…¨1çš„ç»“æœ
        self.max_sampling_attempts = 20         # æœ€å¤§é‡‡æ ·å°è¯•æ¬¡æ•°
        self.success_rate_tolerance = 0.1       # æˆåŠŸç‡å®¹å¿åº¦
        
        # ç»Ÿè®¡å’Œè·Ÿè¸ª
        self.enable_state_tracking = True       # å¯ç”¨çŠ¶æ€è·Ÿè¸ª
        self.stats_save_freq = 2               # ç»Ÿè®¡ä¿å­˜é¢‘ç‡
        
        # ğŸ¯ å¢å¼ºé‡‡æ ·é…ç½® (æ–°å¢)
        self.enable_rollout_stats_tracking = True  # å¯ç”¨rolloutç»Ÿè®¡è·Ÿè¸ª
        self.rollout_skip_threshold = 3            # è·³è¿‡é˜ˆå€¼
        self.rollout_stats_path = f"./rollout_stats_task{self.task_id}.json"  # ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
        self.state_history_window = 20             # çŠ¶æ€å†å²çª—å£å¤§å°
        
        # è°ƒè¯•é…ç½®
        self.save_videos = True
        self.video_dir = "rollout_videos_advanced"
        self.debug_sampling = True
        self.log_training_details = True

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="PI0 RIPT Advanced Training with Smart Sampling")
    parser.add_argument("--training_iterations", type=int, default=2, help="Number of training iterations")
    parser.add_argument("--rollouts_per_batch", type=int, default=2, help="Rollouts per initial state batch")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=2, help="Gradient accumulation steps")
    parser.add_argument("--target_batches", type=int, default=2, help="Target batches per iteration")
    parser.add_argument("--lr", type=float, default=1e-5, help="Learning rate")
    parser.add_argument("--debug_sampling", type=bool, default=True, help="Enable sampling debugging")
    return parser.parse_args()

def compute_state_hash(state_array: np.ndarray) -> str:
    """è®¡ç®—çŠ¶æ€æ•°ç»„çš„å“ˆå¸Œå€¼ç”¨äºè¿½è¸ª"""
    return hashlib.sha256(np.ascontiguousarray(state_array).tobytes()).hexdigest()[:16]

def create_optimizer(policy, config):
    """åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆä¸7è„šæœ¬ä¸€è‡´ï¼‰"""
    print(f"\n=== åˆ›å»ºä¼˜åŒ–å™¨ ===")
    print(f"ä¼˜åŒ–å™¨ç±»å‹: {config.optimizer_type}")
    print(f"å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"æƒé‡è¡°å‡: {config.weight_decay}")
    
    if config.optimizer_type == "AdamW":
        optimizer = torch.optim.AdamW(
            policy.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.999)
        )
    elif config.optimizer_type == "Adam":
        optimizer = torch.optim.Adam(
            policy.parameters(),
            lr=config.learning_rate,
            weight_decay=0.0
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {config.optimizer_type}")
    
    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
    total_params = sum(p.numel() for p in policy.parameters())
    trainable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params:.2%}")
    
    return optimizer

class InitialStateDataset:
    """å¤šæ ·åŒ–çš„åˆå§‹çŠ¶æ€æ•°æ®é›†ï¼Œæ”¯æŒRIPTæ™ºèƒ½é‡‡æ ·"""
    def __init__(self, num_states=50, state_dim=8):
        """åˆ›å»ºå¤šæ ·åŒ–çš„åˆå§‹çŠ¶æ€æ•°æ®é›†"""
        # åˆ›å»ºå¤šæ ·åŒ–çš„åˆå§‹çŠ¶æ€
        self.states = []
        np.random.seed(None)  # ä½¿ç”¨çœŸéšæœºç§å­ä»¥è·å¾—ä¸åŒçš„çŠ¶æ€
        
        for i in range(num_states):
            # ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„åˆå§‹çŠ¶æ€
            base_state = np.zeros(state_dim, dtype=np.float32)
            
            # æ·»åŠ ä¸åŒç¨‹åº¦çš„éšæœºæ‰°åŠ¨
            if i < num_states // 3:
                # å°æ‰°åŠ¨çŠ¶æ€
                noise = np.random.normal(0, 0.05, state_dim).astype(np.float32)
            elif i < 2 * num_states // 3:
                # ä¸­ç­‰æ‰°åŠ¨çŠ¶æ€
                noise = np.random.normal(0, 0.15, state_dim).astype(np.float32)
            else:
                # å¤§æ‰°åŠ¨çŠ¶æ€
                noise = np.random.normal(0, 0.25, state_dim).astype(np.float32)
            
            state = base_state + noise
            self.states.append(state)
        
        print(f"âœ“ åˆ›å»ºäº†åŒ…å« {len(self.states)} ä¸ªå¤šæ ·åŒ–åˆå§‹çŠ¶æ€çš„æ•°æ®é›†")
        print(f"  çŠ¶æ€åˆ†å¸ƒï¼šå°æ‰°åŠ¨({num_states//3}ä¸ª), ä¸­æ‰°åŠ¨({num_states//3}ä¸ª), å¤§æ‰°åŠ¨({num_states - 2*num_states//3}ä¸ª)")
    
    def sample_batch(self, batch_size=1):
        """éšæœºé‡‡æ ·ä¸€æ‰¹ä¸é‡å¤çš„åˆå§‹çŠ¶æ€"""
        if batch_size > len(self.states):
            # å¦‚æœéœ€è¦çš„çŠ¶æ€æ•°è¶…è¿‡å¯ç”¨çŠ¶æ€ï¼Œåˆ™å…è®¸é‡å¤
            indices = np.random.choice(len(self.states), batch_size, replace=True)
        else:
            # ä¸é‡å¤é‡‡æ ·
            indices = np.random.choice(len(self.states), batch_size, replace=False)
        
        sampled_states = np.array([self.states[i] for i in indices])
        return sampled_states

# å¯¼å…¥å¢å¼ºæ™ºèƒ½é‡‡æ ·ç³»ç»Ÿ
from pi0.ript.utils.enhanced_smart_sampling import EnhancedSmartSampler, enhanced_collect_smart_batches

# ä¿ç•™åŸæœ‰çš„SmartSamplerä½œä¸ºå¤‡ç”¨ï¼ˆå·²è¢«EnhancedSmartSampleræ›¿ä»£ï¼‰
class SmartSampler:
    """æ™ºèƒ½é‡‡æ ·å™¨ - RIPTçš„æ ¸å¿ƒåˆ›æ–° (å·²è¢«EnhancedSmartSampleræ›¿ä»£)"""
    def __init__(self, config):
        self.config = config
        self.state_stats = {}  # è®°å½•æ¯ä¸ªçŠ¶æ€çš„ç»Ÿè®¡ä¿¡æ¯
        self.sampling_history = []  # é‡‡æ ·å†å²
        
    def is_batch_useful(self, rollouts: List[Tuple]) -> bool:
        """åˆ¤æ–­ä¸€ä¸ªbatchçš„è½¨è¿¹æ˜¯å¦æœ‰ç”¨ï¼ˆä¸æ˜¯å…¨0æˆ–å…¨1ï¼‰"""
        if not rollouts:
            return False
            
        successes = [r[0] for r in rollouts]  # æå–successæ ‡å¿—
        
        # æ£€æŸ¥æ˜¯å¦å…¨æˆåŠŸæˆ–å…¨å¤±è´¥
        all_success = all(s == True for s in successes)
        all_failure = all(s == False for s in successes)
        
        if all_success:
            if self.config.debug_sampling:
                print(f"    è·³è¿‡ï¼šå…¨éƒ¨æˆåŠŸçš„batch (æˆåŠŸç‡=100%)")
            return False
        elif all_failure:
            if self.config.debug_sampling:
                print(f"    è·³è¿‡ï¼šå…¨éƒ¨å¤±è´¥çš„batch (æˆåŠŸç‡=0%)")
            return False
        else:
            success_rate = sum(successes) / len(successes)
            if self.config.debug_sampling:
                print(f"    âœ“ æœ‰ç”¨çš„batchï¼šæˆåŠŸç‡={success_rate:.2f}")
            return True
    
    def update_state_stats(self, state_hash: str, rollouts: List[Tuple]):
        """æ›´æ–°çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯"""
        if not self.config.enable_state_tracking:
            return
            
        successes = [r[0] for r in rollouts]
        
        if state_hash not in self.state_stats:
            self.state_stats[state_hash] = {
                'attempts': 0,
                'successes': [],
                'last_success_rate': 0.0
            }
        
        self.state_stats[state_hash]['attempts'] += 1
        self.state_stats[state_hash]['successes'].extend(successes)
        
        # åªä¿ç•™æœ€è¿‘çš„è®°å½•
        if len(self.state_stats[state_hash]['successes']) > 20:
            self.state_stats[state_hash]['successes'] = self.state_stats[state_hash]['successes'][-20:]
        
        # è®¡ç®—æœ€æ–°æˆåŠŸç‡
        recent_successes = self.state_stats[state_hash]['successes']
        self.state_stats[state_hash]['last_success_rate'] = sum(recent_successes) / len(recent_successes)

def collect_smart_batches(env_runner, reward_function, init_dataset, sampler, config, iteration_idx):
    """æ™ºèƒ½æ”¶é›†æœ‰ç”¨çš„è½¨è¿¹batches"""
    print(f"\n--- æ™ºèƒ½è½¨è¿¹æ”¶é›† (è¿­ä»£ {iteration_idx + 1}) ---")
    print(f"ç›®æ ‡: æ”¶é›† {config.target_batches_per_iteration} ä¸ªæœ‰ç”¨çš„batches")
    print(f"æ¯ä¸ªbatchåŒ…å« {config.rollouts_per_batch} ä¸ªè½¨è¿¹")
    
    # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼æ”¶é›†è½¨è¿¹
    env_runner.policy.eval()
    
    collected_batches = []
    total_attempts = 0
    
    while len(collected_batches) < config.target_batches_per_iteration:
        if total_attempts >= config.max_sampling_attempts:
            print(f"âš ï¸ è¾¾åˆ°æœ€å¤§é‡‡æ ·æ¬¡æ•° {config.max_sampling_attempts}ï¼Œåœæ­¢æ”¶é›†")
            break
            
        total_attempts += 1
        
        # 1. éšæœºé‡‡æ ·ä¸åŒçš„åˆå§‹çŠ¶æ€
        init_states = init_dataset.sample_batch(config.num_parallel_envs)
        state_hash = compute_state_hash(init_states)
        
        if config.debug_sampling:
            print(f"\nå°è¯• {total_attempts}: ä½¿ç”¨åˆå§‹çŠ¶æ€ {state_hash}")
            print(f"    çŠ¶æ€å€¼: {init_states[0][:4]}")  # æ˜¾ç¤ºå‰4ä¸ªå€¼
        
        # 2. åœ¨è¯¥åˆå§‹çŠ¶æ€ä¸‹è¿è¡Œå¤šä¸ªè½¨è¿¹
        try:
            state_rollouts = []
            for rollout_idx in range(config.rollouts_per_batch):
                if config.debug_sampling:
                    print(f"    è½¨è¿¹ {rollout_idx + 1}/{config.rollouts_per_batch} ä»çŠ¶æ€ {state_hash[:8]}...")
                
                # ä¸ºæ¯ä¸ªè½¨è¿¹å•ç‹¬è°ƒç”¨ç¯å¢ƒ
                rollout_generator = env_runner.run_policy_in_env(
                    env_name=config.task_id,
                    all_init_states=init_states
                )
                
                try:
                    rollout_batch = list(rollout_generator)
                    if rollout_batch:
                        state_rollouts.extend(rollout_batch)
                    else:
                        if config.debug_sampling:
                            print(f"      è½¨è¿¹ {rollout_idx + 1} å¤±è´¥ï¼šæ— æ•°æ®")
                        break
                except StopIteration:
                    if config.debug_sampling:
                        print(f"      è½¨è¿¹ {rollout_idx + 1} å®Œæˆ")
                    break
            
            if not state_rollouts:
                if config.debug_sampling:
                    print(f"    æ— æ•ˆï¼šæ²¡æœ‰æ”¶é›†åˆ°è½¨è¿¹")
                continue
                
        except Exception as e:
            if config.debug_sampling:
                print(f"    é”™è¯¯ï¼šè½¨è¿¹æ”¶é›†å¤±è´¥ - {e}")
            continue
        
        # 3. æ™ºèƒ½è¿‡æ»¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç”¨
        if config.enable_smart_filtering:
            if not sampler.is_batch_useful(state_rollouts):
                sampler.update_state_stats(state_hash, state_rollouts)
                continue  # è·³è¿‡è¿™ä¸ªbatch
        
        # 4. è½¬æ¢ä¸ºepisodeæ ¼å¼å¹¶è®¡ç®—å¥–åŠ±
        episodes = []
        for success, total_reward, episode_data in state_rollouts:
            episode = {
                'success': success,
                'total_reward': total_reward,
                **episode_data
            }
            
            # ä½¿ç”¨å¥–åŠ±å‡½æ•°è®¡ç®—å¥–åŠ±
            try:
                computed_reward = reward_function.compute_reward(len(episodes), episode, None)
                episode['computed_reward'] = computed_reward
            except Exception as e:
                episode['computed_reward'] = 0.0
            
            episodes.append(episode)
        
        # 5. æ”¶é›†æœ‰ç”¨çš„batch
        collected_batches.append(episodes)
        sampler.update_state_stats(state_hash, state_rollouts)
        
        if config.debug_sampling:
            success_count = sum(ep['success'] for ep in episodes)
            success_rate = success_count/len(episodes)
            print(f"    âœ“ æ”¶é›†batch {len(collected_batches)}: {len(episodes)} ä¸ªepisodes, "
                  f"æˆåŠŸç‡={success_rate:.2f} ({'æœ‰ç”¨' if 0 < success_rate < 1 else 'å‡åŒ€'})")
    
    print(f"\nâœ“ æ™ºèƒ½é‡‡æ ·å®Œæˆ:")
    print(f"  æ€»å°è¯•æ¬¡æ•°: {total_attempts}")
    print(f"  æ”¶é›†æœ‰ç”¨batches: {len(collected_batches)}")
    print(f"  æ€»episodes: {sum(len(batch) for batch in collected_batches)}")
    print(f"  é‡‡æ ·æ•ˆç‡: {len(collected_batches)/total_attempts:.2%}")
    
    return collected_batches

def train_with_gradient_accumulation(policy, optimizer, cfg_adapter, all_episodes, advantages, config):
    """ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯è¿›è¡Œè®­ç»ƒ"""
    print(f"\n--- æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ ---")
    print(f"æ€»episodes: {len(all_episodes)}")
    print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.gradient_accumulation_steps}")
    print(f"è®­ç»ƒepochs: {config.num_epochs}")
    
    # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
    policy.train()
    
    total_steps = 0
    all_losses = []
    
    for epoch in range(config.num_epochs):
        print(f"\n=== Epoch {epoch + 1}/{config.num_epochs} ===")
        
        # åˆ›å»ºepisodeæ‰¹æ¬¡
        episode_batches = []
        batch_size = max(1, len(all_episodes) // config.gradient_accumulation_steps)
        
        for i in range(0, len(all_episodes), batch_size):
            batch_episodes = all_episodes[i:i + batch_size]
            batch_advantages = advantages[i:i + len(batch_episodes)]
            episode_batches.append((batch_episodes, batch_advantages))
        
        print(f"åˆ›å»ºäº† {len(episode_batches)} ä¸ªæ‰¹æ¬¡ï¼Œå¹³å‡æ¯æ‰¹ {batch_size} episodes")
        
        # æ¢¯åº¦ç´¯ç§¯å¾ªç¯
        optimizer.zero_grad()
        epoch_losses = []
        
        for step, (batch_episodes, batch_advantages) in enumerate(episode_batches):
            try:
                # è®¡ç®—batchæŸå¤±
                batch_loss = cfg_adapter.compute_weighted_loss(batch_episodes, batch_advantages)
                
                # å½’ä¸€åŒ–æŸå¤±ï¼ˆæ¢¯åº¦ç´¯ç§¯çš„å…³é”®ï¼‰
                normalized_loss = batch_loss / config.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­ï¼ˆç´¯ç§¯æ¢¯åº¦ï¼‰
                normalized_loss.backward()
                
                batch_loss_value = batch_loss.item()
                epoch_losses.append(batch_loss_value)
                
                if config.log_training_details:
                    print(f"  Step {step + 1}: batch_loss={batch_loss_value:.6f}, "
                          f"normalized_loss={normalized_loss.item():.6f}")
                
                # æ¯Næ­¥æˆ–æœ€åä¸€æ­¥è¿›è¡Œå‚æ•°æ›´æ–°
                if (step + 1) % config.gradient_accumulation_steps == 0 or (step + 1) == len(episode_batches):
                    # æ¢¯åº¦è£å‰ª
                    if config.max_grad_norm > 0:
                        grad_norm = torch.nn.utils.clip_grad_norm_(policy.parameters(), config.max_grad_norm)
                        if config.log_training_details:
                            print(f"    æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
                    
                    # å‚æ•°æ›´æ–°
                    optimizer.step()
                    optimizer.zero_grad()
                    total_steps += 1
                    
                    if config.log_training_details:
                        print(f"    âœ“ å‚æ•°æ›´æ–° (æ€»æ­¥æ•°: {total_steps})")
                        
            except Exception as e:
                print(f"    âŒ Step {step + 1} è®­ç»ƒå‡ºé”™: {e}")
                continue
        
        # Epochç»Ÿè®¡
        if epoch_losses:
            epoch_mean_loss = np.mean(epoch_losses)
            all_losses.extend(epoch_losses)
            print(f"âœ“ Epoch {epoch + 1} å¹³å‡æŸå¤±: {epoch_mean_loss:.6f}")
        else:
            print(f"âš ï¸ Epoch {epoch + 1} æ²¡æœ‰æœ‰æ•ˆæŸå¤±")
    
    return {
        'total_steps': total_steps,
        'total_losses': all_losses,
        'mean_loss': np.mean(all_losses) if all_losses else 0.0
    }

def main():
    print("=== ç¬¬2é˜¶æ®µï¼šæ™ºèƒ½é‡‡æ ·å’Œæ‰¹å¤„ç†è®­ç»ƒ ===")
    print("å®Œæ•´çš„RIPTæ™ºèƒ½é‡‡æ ·ã€æ¢¯åº¦ç´¯ç§¯å’Œå¤šè½®è®­ç»ƒ")
    print()
    
    # è§£æå‚æ•°å’Œé…ç½®
    args = parse_args()
    config = AdvancedTrainingConfig()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    config.training_iterations = args.training_iterations
    config.rollouts_per_batch = args.rollouts_per_batch
    config.gradient_accumulation_steps = args.gradient_accumulation_steps
    config.target_batches_per_iteration = args.target_batches
    config.learning_rate = args.lr
    config.debug_sampling = args.debug_sampling
    
    print(f"ğŸ¯ é«˜çº§è®­ç»ƒé…ç½®:")
    print(f"  è®­ç»ƒè¿­ä»£æ•°: {config.training_iterations}")
    print(f"  æ¯batchè½¨è¿¹æ•°: {config.rollouts_per_batch}")
    print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.gradient_accumulation_steps}")
    print(f"  ç›®æ ‡batchæ•°: {config.target_batches_per_iteration}")
    print(f"  å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  æ™ºèƒ½é‡‡æ ·: {config.enable_smart_filtering}")
    print(f"  ä»»åŠ¡ID: {config.task_id}")
    print("")
    
    # åŠ è½½PI0æ¨¡å‹
    print(f"åŠ è½½PI0æ¨¡å‹: {config.model_path}")
    policy = PI0Policy.from_pretrained(config.model_path)
    device = policy.config.device
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(policy, config)
    
    # åˆ›å»ºCFGé€‚é…å™¨
    print(f"\n=== åˆ›å»ºCFGé€‚é…å™¨ ===")
    os.environ["PI0_DEBUG_SAVE_VIDEO"] = "false"  # é¿å…é‡å¤è§†é¢‘
    cfg_adapter = PI0_CFG_Adapter(policy, norm_stats_path=config.norm_stats_path)
    print("âœ“ CFGé€‚é…å™¨åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºç¯å¢ƒè¿è¡Œå™¨
    print("\n=== åˆå§‹åŒ–ç¯å¢ƒè¿è¡Œå™¨ ===")
    env_runner = LIBEROEnvRunner(
        policy=policy,
        benchmark_name=config.benchmark_name,
        rollouts_per_env=1,
        num_parallel_envs=config.num_parallel_envs,
        max_episode_length=config.max_episode_length,
        task_names_to_use=[config.task_id],
        config=config.__dict__,
        rank=0,
        world_size=1,
        norm_stats_path=config.norm_stats_path
    )
    
    # åˆ›å»ºç»„ä»¶
    reward_function = BinarySuccessReward()
    init_dataset = InitialStateDataset()
    # ä½¿ç”¨å¢å¼ºç‰ˆæ™ºèƒ½é‡‡æ ·å™¨
    smart_sampler = EnhancedSmartSampler(config)
    
    # åˆ›å»ºè§†é¢‘ç›®å½•
    video_dir = Path(config.video_dir)
    video_dir.mkdir(exist_ok=True)
    
    # ğŸš€ ä¸»è®­ç»ƒå¾ªç¯å¼€å§‹
    print(f"\n=== å¼€å§‹æ™ºèƒ½é‡‡æ ·è®­ç»ƒå¾ªç¯ ===")
    print(f"å°†è¿›è¡Œ {config.training_iterations} è½®å®Œæ•´çš„è®­ç»ƒè¿­ä»£")
    print("ğŸ” ä½¿ç”¨RIPTæ™ºèƒ½é‡‡æ ·ï¼šè‡ªåŠ¨é€‰æ‹©ä¸åŒåˆå§‹çŠ¶æ€ï¼Œè¿‡æ»¤å…¨0/å…¨1ç»“æœ")
    
    all_training_metrics = []
    
    for iteration in range(config.training_iterations):
        print(f"\n" + "="*60)
        print(f"ğŸ”„ è®­ç»ƒè¿­ä»£ {iteration + 1}/{config.training_iterations}")
        print("="*60)
        
        # ç¬¬1æ­¥ï¼šå¢å¼ºæ™ºèƒ½é‡‡æ ·æ”¶é›†æ•°æ®
        print(f"ğŸ¯ å¼€å§‹å¢å¼ºæ™ºèƒ½é‡‡æ ·æ•°æ®æ”¶é›†...")
        collected_batches = enhanced_collect_smart_batches(
            env_runner, reward_function, init_dataset, 
            smart_sampler, config, iteration
        )
        
        if not collected_batches:
            print("âš ï¸ æ™ºèƒ½é‡‡æ ·æœªæ”¶é›†åˆ°æœ‰ç”¨çš„æ•°æ®ï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒ")
            continue
        
        # ç¬¬2æ­¥ï¼šåˆå¹¶æ‰€æœ‰episodes
        all_episodes = []
        for batch in collected_batches:
            all_episodes.extend(batch)
        
        if not all_episodes:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„episodesï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒ")
            continue
        
        # ç¬¬3æ­¥ï¼šè®¡ç®—ä¼˜åŠ¿å€¼
        print(f"\n--- ä¼˜åŠ¿è®¡ç®— ---")
        all_rewards = [ep['total_reward'] for ep in all_episodes]
        rewards_tensor = torch.tensor(all_rewards, dtype=torch.float32)
        
        # Leave-One-Outä¼˜åŠ¿è®¡ç®—
        advantages = []
        for i in range(len(all_rewards)):
            other_rewards = torch.cat([rewards_tensor[:i], rewards_tensor[i+1:]])
            baseline = other_rewards.mean() if len(other_rewards) > 0 else 0.0
            advantage = rewards_tensor[i] - baseline
            advantages.append(advantage.item())
        
        advantages = torch.tensor(advantages, dtype=torch.float32, device=device)
        
        print(f"Episodesæ€»æ•°: {len(all_episodes)}")
        print(f"å¹³å‡å¥–åŠ±: {rewards_tensor.mean().item():.4f}")
        print(f"æˆåŠŸç‡: {sum(ep['success'] for ep in all_episodes) / len(all_episodes):.2f}")
        print(f"ä¼˜åŠ¿æ–¹å·®: {advantages.var().item():.6f}")
        
        # ç¬¬4æ­¥ï¼šæ¢¯åº¦ç´¯ç§¯è®­ç»ƒ
        training_metrics = train_with_gradient_accumulation(
            policy, optimizer, cfg_adapter, all_episodes, advantages, config
        )
        
        # ç¬¬5æ­¥ï¼šè®°å½•è¿­ä»£æŒ‡æ ‡
        iteration_metrics = {
            'iteration': iteration + 1,
            'collected_batches': len(collected_batches),
            'total_episodes': len(all_episodes),
            'mean_reward': rewards_tensor.mean().item(),
            'success_rate': sum(ep['success'] for ep in all_episodes) / len(all_episodes),
            'advantage_variance': advantages.var().item(),
            **training_metrics
        }
        
        all_training_metrics.append(iteration_metrics)
        
        print(f"\nâœ“ è¿­ä»£ {iteration + 1} å®Œæˆ:")
        print(f"  æ”¶é›†batches: {iteration_metrics['collected_batches']}")
        print(f"  æ€»episodes: {iteration_metrics['total_episodes']}")
        print(f"  å¹³å‡å¥–åŠ±: {iteration_metrics['mean_reward']:.4f}")
        print(f"  æˆåŠŸç‡: {iteration_metrics['success_rate']:.2f}")
        print(f"  å¹³å‡æŸå¤±: {iteration_metrics['mean_loss']:.6f}")
        print(f"  è®­ç»ƒæ­¥æ•°: {iteration_metrics['total_steps']}")
    
    # è®­ç»ƒæ€»ç»“
    print(f"\n" + "="*60)
    print("ğŸ‰ å®Œæ•´è®­ç»ƒå¾ªç¯ç»“æŸï¼")
    print("="*60)
    
    print("\nâœ… æˆåŠŸå®ç°çš„RIPTæ ¸å¿ƒåŠŸèƒ½:")
    print("1. âœ“ æ™ºèƒ½åˆå§‹çŠ¶æ€é‡‡æ ·")
    print("2. âœ“ å…¨0/å…¨1è¿‡æ»¤æœºåˆ¶")
    print("3. âœ“ å¤šè½®è½¨è¿¹æ”¶é›†") 
    print("4. âœ“ æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ")
    print("5. âœ“ å¤šè®­ç»ƒè¿­ä»£å¾ªç¯")
    print("6. âœ“ ç»Ÿè®¡è·Ÿè¸ªå’Œç®¡ç†")
    
    if all_training_metrics:
        print(f"\nğŸ“Š è®­ç»ƒè½¨è¿¹æ€»ç»“:")
        print("Iter | Batches | Episodes | Reward | Success | Loss    | Steps")
        print("-----|---------|----------|--------|---------|---------|------")
        for m in all_training_metrics:
            print(f"{m['iteration']:4d} | {m['collected_batches']:7d} | "
                  f"{m['total_episodes']:8d} | {m['mean_reward']:6.3f} | "
                  f"{m['success_rate']:7.2f} | {m['mean_loss']:7.4f} | {m['total_steps']:5d}")
        
        # è®­ç»ƒè¶‹åŠ¿åˆ†æ
        first_loss = all_training_metrics[0]['mean_loss']
        last_loss = all_training_metrics[-1]['mean_loss']
        if first_loss > 0:
            loss_change = ((last_loss - first_loss) / first_loss) * 100
            print(f"\nğŸ“ˆ è®­ç»ƒè¶‹åŠ¿:")
            print(f"  åˆå§‹æŸå¤±: {first_loss:.6f}")
            print(f"  æœ€ç»ˆæŸå¤±: {last_loss:.6f}")
            print(f"  æŸå¤±å˜åŒ–: {loss_change:+.2f}%")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    results_file = f"advanced_training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w') as f:
        json.dump({
            'config': config.__dict__,
            'training_metrics': all_training_metrics,
            'sampler_stats': smart_sampler.state_stats,
            'final_summary': {
                'total_iterations': len(all_training_metrics),
                'total_episodes': sum(m['total_episodes'] for m in all_training_metrics),
                'total_training_steps': sum(m['total_steps'] for m in all_training_metrics),
                'final_success_rate': all_training_metrics[-1]['success_rate'] if all_training_metrics else 0.0
            }
        }, f, indent=2)
    
    print(f"\nğŸ’¾ å®Œæ•´è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print("\nğŸ¯ ç¬¬2é˜¶æ®µå®Œæˆï¼ç°åœ¨å…·å¤‡äº†å®Œæ•´çš„RIPTæ™ºèƒ½é‡‡æ ·å’Œæ‰¹å¤„ç†èƒ½åŠ›")

if __name__ == "__main__":
    main()