#!/usr/bin/env python3
"""
ç¬¬10é˜¶æ®µï¼šåˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿå‡çº§ (10_train_with_distributed.py)
åŸºäº9_train_with_config.pyï¼Œæ·»åŠ å®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

æ ¸å¿ƒå‡çº§åŠŸèƒ½ï¼š
1. æ ‡å‡†PyTorchåˆ†å¸ƒå¼è®­ç»ƒ (DDP)
2. ä»»åŠ¡åˆ†ç‰‡å’Œè´Ÿè½½å‡è¡¡
3. åˆ†å¸ƒå¼æ•°æ®é‡‡æ ·
4. æ¢¯åº¦åŒæ­¥å’Œèšåˆ
5. åˆ†å¸ƒå¼ç»Ÿè®¡æ•°æ®åŒæ­¥
6. å¤šGPUç¯å¢ƒä¼˜åŒ–

ä½¿ç”¨æ–¹æ³•:
    # å•æœºå¤šGPUè®­ç»ƒ
    cd /zhaohan/ZJH/openpi_pytorch
    CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun \
        --nproc_per_node=4 \
        10_train_with_distributed.py \
        --config_path pi0/ript/config/distributed_train_pi0.yaml
    
    # å¤šæœºè®­ç»ƒ
    # Node 0:
    torchrun --nnodes=2 --nproc_per_node=4 --node_rank=0 \
        --master_addr=MASTER_IP --master_port=12355 \
        10_train_with_distributed.py --config_path ...
    
    # Node 1:
    torchrun --nnodes=2 --nproc_per_node=4 --node_rank=1 \
        --master_addr=MASTER_IP --master_port=12355 \
        10_train_with_distributed.py --config_path ...
"""

import os
import sys
import json
import numpy as np
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from pathlib import Path
from datetime import datetime, timedelta
import argparse
import hashlib
from typing import List, Dict, Any, Tuple, Optional, Union
import yaml
import time
import gc

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥OmegaConfç”¨äºé…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ ä½¿ç”¨OmegaConfè¿›è¡Œé…ç½®ç®¡ç†")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfæœªå®‰è£…ï¼Œå›é€€åˆ°åŸºç¡€YAMLè§£æ")

from pi0 import PI0Policy
from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
from pi0.ript.reward_function import BinarySuccessReward
from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter

# å¯¼å…¥å¢å¼ºæ™ºèƒ½é‡‡æ ·ç³»ç»Ÿ
try:
    from pi0.ript.utils.enhanced_smart_sampling import EnhancedSmartSampler, enhanced_collect_smart_batches
    ENHANCED_SAMPLING_AVAILABLE = True
    print("âœ“ å¢å¼ºæ™ºèƒ½é‡‡æ ·ç³»ç»Ÿå·²å¯¼å…¥")
except ImportError:
    ENHANCED_SAMPLING_AVAILABLE = False
    print("âš ï¸ å¢å¼ºæ™ºèƒ½é‡‡æ ·ç³»ç»Ÿæœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨åŸºç¡€é‡‡æ ·")
    
    # åŸºç¡€é‡‡æ ·å®ç°ä½œä¸ºåå¤‡
    class EnhancedSmartSampler:
        def __init__(self, config):
            self.config = config
        
        def smart_sample_init_state(self, init_dataset):
            # ğŸ”¥ ä½¿ç”¨ç¯å½¢ç´¢å¼•è€Œééšæœºé‡‡æ ·
            if hasattr(init_dataset, 'get_states_for_envs'):
                # æ–°çš„ç¯å½¢ç´¢å¼•æ–¹æ³•
                init_states = init_dataset.get_states_for_envs(
                    self.config.num_parallel_envs, 
                    self.config.rloo_batch_size
                )
            else:
                # åå¤‡ï¼šä½¿ç”¨åŸæœ‰éšæœºé‡‡æ ·
                init_states = init_dataset.sample_batch(self.config.num_parallel_envs)
            
            state_hash = self.compute_state_hash(init_states)
            return init_states, state_hash
        
        def compute_state_hash(self, state_array):
            import hashlib
            return hashlib.sha256(np.ascontiguousarray(state_array).tobytes()).hexdigest()[:16]
    
    def enhanced_collect_smart_batches(env_runner, reward_function, init_dataset, sampler, config, iteration_idx):
        # ç®€åŒ–çš„é‡‡æ ·å®ç°
        print(f"ä½¿ç”¨åŸºç¡€é‡‡æ ·æ¨¡å¼ (è¿­ä»£ {iteration_idx + 1})")
        collected_batches = []
        
        for i in range(config.target_batches_per_iteration):
            init_states, _ = sampler.smart_sample_init_state(init_dataset)
            
            try:
                rollout_generator = env_runner.run_policy_in_env(
                    env_name=config.task_id,
                    all_init_states=init_states
                )
                
                rollout_batch = list(rollout_generator)
                if rollout_batch:
                    episodes = []
                    for success, total_reward, episode_data in rollout_batch:
                        episode = {
                            'success': success,
                            'total_reward': total_reward,
                            **episode_data
                        }
                        episodes.append(episode)
                    
                    if episodes:
                        collected_batches.append(episodes)
            except Exception as e:
                print(f"é‡‡æ ·å¤±è´¥: {e}")
                continue
        
        return collected_batches

# åˆ†å¸ƒå¼å·¥å…·å‡½æ•°
def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    # å¦‚æœé»˜è®¤è¿›ç¨‹ç»„å·²ç»åˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›å½“å‰ä¿¡æ¯ï¼Œé¿å…é‡å¤åˆå§‹åŒ–é”™è¯¯
    if dist.is_available() and dist.is_initialized():
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        # å¦‚æœ LOCAL_RANK ç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•ä»å½“å‰è®¾å¤‡è·å–
        local_rank = int(os.environ.get('LOCAL_RANK', rank))
        device = torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')
        return True, rank, world_size, local_rank, device

    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
        
        # torchrun å¯èƒ½å·²ç»åˆå§‹åŒ–äº†è¿›ç¨‹ç»„ï¼Œå†æ¬¡æ£€æŸ¥
        if not dist.is_initialized():
            # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢æ­»é”
            timeout = timedelta(seconds=10800)  # 3å°æ—¶
            
            # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
            dist.init_process_group(
                backend='nccl',
                timeout=timeout,
                rank=rank,
                world_size=world_size
            )
        
        # è®¾ç½®å½“å‰è¿›ç¨‹çš„GPU
        torch.cuda.set_device(local_rank)
        device = torch.device(f'cuda:{local_rank}')
        
        return True, rank, world_size, local_rank, device
    else:
        return False, 0, 1, 0, torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()

def sync_across_processes(tensor, device):
    """åœ¨æ‰€æœ‰è¿›ç¨‹é—´åŒæ­¥å¼ é‡æ•°æ®"""
    if dist.is_initialized():
        tensor = tensor.to(device)
        dist.all_reduce(tensor, op=dist.ReduceOp.AVG)
    return tensor

def gather_object_across_processes(obj):
    """åœ¨æ‰€æœ‰è¿›ç¨‹é—´æ”¶é›†å¯¹è±¡æ•°æ®"""
    if dist.is_initialized():
        world_size = dist.get_world_size()
        gathered_objects = [None] * world_size
        dist.all_gather_object(gathered_objects, obj)
        return gathered_objects
    else:
        return [obj]

def load_config_from_yaml(config_path: str, overrides: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®ï¼Œæ”¯æŒOmegaConfå’ŒåŸºç¡€YAML
    
    Args:
        config_path: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
        overrides: é…ç½®è¦†ç›–åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ ["training.lr=1e-5", "algo.batch_size=32"]
        
    Returns:
        é…ç½®å­—å…¸
    """
    config_path = Path(config_path)
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    print(f"ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    if OMEGACONF_AVAILABLE:
        # ä½¿ç”¨OmegaConfåŠ è½½é…ç½®
        config = OmegaConf.load(config_path)
        
        # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
        if overrides:
            print(f"ğŸ”§ åº”ç”¨é…ç½®è¦†ç›–:")
            for override in overrides:
                print(f"  {override}")
                key, value = override.split('=', 1)
                # å°è¯•è½¬æ¢ä¸ºæ­£ç¡®çš„ç±»å‹
                try:
                    # å°è¯•æ•°å­—è½¬æ¢
                    if '.' in value and value.replace('.', '').replace('-', '').isdigit():
                        value = float(value)
                    elif value.replace('-', '').isdigit():
                        value = int(value)
                    elif value.lower() in ['true', 'false']:
                        value = value.lower() == 'true'
                except:
                    pass  # ä¿æŒå­—ç¬¦ä¸²ç±»å‹
                
                # ä½¿ç”¨OmegaConfçš„æ­£ç¡®æ–¹æ³•è®¾ç½®åµŒå¥—é”®
                keys = key.split('.')
                target = config
                for k in keys[:-1]:
                    if k not in target:
                        target[k] = {}
                    target = target[k]
                target[keys[-1]] = value
        
        # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
        config_dict = OmegaConf.to_container(config, resolve=True)
        
    else:
        # ä½¿ç”¨åŸºç¡€YAMLåŠ è½½
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        
        # åº”ç”¨ç®€å•çš„é…ç½®è¦†ç›–
        if overrides:
            print(f"ğŸ”§ åº”ç”¨é…ç½®è¦†ç›– (åŸºç¡€æ¨¡å¼):")
            for override in overrides:
                print(f"  {override}")
                key, value = override.split('=', 1)
                # ç®€å•çš„åµŒå¥—é”®å¤„ç†
                keys = key.split('.')
                target = config_dict
                for k in keys[:-1]:
                    if k not in target:
                        target[k] = {}
                    target = target[k]
                
                # ç±»å‹è½¬æ¢
                try:
                    if '.' in value and value.replace('.', '').replace('-', '').isdigit():
                        value = float(value)
                    elif value.replace('-', '').isdigit():
                        value = int(value)
                    elif value.lower() in ['true', 'false']:
                        value = value.lower() == 'true'
                except:
                    pass
                
                target[keys[-1]] = value
    
    print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    return config_dict

def validate_distributed_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    éªŒè¯å’Œè¡¥å……åˆ†å¸ƒå¼é…ç½®å‚æ•°
    
    Args:
        config: åŸå§‹é…ç½®å­—å…¸
        
    Returns:
        éªŒè¯åçš„é…ç½®å­—å…¸
    """
    print("ğŸ” éªŒè¯åˆ†å¸ƒå¼é…ç½®å‚æ•°...")
    
    # è®¾ç½®åˆ†å¸ƒå¼ç›¸å…³çš„é»˜è®¤å€¼
    distributed_defaults = {
        # åˆ†å¸ƒå¼é…ç½®
        'distributed': {
            'enabled': True,
            'backend': 'nccl',
            'timeout_seconds': 10800,
            'find_unused_parameters': False,
            'bucket_cap_mb': 25,
        },
        
        # æ•°æ®å¹¶è¡Œé…ç½®
        'data_parallel': {
            'sync_every_n_steps': 10,
            'enable_gradient_checkpointing': False,
            'async_data_loading': True,
        },
        
        # ä»»åŠ¡åˆ†ç‰‡é…ç½®
        'task_distribution': {
            'enable_task_sharding': True,
            'balance_tasks_across_gpus': True,
            'min_tasks_per_gpu': 1,
        }
    }
    
    # é€’å½’åˆå¹¶é»˜è®¤å€¼
    def merge_defaults(target: Dict, source: Dict) -> Dict:
        for key, value in source.items():
            if key not in target:
                target[key] = value
            elif isinstance(value, dict) and isinstance(target[key], dict):
                target[key] = merge_defaults(target[key], value)
        return target
    
    config = merge_defaults(config, distributed_defaults)
    
    print("âœ“ åˆ†å¸ƒå¼é…ç½®éªŒè¯é€šè¿‡")
    return config

class DistributedTrainingRunner:
    """åˆ†å¸ƒå¼è®­ç»ƒè¿è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        self.is_distributed, self.rank, self.world_size, self.local_rank, self.device = setup_distributed()
        
        if self.rank == 0:
            print(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–:")
            print(f"  World Size: {self.world_size}")
            print(f"  Backend: {config.get('distributed', {}).get('backend', 'nccl')}")
            print(f"  Device: {self.device}")
        
        # è®¾ç½®éšæœºç§å­ï¼ˆç¡®ä¿ä¸åŒè¿›ç¨‹ä½¿ç”¨ä¸åŒç§å­ï¼‰
        seed = config.get('training', {}).get('seed', 42)
        torch.manual_seed(seed + self.rank)
        np.random.seed(seed + self.rank)
        
        self.setup_directories()
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„è¾“å‡ºç›®å½•"""
        output_dir = Path(self.config['output_dir'])
        
        # åˆ›å»ºå®éªŒç‰¹å®šçš„è¾“å‡ºç›®å½•
        exp_name = self.config.get('exp_name', 'pi0_distributed_experiment')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.exp_output_dir = output_dir / f"{exp_name}_{timestamp}_rank{self.rank}"
        
        # åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºç›®å½•
        if self.rank == 0:
            output_dir.mkdir(parents=True, exist_ok=True)
            self.exp_output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if self.is_distributed:
            dist.barrier()
        
        # å…¶ä»–è¿›ç¨‹ä¹Ÿåˆ›å»ºè‡ªå·±çš„ç›®å½•
        if self.rank != 0:
            self.exp_output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºè§†é¢‘ç›®å½•
        video_dir = self.config.get('enhanced_sampling', {}).get('video_dir', f'rollout_videos_distributed_rank{self.rank}')
        self.video_dir = Path(video_dir)
        self.video_dir.mkdir(exist_ok=True)
        
        if self.rank == 0:
            print(f"ğŸ“ å®éªŒè¾“å‡ºç›®å½•: {self.exp_output_dir}")
            print(f"ğŸ“ è§†é¢‘ä¿å­˜ç›®å½•: {self.video_dir}")
    
    def get_distributed_tasks(self):
        """æ ¹æ®ä»»åŠ¡é…ç½®å’Œè¿›ç¨‹æ•°åˆ†é…ä»»åŠ¡"""
        all_tasks = self.config.get('task', {}).get('task_names_to_use', None)
        
        if all_tasks is None:
            # ä½¿ç”¨å•ä¸ªä»»åŠ¡ID
            task_id = self.config.get('task', {}).get('task_name', 8)
            all_tasks = [task_id]
        
        # ä»»åŠ¡åˆ†ç‰‡ï¼šå°†ä»»åŠ¡åˆ†é…ç»™ä¸åŒçš„GPU
        if self.config.get('task_distribution', {}).get('enable_task_sharding', True):
            rank_to_tasks = {rank_i: [] for rank_i in range(self.world_size)}
            for task_i, task_name in enumerate(all_tasks):
                rank_to_tasks[task_i % self.world_size].append(task_name)
            local_tasks = rank_to_tasks[self.rank]
        else:
            # æ‰€æœ‰è¿›ç¨‹å¤„ç†ç›¸åŒä»»åŠ¡
            local_tasks = all_tasks
        
        if self.rank == 0:
            print(f"ğŸ¯ ä»»åŠ¡åˆ†ç‰‡é…ç½®:")
            for rank_i in range(self.world_size):
                tasks = rank_to_tasks[rank_i] if self.config.get('task_distribution', {}).get('enable_task_sharding', True) else all_tasks
                print(f"  Rank {rank_i}: {tasks}")
        
        return local_tasks
    
    def create_distributed_optimizer(self, policy):
        """åˆ›å»ºåˆ†å¸ƒå¼ä¼˜åŒ–å™¨"""
        optimizer_config = self.config['training']['optimizer']
        lr = self.config['algo']['lr']
        
        if self.rank == 0:
            print(f"\n=== åˆ›å»ºåˆ†å¸ƒå¼ä¼˜åŒ–å™¨ ===")
            print(f"ä¼˜åŒ–å™¨ç±»å‹: {optimizer_config['type']}")
            print(f"å­¦ä¹ ç‡: {lr}")
            print(f"æƒé‡è¡°å‡: {optimizer_config.get('weight_decay', 0.0)}")
        
        if optimizer_config['type'] == "AdamW":
            optimizer = torch.optim.AdamW(
                policy.parameters(),
                lr=lr,
                weight_decay=optimizer_config.get('weight_decay', 0.01),
                betas=(optimizer_config.get('beta1', 0.9), optimizer_config.get('beta2', 0.999))
            )
        elif optimizer_config['type'] == "Adam":
            optimizer = torch.optim.Adam(
                policy.parameters(),
                lr=lr,
                weight_decay=0.0
            )
        elif optimizer_config['type'] == "SGD":
            optimizer = torch.optim.SGD(
                policy.parameters(),
                lr=lr,
                momentum=optimizer_config.get('momentum', 0.9),
                weight_decay=optimizer_config.get('weight_decay', 0.0)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_config['type']}")
        
        # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
        total_params = sum(p.numel() for p in policy.parameters())
        trainable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)
        
        if self.rank == 0:
            print(f"æ€»å‚æ•°é‡: {total_params:,}")
            print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            print(f"è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params:.2%}")
        
        return optimizer
    
    def wrap_model_for_distributed(self, policy):
        """å°†æ¨¡å‹åŒ…è£…ä¸ºåˆ†å¸ƒå¼è®­ç»ƒ"""
        if self.is_distributed:
            # ä½¿ç”¨DistributedDataParallelåŒ…è£…æ¨¡å‹
            ddp_config = self.config.get('distributed', {})
            
            # æ¨¡å‹å·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼Œç›´æ¥åŒ…è£…
            policy = DDP(
                policy,  # å·²ç»åœ¨ç›®æ ‡è®¾å¤‡ä¸Š
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=ddp_config.get('find_unused_parameters', False),
                bucket_cap_mb=ddp_config.get('bucket_cap_mb', 25),
            )
            
            if self.rank == 0:
                print("âœ“ æ¨¡å‹å·²åŒ…è£…ä¸ºDistributedDataParallel")
        else:
            # éåˆ†å¸ƒå¼æ¨¡å¼ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if not next(policy.parameters()).device == self.device:
                policy = policy.to(self.device)
        
        return policy
    
    def create_distributed_sampler_config(self):
        """åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨çš„é…ç½®å¯¹è±¡"""
        class DistributedSamplerConfig:
            def __init__(self, config_dict, rank, world_size):
                # åŸºç¡€é…ç½®
                task_config = config_dict.get('task', {})
                algo_config = config_dict.get('algo', {})
                sampling_config = config_dict.get('enhanced_sampling', {})
                
                # ä»»åŠ¡é…ç½®
                self.benchmark_name = task_config.get('benchmark_name', 'libero_spatial')
                self.task_id = task_config.get('task_name', 8)
                self.num_parallel_envs = task_config.get('num_parallel_envs', 1)
                self.rollouts_per_env = task_config.get('rollouts_per_env', 1)
                self.max_episode_length = task_config.get('max_episode_length', 200)
                
                # åˆ†å¸ƒå¼é‡‡æ ·é…ç½® - æ ¹æ®è¿›ç¨‹æ•°è°ƒæ•´batchå¤§å°
                self.rollouts_per_batch = algo_config.get('rloo_batch_size', 4)
                self.target_batches_per_iteration = max(1, algo_config.get('data_batch_size', 2) // world_size)
                self.gradient_accumulation_steps = algo_config.get('gradient_accumulation_steps', 1)
                self.num_epochs = algo_config.get('num_epochs', 4)
                
                # å¢å¼ºé‡‡æ ·é…ç½®
                self.enable_smart_filtering = sampling_config.get('enable_smart_filtering', True)
                self.max_sampling_attempts = sampling_config.get('max_sampling_attempts', 20)
                self.debug_sampling = sampling_config.get('debug_sampling', True)
                self.save_videos = sampling_config.get('save_videos', True)
                self.video_dir = sampling_config.get('video_dir', f'rollout_videos_distributed_rank{rank}')
                
                # æ™ºèƒ½çŠ¶æ€è·Ÿè¸ªé…ç½®
                self.enable_rollout_stats_tracking = algo_config.get('enable_rollout_stats_tracking', True)
                self.rollout_skip_threshold = algo_config.get('rollout_skip_threshold', 3)
                self.rollout_stats_path = algo_config.get('rollout_stats_path', f'./rollout_stats_rank{rank}.json')
                self.state_history_window = 20
                
                # åˆ†å¸ƒå¼ç‰¹å®šé…ç½®
                self.rank = rank
                self.world_size = world_size
                self.sync_every_n_steps = config_dict.get('data_parallel', {}).get('sync_every_n_steps', 10)
        
        return DistributedSamplerConfig(self.config, self.rank, self.world_size)
    
    def distributed_collect_batches(self, env_runner, reward_function, init_dataset, sampler, config, iteration):
        """ğŸ”¥ ç²¾ç¡®æ§åˆ¶æ•°æ®æ”¶é›†æ•°é‡çš„åˆ†å¸ƒå¼é‡‡æ ·"""
        target_episodes = getattr(config, 'data_batch_size', 4)
        current_task = env_runner.get_current_task()
        
        if self.rank == 0:
            print(f"ğŸ¯ åˆ†å¸ƒå¼é‡‡æ · (è¿­ä»£ {iteration + 1}): ç›®æ ‡æ”¶é›† {target_episodes} æ¡è½¨è¿¹")
            print(f"   å½“å‰ä»»åŠ¡: {current_task}")
        
        collected_episodes = []
        attempt = 0
        max_attempts = getattr(config, 'max_sampling_attempts', 10)
        
        # ğŸ”¥ ç²¾ç¡®æ§åˆ¶ï¼šæ”¶é›†åˆ°data_batch_sizeæ¡å°±åœæ­¢
        while len(collected_episodes) < target_episodes and attempt < max_attempts:
            remaining_needed = target_episodes - len(collected_episodes)
            
            if self.rank == 0:
                print(f"    å°è¯• {attempt + 1}: è¿˜éœ€è¦ {remaining_needed} æ¡è½¨è¿¹")
            
            # ç¯å½¢ç´¢å¼•é€‰æ‹©åˆå§‹çŠ¶æ€
            try:
                init_states, state_hash = sampler.smart_sample_init_state(init_dataset)
                
                if self.rank == 0:
                    print(f"    ğŸ” é€‰æ‹©çŠ¶æ€å“ˆå¸Œ: {state_hash[:8]}")
                
                # è¿è¡Œç­–ç•¥æ”¶é›†è½¨è¿¹
                rollout_generator = env_runner.run_policy_in_env(
                    env_name=current_task,  # ğŸ”¥ ä½¿ç”¨å½“å‰ä»»åŠ¡
                    all_init_states=init_states,
                    debug_save_video=False  # é¿å…è¿‡å¤šè§†é¢‘æ–‡ä»¶
                )
                
                # æ”¶é›†æœ¬æ‰¹æ¬¡çš„è½¨è¿¹
                batch_episodes = []
                for success, total_reward, episode_data in rollout_generator:
                    episode = {
                        'success': success,
                        'total_reward': total_reward,
                        'task_name': current_task,  # è®°å½•ä»»åŠ¡ä¿¡æ¯
                        **episode_data
                    }
                    batch_episodes.append(episode)
                    
                    # ğŸ”¥ ç²¾ç¡®æ§åˆ¶ï¼šè¾¾åˆ°ç›®æ ‡æ•°é‡å°±åœæ­¢
                    if len(collected_episodes) + len(batch_episodes) >= target_episodes:
                        break
                
                collected_episodes.extend(batch_episodes)
                
                if self.rank == 0:
                    print(f"    âœ“ æœ¬æ‰¹æ¬¡æ”¶é›† {len(batch_episodes)} æ¡è½¨è¿¹ "
                          f"(ç´¯è®¡: {len(collected_episodes)}/{target_episodes})")
                
                # æ›´æ–°é‡‡æ ·å™¨ç»Ÿè®¡ä¿¡æ¯
                if hasattr(sampler, 'update_state_stats'):
                    sampler.update_state_stats(state_hash, [ep['success'] for ep in batch_episodes])
            
            except Exception as e:
                if self.rank == 0:
                    print(f"    âŒ é‡‡æ ·å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            
            attempt += 1
        
        # ğŸ”¥ è£å‰ªåˆ°ç²¾ç¡®æ•°é‡
        if len(collected_episodes) > target_episodes:
            collected_episodes = collected_episodes[:target_episodes]
            if self.rank == 0:
                print(f"    âœ‚ï¸ è£å‰ªåˆ°ç²¾ç¡®æ•°é‡: {len(collected_episodes)}")
        
        # å¦‚æœéœ€è¦ï¼ŒåŒæ­¥ç»Ÿè®¡ä¿¡æ¯
        if config.sync_every_n_steps > 0 and (iteration + 1) % config.sync_every_n_steps == 0:
            self.sync_sampling_stats(sampler)
        
        if self.rank == 0:
            success_count = sum(1 for ep in collected_episodes if ep['success'])
            print(f"âœ… Rank {self.rank} æ”¶é›†å®Œæˆ: {len(collected_episodes)} æ¡è½¨è¿¹ "
                  f"(æˆåŠŸç‡: {success_count}/{len(collected_episodes)})")
        
        return [collected_episodes] if collected_episodes else []
    
    def sync_sampling_stats(self, sampler):
        """åŒæ­¥é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_distributed:
            return
        
        if self.rank == 0:
            print("ğŸ”„ åŒæ­¥é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯...")
        
        # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„çŠ¶æ€ç»Ÿè®¡
        if hasattr(sampler, 'state_stats'):
            all_stats = gather_object_across_processes(sampler.state_stats)
            
            if self.rank == 0:
                # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
                merged_stats = {}
                for stats in all_stats:
                    for state_hash, state_info in stats.items():
                        if state_hash not in merged_stats:
                            merged_stats[state_hash] = {
                                'attempts': 0,
                                'successes': [],
                                'last_success_rate': 0.0
                            }
                        
                        merged_stats[state_hash]['attempts'] += state_info.get('attempts', 0)
                        merged_stats[state_hash]['successes'].extend(state_info.get('successes', []))
                        
                        # é™åˆ¶å†å²è®°å½•é•¿åº¦
                        if len(merged_stats[state_hash]['successes']) > 50:
                            merged_stats[state_hash]['successes'] = merged_stats[state_hash]['successes'][-50:]
                        
                        # é‡æ–°è®¡ç®—æˆåŠŸç‡
                        if merged_stats[state_hash]['successes']:
                            merged_stats[state_hash]['last_success_rate'] = (
                                sum(merged_stats[state_hash]['successes']) / 
                                len(merged_stats[state_hash]['successes'])
                            )
                
                # å¹¿æ’­åˆå¹¶åçš„ç»Ÿè®¡ä¿¡æ¯
                broadcast_stats = [merged_stats]
            else:
                broadcast_stats = [None]
            
            # å¹¿æ’­ç»Ÿè®¡ä¿¡æ¯ç»™æ‰€æœ‰è¿›ç¨‹
            dist.broadcast_object_list(broadcast_stats, src=0)
            merged_stats = broadcast_stats[0]
            
            # æ›´æ–°æœ¬åœ°ç»Ÿè®¡ä¿¡æ¯
            if hasattr(sampler, 'state_stats'):
                sampler.state_stats = merged_stats
    
    def distributed_gradient_accumulation(self, policy, optimizer, cfg_adapter, all_episodes, advantages, config):
        """åˆ†å¸ƒå¼æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ"""
        if self.rank == 0:
            print(f"ğŸŒ å¼€å§‹åˆ†å¸ƒå¼æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ...")
            print(f"  æœ¬åœ°episodes: {len(all_episodes)}")
            print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.gradient_accumulation_steps}")
            print(f"  è®­ç»ƒepochs: {config.num_epochs}")
        
        # è·å–åŸå§‹æ¨¡å‹ï¼ˆå¦‚æœè¢«DDPåŒ…è£…ï¼‰
        model = policy.module if hasattr(policy, 'module') else policy
        model.train()
        
        total_steps = 0
        all_losses = []
        
        for epoch in range(config.num_epochs):
            if self.rank == 0:
                print(f"\n=== Epoch {epoch + 1}/{config.num_epochs} ===(Rank {self.rank})")
            
            # åˆ›å»ºepisodeæ‰¹æ¬¡
            episode_batches = []
            batch_size = max(1, len(all_episodes) // config.gradient_accumulation_steps)
            
            for i in range(0, len(all_episodes), batch_size):
                batch_episodes = all_episodes[i:i + batch_size]
                batch_advantages = advantages[i:i + len(batch_episodes)]
                episode_batches.append((batch_episodes, batch_advantages))
            
            if self.rank == 0:
                print(f"åˆ›å»ºäº† {len(episode_batches)} ä¸ªæ‰¹æ¬¡ï¼Œå¹³å‡æ¯æ‰¹ {batch_size} episodes")
            
            # æ¢¯åº¦ç´¯ç§¯å¾ªç¯
            optimizer.zero_grad()
            epoch_losses = []
            
            for step, (batch_episodes, batch_advantages) in enumerate(episode_batches):
                try:
                    # è®¡ç®—batchæŸå¤±
                    batch_loss = cfg_adapter.compute_weighted_loss(batch_episodes, batch_advantages)
                    
                    # å½’ä¸€åŒ–æŸå¤±ï¼ˆæ¢¯åº¦ç´¯ç§¯çš„å…³é”®ï¼‰
                    normalized_loss = batch_loss / config.gradient_accumulation_steps
                    
                    # åå‘ä¼ æ’­ï¼ˆç´¯ç§¯æ¢¯åº¦ï¼‰
                    normalized_loss.backward()
                    
                    batch_loss_value = batch_loss.item()
                    epoch_losses.append(batch_loss_value)
                    
                    # æ¯Næ­¥æˆ–æœ€åä¸€æ­¥è¿›è¡Œå‚æ•°æ›´æ–°
                    if (step + 1) % config.gradient_accumulation_steps == 0 or (step + 1) == len(episode_batches):
                        # æ¢¯åº¦è£å‰ª
                        if self.config['algo']['grad_norm_clip'] > 0:
                            grad_norm = torch.nn.utils.clip_grad_norm_(
                                policy.parameters(), 
                                self.config['algo']['grad_norm_clip']
                            )
                        
                        # å‚æ•°æ›´æ–° (DDPä¼šè‡ªåŠ¨åŒæ­¥æ¢¯åº¦)
                        optimizer.step()
                        optimizer.zero_grad()
                        total_steps += 1
                        
                except Exception as e:
                    if self.rank == 0:
                        print(f"    âŒ Step {step + 1} è®­ç»ƒå‡ºé”™: {e}")
                    continue
            
            # Epochç»Ÿè®¡
            if epoch_losses:
                epoch_mean_loss = np.mean(epoch_losses)
                all_losses.extend(epoch_losses)
                if self.rank == 0:
                    print(f"âœ“ Epoch {epoch + 1} å¹³å‡æŸå¤±: {epoch_mean_loss:.6f}")
            else:
                if self.rank == 0:
                    print(f"âš ï¸ Epoch {epoch + 1} æ²¡æœ‰æœ‰æ•ˆæŸå¤±")
        
        return {
            'total_steps': total_steps,
            'total_losses': all_losses,
            'mean_loss': np.mean(all_losses) if all_losses else 0.0,
            'rank': self.rank
        }
    
    def aggregate_training_metrics(self, local_metrics):
        """èšåˆåˆ†å¸ƒå¼è®­ç»ƒæŒ‡æ ‡"""
        if not self.is_distributed:
            return local_metrics
        
        # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æŒ‡æ ‡
        all_metrics = gather_object_across_processes(local_metrics)
        
        if self.rank == 0:
            # èšåˆæŒ‡æ ‡
            aggregated_metrics = {}
            
            # æ•°å€¼æŒ‡æ ‡æ±‚å¹³å‡
            numeric_keys = ['mean_loss', 'total_steps']
            for key in numeric_keys:
                values = [m.get(key, 0) for m in all_metrics if m.get(key) is not None]
                aggregated_metrics[key] = np.mean(values) if values else 0.0
            
            # åˆ—è¡¨æŒ‡æ ‡åˆå¹¶
            all_losses = []
            for m in all_metrics:
                if 'total_losses' in m:
                    all_losses.extend(m['total_losses'])
            aggregated_metrics['total_losses'] = all_losses
            aggregated_metrics['aggregated_mean_loss'] = np.mean(all_losses) if all_losses else 0.0
            
            # æ€»æ­¥æ•°æ±‚å’Œ
            aggregated_metrics['total_training_steps'] = sum(m.get('total_steps', 0) for m in all_metrics)
            
            return aggregated_metrics
        else:
            return local_metrics
    
    def run_distributed_training(self):
        """æ‰§è¡Œå®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒæµç¨‹"""
        if self.rank == 0:
            print("=== ç¬¬10é˜¶æ®µï¼šåˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ ===")
            print("æ”¯æŒå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒå’Œä»»åŠ¡åˆ†ç‰‡")
            print()
        
        # è·å–åˆ†å¸ƒå¼ä»»åŠ¡åˆ†é…
        local_tasks = self.get_distributed_tasks()
        
        # æ˜¾ç¤ºå…³é”®é…ç½®
        if self.rank == 0:
            print(f"ğŸ¯ åˆ†å¸ƒå¼è®­ç»ƒé…ç½®æ‘˜è¦:")
            print(f"  æ¨¡å‹è·¯å¾„: {self.config['policy_path']}")
            print(f"  åŸºå‡†æµ‹è¯•: {self.config['task']['benchmark_name']}")
            print(f"  æœ¬åœ°ä»»åŠ¡: {local_tasks}")
            print(f"  è®­ç»ƒæ­¥æ•°: {self.config['training']['num_train_steps']}")
            print(f"  å­¦ä¹ ç‡: {self.config['algo']['lr']}")
            print(f"  æ‰¹æ¬¡å¤§å°: {self.config['algo']['rloo_batch_size']}")
            print(f"  ä¸–ç•Œå¤§å°: {self.world_size}")
            print(f"  æ¢¯åº¦ç´¯ç§¯: {self.config['algo']['gradient_accumulation_steps']}")
            print("")
        
        # åŠ è½½PI0æ¨¡å‹ - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        if self.rank == 0:
            print(f"åŠ è½½PI0æ¨¡å‹: {self.config['policy_path']}")
        
        # æ¸…ç†å†…å­˜å¹¶è®¾ç½®è®¾å¤‡
        torch.cuda.empty_cache()
        torch.cuda.set_device(self.local_rank)
        
        # ç›´æ¥åœ¨ç›®æ ‡GPUä¸ŠåŠ è½½æ¨¡å‹ä»¥èŠ‚çœå†…å­˜
        with torch.cuda.device(self.local_rank):
            policy = PI0Policy.from_pretrained(self.config['policy_path'])
            # ç«‹å³ç§»åŠ¨åˆ°ç›®æ ‡GPU
            policy = policy.to(self.device)
        
        if self.rank == 0:
            print(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ° {self.device}")
        
        # åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ¨¡å‹
        policy = self.wrap_model_for_distributed(policy)
        
        if self.rank == 0:
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = self.create_distributed_optimizer(policy)
        
        # åˆ›å»ºCFGé€‚é…å™¨
        if self.rank == 0:
            print(f"\n=== åˆ›å»ºCFGé€‚é…å™¨ ===")
        
        os.environ["PI0_DEBUG_SAVE_VIDEO"] = "false"  # é¿å…é‡å¤è§†é¢‘
        cfg_adapter = PI0_CFG_Adapter(policy, norm_stats_path=self.config['norm_stats_path'])
        
        if self.rank == 0:
            print("âœ“ CFGé€‚é…å™¨åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºç¯å¢ƒè¿è¡Œå™¨
        if self.rank == 0:
            print("\\n=== åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒè¿è¡Œå™¨ ===")
        
        sampler_config = self.create_distributed_sampler_config()
        
        env_runner = LIBEROEnvRunner(
            policy=policy,
            benchmark_name=sampler_config.benchmark_name,
            rollouts_per_env=sampler_config.rollouts_per_env,
            num_parallel_envs=sampler_config.num_parallel_envs,
            max_episode_length=sampler_config.max_episode_length,
            task_names_to_use=local_tasks,
            config=self.config,
            rank=self.rank,
            world_size=self.world_size,
            norm_stats_path=self.config['norm_stats_path']
        )
        
        # åˆ›å»ºç»„ä»¶
        reward_function = BinarySuccessReward()
        init_dataset = self.create_initial_state_dataset()
        smart_sampler = EnhancedSmartSampler(sampler_config)
        
        # åˆ›å»ºè§†é¢‘ç›®å½•
        video_dir = Path(sampler_config.video_dir)
        video_dir.mkdir(exist_ok=True)
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿åˆå§‹åŒ–å®Œæˆ
        if self.is_distributed:
            dist.barrier()
        
        # ğŸš€ ä¸»åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯å¼€å§‹
        if self.rank == 0:
            print(f"\\n=== å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯ ===")
            print(f"å°†è¿›è¡Œ {self.config['training']['num_train_steps']} è½®å®Œæ•´çš„è®­ç»ƒè¿­ä»£")
            print("ğŸ” ä½¿ç”¨åˆ†å¸ƒå¼RIPTæ™ºèƒ½é‡‡æ ·ï¼šå¤šGPUå¹¶è¡Œæ•°æ®æ”¶é›†")
        
        all_training_metrics = []
        
        for iteration in range(self.config['training']['num_train_steps']):
            if self.rank == 0:
                print(f"\\n" + "="*60)
                print(f"ğŸ”„ åˆ†å¸ƒå¼è®­ç»ƒè¿­ä»£ {iteration + 1}/{self.config['training']['num_train_steps']}")
                print("="*60)
            
            # ğŸ”¥ æ–°å¢ï¼šè®¾ç½®å½“å‰è¿­ä»£çš„ä»»åŠ¡
            current_task = env_runner.set_current_task_by_cursor()
            
            if current_task is None:  # è¯¥rankæ²¡æœ‰åˆ†é…ä»»åŠ¡
                if self.rank == 0:
                    print(f"âš ï¸ Rank {self.rank} æ²¡æœ‰åˆ†é…ä»»åŠ¡ï¼Œè·³è¿‡è¿­ä»£ {iteration + 1}")
                # æ¨è¿›cursorä»¥ä¿æŒåŒæ­¥
                env_runner.advance_task_cursor()
                continue
            
            print(f"ğŸ¯ Rank {self.rank} è¿­ä»£ {iteration + 1}: å¤„ç†ä»»åŠ¡ {current_task}")
            
            # ç¬¬1æ­¥ï¼šåˆ†å¸ƒå¼æ™ºèƒ½é‡‡æ ·æ”¶é›†æ•°æ®ï¼ˆåªå¤„ç†å½“å‰ä»»åŠ¡ï¼‰
            collected_batches = self.distributed_collect_batches(
                env_runner, reward_function, init_dataset, 
                smart_sampler, sampler_config, iteration
            )
            
            if not collected_batches:
                if self.rank == 0:
                    print("âš ï¸ æ™ºèƒ½é‡‡æ ·æœªæ”¶é›†åˆ°æœ‰ç”¨çš„æ•°æ®ï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒ")
                continue
            
            # ç¬¬2æ­¥ï¼šåˆå¹¶æ‰€æœ‰episodes
            all_episodes = []
            for batch in collected_batches:
                all_episodes.extend(batch)
            
            if not all_episodes:
                if self.rank == 0:
                    print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„episodesï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒ")
                continue
            
            # ç¬¬3æ­¥ï¼šè®¡ç®—ä¼˜åŠ¿å€¼
            if self.rank == 0:
                print(f"\\n--- åˆ†å¸ƒå¼ä¼˜åŠ¿è®¡ç®— ---")
            
            all_rewards = [ep['total_reward'] for ep in all_episodes]
            rewards_tensor = torch.tensor(all_rewards, dtype=torch.float32)
            
            # Leave-One-Outä¼˜åŠ¿è®¡ç®—
            advantages = []
            for i in range(len(all_rewards)):
                other_rewards = torch.cat([rewards_tensor[:i], rewards_tensor[i+1:]])
                baseline = other_rewards.mean() if len(other_rewards) > 0 else 0.0
                advantage = rewards_tensor[i] - baseline
                advantages.append(advantage.item())
            
            advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)
            
            if self.rank == 0:
                print(f"Rank {self.rank} - Episodesæ€»æ•°: {len(all_episodes)}")
                print(f"Rank {self.rank} - å¹³å‡å¥–åŠ±: {rewards_tensor.mean().item():.4f}")
                print(f"Rank {self.rank} - æˆåŠŸç‡: {sum(ep['success'] for ep in all_episodes) / len(all_episodes):.2f}")
                print(f"Rank {self.rank} - ä¼˜åŠ¿æ–¹å·®: {advantages.var().item():.6f}")
            
            # ç¬¬4æ­¥ï¼šåˆ†å¸ƒå¼æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ
            training_metrics = self.distributed_gradient_accumulation(
                policy, optimizer, cfg_adapter, all_episodes, advantages, sampler_config
            )
            
            # ç¬¬5æ­¥ï¼šèšåˆè®­ç»ƒæŒ‡æ ‡
            aggregated_metrics = self.aggregate_training_metrics(training_metrics)
            
            # ç¬¬6æ­¥ï¼šè®°å½•è¿­ä»£æŒ‡æ ‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
            if self.rank == 0:
                iteration_metrics = {
                    'iteration': iteration + 1,
                    'collected_batches': len(collected_batches),
                    'total_episodes': len(all_episodes),
                    'mean_reward': rewards_tensor.mean().item(),
                    'success_rate': sum(ep['success'] for ep in all_episodes) / len(all_episodes),
                    'advantage_variance': advantages.var().item(),
                    'world_size': self.world_size,
                    **aggregated_metrics
                }
                
                all_training_metrics.append(iteration_metrics)
                
                print(f"\\nâœ“ åˆ†å¸ƒå¼è¿­ä»£ {iteration + 1} å®Œæˆ:")
                print(f"  æ”¶é›†batches: {iteration_metrics['collected_batches']}")
                print(f"  æ€»episodes: {iteration_metrics['total_episodes']}")
                print(f"  å¹³å‡å¥–åŠ±: {iteration_metrics['mean_reward']:.4f}")
                print(f"  æˆåŠŸç‡: {iteration_metrics['success_rate']:.2f}")
                print(f"  èšåˆå¹³å‡æŸå¤±: {iteration_metrics.get('aggregated_mean_loss', 0.0):.6f}")
                print(f"  æ€»è®­ç»ƒæ­¥æ•°: {iteration_metrics.get('total_training_steps', 0)}")
            
            # ğŸ”¥ æ–°å¢ï¼šè¿­ä»£ç»“æŸåæ¨è¿›ä»»åŠ¡cursor
            next_task = env_runner.advance_task_cursor()
            if self.rank == 0 and next_task:
                print(f"ğŸ“ ä¸‹æ¬¡è¿­ä»£å°†å¤„ç†ä»»åŠ¡: {next_task}")
        
        # è®­ç»ƒæ€»ç»“
        if self.rank == 0:
            self.save_distributed_training_results(all_training_metrics)
            self.print_distributed_training_summary(all_training_metrics)
        
        return all_training_metrics
    
    def create_initial_state_dataset(self):
        """åˆ›å»ºåˆ†å¸ƒå¼åˆå§‹çŠ¶æ€æ•°æ®é›†"""
        class DistributedInitialStateDataset:
            def __init__(self, num_states=50, state_dim=8, rank=0, world_size=1):
                self.rank = rank
                self.world_size = world_size
                self.states = []
                
                # ğŸ”¥ æ–°å¢ï¼šç¯å½¢ç´¢å¼•ç›¸å…³å±æ€§
                self.init_state_cursor = 0  # ç¯å½¢ç´¢å¼•æ¸¸æ ‡
                
                # ç¡®ä¿ä¸åŒè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
                np.random.seed(42 + rank)
                
                for i in range(num_states):
                    base_state = np.zeros(state_dim, dtype=np.float32)
                    
                    if i < num_states // 3:
                        noise = np.random.normal(0, 0.05, state_dim).astype(np.float32)
                    elif i < 2 * num_states // 3:
                        noise = np.random.normal(0, 0.15, state_dim).astype(np.float32)
                    else:
                        noise = np.random.normal(0, 0.25, state_dim).astype(np.float32)
                    
                    state = base_state + noise
                    self.states.append(state)
                
                if self.rank == 0:
                    print(f"âœ“ åˆ›å»ºäº†åŒ…å« {len(self.states)} ä¸ªå¤šæ ·åŒ–åˆå§‹çŠ¶æ€çš„åˆ†å¸ƒå¼æ•°æ®é›†")
            
            def sample_batch(self, batch_size=1):
                """ä¿æŒå‘åå…¼å®¹çš„éšæœºé‡‡æ ·æ–¹æ³•"""
                if batch_size > len(self.states):
                    indices = np.random.choice(len(self.states), batch_size, replace=True)
                else:
                    indices = np.random.choice(len(self.states), batch_size, replace=False)
                
                sampled_states = np.array([self.states[i] for i in indices])
                return sampled_states
            
            # ğŸ”¥ æ–°å¢ï¼šç¯å½¢ç´¢å¼•é‡‡æ ·æ–¹æ³•
            def sample_batch_circular(self, batch_size: int) -> np.ndarray:
                """ç¯å½¢ç´¢å¼•é‡‡æ ·ï¼Œç¡®ä¿ä¸é‡å¤ä¸”æœ‰åº"""
                if len(self.states) == 0:
                    raise ValueError("çŠ¶æ€æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•é‡‡æ ·")
                
                selected_states = []
                
                for i in range(batch_size):
                    idx = (self.init_state_cursor + i) % len(self.states)
                    selected_states.append(self.states[idx])
                
                # æ›´æ–°cursor
                self.init_state_cursor = (self.init_state_cursor + batch_size) % len(self.states)
                
                return np.array(selected_states)
            
            def get_states_for_envs(self, num_parallel_envs: int, rloo_batch_size: int) -> np.ndarray:
                """ä¸ºå¹¶è¡Œç¯å¢ƒåˆ†é…ä¸åŒçš„åˆå§‹çŠ¶æ€"""
                total_states_needed = num_parallel_envs * rloo_batch_size
                
                if self.rank == 0:
                    print(f"ğŸ” ç¯å½¢ç´¢å¼•é‡‡æ ·: éœ€è¦ {total_states_needed} ä¸ªçŠ¶æ€ "
                          f"({num_parallel_envs} envs Ã— {rloo_batch_size} batch)")
                
                states = self.sample_batch_circular(total_states_needed)
                
                if self.rank == 0:
                    print(f"âœ“ å·²é€‰æ‹©çŠ¶æ€ç´¢å¼•èŒƒå›´: "
                          f"{(self.init_state_cursor - total_states_needed) % len(self.states)} - "
                          f"{(self.init_state_cursor - 1) % len(self.states)}")
                
                return states
        
        return DistributedInitialStateDataset(rank=self.rank, world_size=self.world_size)
    
    def save_distributed_training_results(self, all_training_metrics):
        """ä¿å­˜åˆ†å¸ƒå¼è®­ç»ƒç»“æœ"""
        results_file = self.exp_output_dir / "distributed_training_results.json"
        
        results = {
            'config': self.config,
            'training_metrics': all_training_metrics,
            'distributed_info': {
                'world_size': self.world_size,
                'backend': self.config.get('distributed', {}).get('backend', 'nccl'),
                'exp_output_dir': str(self.exp_output_dir),
                'video_dir': str(self.video_dir),
                'timestamp': datetime.now().isoformat(),
            },
            'experiment_summary': {
                'total_iterations': len(all_training_metrics),
                'total_episodes': sum(m['total_episodes'] for m in all_training_metrics),
                'total_training_steps': sum(m.get('total_training_steps', 0) for m in all_training_metrics),
                'final_success_rate': all_training_metrics[-1]['success_rate'] if all_training_metrics else 0.0,
                'final_loss': all_training_metrics[-1].get('aggregated_mean_loss', 0.0) if all_training_metrics else 0.0
            }
        }
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\\nğŸ’¾ åˆ†å¸ƒå¼è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    def print_distributed_training_summary(self, all_training_metrics):
        """æ‰“å°åˆ†å¸ƒå¼è®­ç»ƒæ‘˜è¦"""
        print(f"\\n" + "="*80)
        print("ğŸ‰ åˆ†å¸ƒå¼è®­ç»ƒå¾ªç¯ç»“æŸï¼")
        print("="*80)
        
        print("\\nâœ… æˆåŠŸå®ç°çš„åˆ†å¸ƒå¼åŠŸèƒ½:")
        print("1. âœ“ æ ‡å‡†PyTorchåˆ†å¸ƒå¼è®­ç»ƒ (DDP)")
        print("2. âœ“ ä»»åŠ¡åˆ†ç‰‡å’Œè´Ÿè½½å‡è¡¡")
        print("3. âœ“ åˆ†å¸ƒå¼æ•°æ®é‡‡æ ·")
        print("4. âœ“ æ¢¯åº¦è‡ªåŠ¨åŒæ­¥å’Œèšåˆ")
        print("5. âœ“ åˆ†å¸ƒå¼ç»Ÿè®¡æ•°æ®åŒæ­¥")
        print("6. âœ“ å¤šGPUç¯å¢ƒä¼˜åŒ–")
        
        if all_training_metrics:
            print(f"\\nğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒè½¨è¿¹æ€»ç»“:")
            print("Iter | Batches | Episodes | Reward | Success | Loss    | Steps | GPUs")
            print("-----|---------|----------|--------|---------|---------|-------|-----")
            for m in all_training_metrics:
                print(f"{m['iteration']:4d} | {m['collected_batches']:7d} | "
                      f"{m['total_episodes']:8d} | {m['mean_reward']:6.3f} | "
                      f"{m['success_rate']:7.2f} | {m.get('aggregated_mean_loss', 0.0):7.4f} | "
                      f"{m.get('total_training_steps', 0):5d} | {m['world_size']:4d}")
            
            first_loss = all_training_metrics[0].get('aggregated_mean_loss', 0.0)
            last_loss = all_training_metrics[-1].get('aggregated_mean_loss', 0.0)
            if first_loss > 0:
                loss_change = ((last_loss - first_loss) / first_loss) * 100
                print(f"\\nğŸ“ˆ åˆ†å¸ƒå¼è®­ç»ƒè¶‹åŠ¿:")
                print(f"  åˆå§‹æŸå¤±: {first_loss:.6f}")
                print(f"  æœ€ç»ˆæŸå¤±: {last_loss:.6f}")
                print(f"  æŸå¤±å˜åŒ–: {loss_change:+.2f}%")
                print(f"  æ€»GPUæ•°: {self.world_size}")
                print(f"  è®­ç»ƒåŠ é€Ÿæ¯”: ~{self.world_size:.1f}x (ç†è®º)")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="PI0 RIPTåˆ†å¸ƒå¼è®­ç»ƒ - ç¬¬10é˜¶æ®µ")
    parser.add_argument("--config_path", type=str, required=True,
                        help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--override", action='append', dest='overrides',
                        help="è¦†ç›–é…ç½®å‚æ•°ï¼Œæ ¼å¼: key=value")
    
    # ä¿æŒä¸ç¬¬9é˜¶æ®µçš„å…¼å®¹æ€§
    parser.add_argument("--training_iterations", type=int, default=None,
                        help="è®­ç»ƒè¿­ä»£æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--lr", type=float, default=None,
                        help="å­¦ä¹ ç‡ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--debug_sampling", action='store_true',
                        help="å¯ç”¨é‡‡æ ·è°ƒè¯•")
    
    return parser.parse_args()

def main():
    print("=== ç¬¬10é˜¶æ®µï¼šåˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ ===")
    print("ä»å•æœºè®­ç»ƒå‡çº§ä¸ºå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ")
    print()
    
    # è§£æå‚æ•°
    args = parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    is_distributed, rank, world_size, local_rank, device = setup_distributed()
    
    if rank == 0:
        print(f"ğŸŒ åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ:")
        print(f"  Is Distributed: {is_distributed}")
        print(f"  World Size: {world_size}")
        print(f"  Rank: {rank}")
        print(f"  Local Rank: {local_rank}")
        print(f"  Device: {device}")
        print()
    
    # æ„å»ºé…ç½®è¦†ç›–åˆ—è¡¨
    overrides = args.overrides or []
    
    # æ·»åŠ å…¼å®¹æ€§è¦†ç›–
    if args.training_iterations is not None:
        overrides.append(f"training.num_train_steps={args.training_iterations}")
    if args.lr is not None:
        overrides.append(f"algo.lr={args.lr}")
    if args.debug_sampling:
        overrides.append("enhanced_sampling.debug_sampling=true")
    
    # åŠ è½½å’ŒéªŒè¯é…ç½®
    try:
        config = load_config_from_yaml(args.config_path, overrides)
        config = validate_distributed_config(config)
    except Exception as e:
        if rank == 0:
            print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        cleanup_distributed()
        return 1
    
    # åˆ›å»ºå¹¶è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
    try:
        trainer = DistributedTrainingRunner(config)
        training_metrics = trainer.run_distributed_training()
        
        if rank == 0:
            print("\\nğŸ¯ ç¬¬10é˜¶æ®µå®Œæˆï¼ç°åœ¨æ”¯æŒå®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ")
        
        cleanup_distributed()
        return 0
        
    except Exception as e:
        if rank == 0:
            print(f"âŒ åˆ†å¸ƒå¼è®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        cleanup_distributed()
        return 1

if __name__ == "__main__":
    exit(main())