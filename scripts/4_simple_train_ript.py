#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆRIPTè®­ç»ƒè„šæœ¬ - ä¸ä½¿ç”¨wrapperï¼Œç›´æ¥è°ƒç”¨policy
æœ€æ­£ç¡®çš„ç»“æ„ï¼Œä¸ä½¿ç”¨try-catchå®¹é”™
åŒ…å«è§†é¢‘è¾“å‡ºåŠŸèƒ½ï¼Œä¿å­˜æ¯ä¸ªrolloutçš„MP4è§†é¢‘

ä½¿ç”¨æ–¹æ³•:
    cd /zhaohan/ZJH/openpi_pytorch
    python 4_simple_train_ript.py
    
è¾“å‡º:
    - rollout_videos/ ç›®å½•ä¸­ä¿å­˜æ‰€æœ‰episodeçš„è§†é¢‘æ–‡ä»¶
    - æ§åˆ¶å°è¾“å‡ºè®­ç»ƒè¿‡ç¨‹å’Œç»Ÿè®¡ä¿¡æ¯
"""

import os
import sys
import json
import numpy as np
import torch
from pathlib import Path
from datetime import datetime
import gym
from cleandiffuser.env import libero
import robosuite.utils.transform_utils as T
import imageio

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from pi0 import PI0Policy

def save_episode_video(frames, episode_idx, success, total_reward, task_description, video_dir):
    """ä¿å­˜å•ä¸ªepisodeçš„è§†é¢‘"""
    if not frames:
        print(f"  è­¦å‘Š: Episode {episode_idx} æ²¡æœ‰å¸§æ•°æ®ï¼Œè·³è¿‡è§†é¢‘ä¿å­˜")
        return None
        
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å¤„ç†ä»»åŠ¡æè¿°ï¼Œä½¿å…¶é€‚åˆä½œä¸ºæ–‡ä»¶å
    clean_task = task_description.lower().replace(" ", "_").replace(".", "").replace(",", "")[:30]
    
    # åˆ›å»ºæ–‡ä»¶å
    filename = f"episode_{episode_idx:02d}_{timestamp}_success_{success}_reward_{total_reward:.2f}_{clean_task}.mp4"
    video_path = video_dir / filename
    
    print(f"  ä¿å­˜è§†é¢‘: {video_path} (å…±{len(frames)}å¸§)")
    
    # æ£€æŸ¥ç¬¬ä¸€å¸§çš„æ ¼å¼
    first_frame = frames[0]
    print(f"  å¸§æ ¼å¼: shape={first_frame.shape}, dtype={first_frame.dtype}")
    
    # åˆ›å»ºè§†é¢‘
    writer = imageio.get_writer(str(video_path), fps=10)  # 10 FPS for better visibility
    
    frame_count = 0
    for frame in frames:
        # ç¡®ä¿å¸§æ ¼å¼æ­£ç¡®
        if frame.ndim != 3 or frame.shape[2] != 3:
            print(f"  è­¦å‘Š: è·³è¿‡æ— æ•ˆå¸§ï¼Œå½¢çŠ¶={frame.shape}")
            continue
            
        if frame.dtype != np.uint8:
            frame = (frame * 255).astype(np.uint8) if frame.max() <= 1.0 else frame.astype(np.uint8)
        
        writer.append_data(frame)
        frame_count += 1
    
    writer.close()
    print(f"  æˆåŠŸä¿å­˜ {frame_count} å¸§åˆ°è§†é¢‘æ–‡ä»¶")
    return str(video_path)

def main():
    print("=== ç®€åŒ–ç‰ˆRIPTè®­ç»ƒè„šæœ¬ ===")
    print("ç›´æ¥è°ƒç”¨policyï¼Œä¸ä½¿ç”¨wrapper")
    print()
    
    # åŠ è½½æ¨¡å‹
    PATH_TO_PI_MODEL = "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch"
    print(f"åŠ è½½PI0æ¨¡å‹: {PATH_TO_PI_MODEL}")
    policy = PI0Policy.from_pretrained(PATH_TO_PI_MODEL)
    
    # ä½¿ç”¨ä¸å‚è€ƒå®ç°ä¸€è‡´çš„è®¾å¤‡è·å–æ–¹å¼
    device = policy.config.device
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€çš„å½’ä¸€åŒ–å‚æ•°è·¯å¾„
    norm_stats_path = "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json"
    print(f"åŠ è½½å½’ä¸€åŒ–å‚æ•°: {norm_stats_path}")
    
    with open(norm_stats_path) as f:
        norm_stats = json.load(f)
    
    state_mean = np.array(norm_stats["norm_stats"]["state"]["mean"][:8], dtype=np.float32)
    state_std = np.array(norm_stats["norm_stats"]["state"]["std"][:8], dtype=np.float32)
    action_mean = np.array(norm_stats["norm_stats"]["actions"]["mean"][:7], dtype=np.float32)
    action_std = np.array(norm_stats["norm_stats"]["actions"]["std"][:7], dtype=np.float32)
    
    print(f"State mean shape: {state_mean.shape}")
    print(f"Action mean shape: {action_mean.shape}")
    
    # åˆ›å»ºç¯å¢ƒ
    print("åˆ›å»ºLIBEROç¯å¢ƒ...")
    env = gym.make(
        "libero-goal-v0",
        task_id=2,
        image_size=224,
        camera_names=["agentview", "robot0_eye_in_hand"],
        seed=0,
    )
    
    # é‡ç½®ç¯å¢ƒ
    obs = env.reset()
    
    # æ‰§è¡Œçƒ­æœºæ­¥éª¤
    dummy_action = np.array([0, 0, 0, 0, 0, 0, -1])
    for _ in range(20):
        obs, _, _, _ = env.step(dummy_action)
    
    print(f"ä»»åŠ¡æè¿°: {env.task_description}")
    print(f"åˆå§‹End-effectorä½ç½®: {obs['robot0_eef_pos']}")
    
    # å¼€å§‹ç®€å•çš„è®­ç»ƒå¾ªç¯
    print("\n=== å¼€å§‹ç®€åŒ–è®­ç»ƒå¾ªç¯ ===")
    
    num_rollouts = 5  # ç®€åŒ–ï¼šåªåš5ä¸ªrollout
    max_steps_per_rollout = 200  # æ¯ä¸ªrolloutæœ€å¤š50æ­¥
    
    all_episodes = []
    all_rewards = []
    
    # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
    video_dir = Path("rollout_videos")
    video_dir.mkdir(exist_ok=True)
    
    for rollout_idx in range(num_rollouts):
        print(f"\n--- Rollout {rollout_idx + 1}/{num_rollouts} ---")
        
        # é‡ç½®ç¯å¢ƒ
        obs = env.reset()
        for _ in range(20):  # çƒ­æœº
            obs, _, _, _ = env.step(dummy_action)
        
        episode_observations = []
        episode_actions = []
        episode_rewards = []
        episode_frames = []  # ç”¨äºä¿å­˜è§†é¢‘å¸§
        total_reward = 0.0
        done = False
        step = 0
        
        # ä¸å‚è€ƒå®ç°ä¸€è‡´ï¼šå¾ªç¯ç›´åˆ°doneæˆ–è¾¾åˆ°æ­¥æ•°é™åˆ¶
        while not done and step < max_steps_per_rollout:
            # å‡†å¤‡è§‚æµ‹æ•°æ®
            unnorm_state = np.concatenate([
                obs["robot0_eef_pos"],
                T.quat2axisangle(obs["robot0_eef_quat"]),
                obs["robot0_gripper_qpos"],
            ], dtype=np.float32)
            
            # æ·»åŠ è°ƒè¯•è¾“å‡ºï¼ˆä¸å‚è€ƒå®ç°ä¸€è‡´ï¼‰
            if step == 0 and rollout_idx == 0:
                print(">> gripper_qpos shape:", np.asarray(obs["robot0_gripper_qpos"]).shape)
            
            state = (unnorm_state - state_mean) / (state_std + 1e-6)
            
            # æ³¨æ„ï¼šè¿™é‡Œçš„å›¾åƒå¤„ç†è¦ä¸2_test_pi0_on_libero.pyå®Œå…¨ä¸€è‡´
            # policyè¾“å…¥çš„å›¾åƒä¸ä½¿ç”¨transpose
            base_0_rgb = obs["agentview_image"][:, :, ::-1].copy()
            left_wrist_0_rgb = obs["robot0_eye_in_hand_image"][:, :, ::-1].copy()
            
            observation = {
                "image": {
                    "base_0_rgb": torch.from_numpy(base_0_rgb).to(device)[None],
                    "left_wrist_0_rgb": torch.from_numpy(left_wrist_0_rgb).to(device)[None],
                },
                "state": torch.from_numpy(state).to(device)[None],
                "prompt": [env.task_description],
            }
            
            # ç›´æ¥è°ƒç”¨policyè¿›è¡Œæ¨ç†ï¼ˆä¸å‚è€ƒå®ç°å®Œå…¨ä¸€è‡´ï¼‰
            action = policy.select_action(observation)[0, :, :7]
            action = action.cpu().numpy()
            action = action * (action_std + 1e-6) + action_mean
            action[:, :6] += unnorm_state[None, :6]
            
            # ä¸ºäº†ä¿æŒä¸è®­ç»ƒé€»è¾‘çš„å…¼å®¹æ€§ï¼Œé‡å‘½åä¸ºaction_denorm
            action_denorm = action
            
            # è®°å½•è§‚æµ‹å’ŒåŠ¨ä½œ
            episode_observations.append(obs.copy())
            episode_actions.append(action_denorm.copy())
            
            # è®°å½•åŠ¨ä½œæ‰§è¡Œå‰çš„åˆå§‹å¸§
            frame = obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
            if frame.ndim == 3 and frame.shape[2] == 3:
                episode_frames.append(frame)
            
            # æ‰§è¡ŒåŠ¨ä½œåºåˆ— - å‚è€ƒ2_test_pi0_on_libero.pyçš„æ–¹å¼
            # åŸå®ç°æ‰§è¡Œæ•´ä¸ª50æ­¥çš„åŠ¨ä½œåºåˆ—
            for i in range(min(action_denorm.shape[0], max_steps_per_rollout - step)):
                current_action = action_denorm[i, :7]
                obs, reward, done, info = env.step(current_action)
                
                # è®°å½•æ¯æ­¥çš„è§†é¢‘å¸§
                frame = obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                if frame.ndim == 3 and frame.shape[2] == 3:
                    episode_frames.append(frame)
                
                episode_rewards.append(reward)
                total_reward += reward
                step += 1
                
                if step % 10 == 0:
                    print(f"  Step {step}: reward={reward:.4f}, total_reward={total_reward:.4f}")
                
                if done or step >= max_steps_per_rollout:
                    break
        
        # æ·»åŠ æœ€åä¸€å¸§ï¼ˆå¦‚æœç¯å¢ƒæ²¡æœ‰ç»“æŸï¼‰
        if not done:
            final_frame = obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
            if final_frame.ndim == 3 and final_frame.shape[2] == 3:
                episode_frames.append(final_frame)
        
        # è®¡ç®—episodeæˆåŠŸç‡ï¼ˆç®€åŒ–ï¼šåŸºäºæ€»å¥–åŠ±ï¼‰
        success = total_reward > 0.1  # ğŸ”¥ RIPTå¯¹é½ï¼šè°ƒæ•´æˆåŠŸåˆ¤æ–­é˜ˆå€¼
        
        episode_data = {
            'observations': episode_observations,
            'actions': episode_actions,
            'rewards': episode_rewards,
            'task': [env.task_description],
            'success': success,
            'total_reward': total_reward,
            'length': len(episode_rewards)
        }
        
        all_episodes.append(episode_data)
        all_rewards.append(total_reward)
        
        # ä¿å­˜è¯¥episodeçš„è§†é¢‘
        if episode_frames:
            save_episode_video(episode_frames, rollout_idx, success, total_reward, env.task_description, video_dir)
        
        print(f"  Episodeå®Œæˆ: é•¿åº¦={len(episode_rewards)}, æ€»å¥–åŠ±={total_reward:.4f}, æˆåŠŸ={success}")
    
    # è®¡ç®—ä¼˜åŠ¿å€¼ï¼ˆç®€åŒ–ï¼šä½¿ç”¨Leave-One-Out baselineï¼‰
    print("\n=== è®¡ç®—ä¼˜åŠ¿å€¼ ===")
    rewards_tensor = torch.tensor(all_rewards, dtype=torch.float32)
    advantages = []
    
    for i in range(len(all_rewards)):
        # Leave-One-Out baseline
        other_rewards = torch.cat([rewards_tensor[:i], rewards_tensor[i+1:]])
        baseline = other_rewards.mean() if len(other_rewards) > 0 else 0.0
        advantage = rewards_tensor[i] - baseline
        advantages.append(advantage.item())
    
    advantages = torch.tensor(advantages, dtype=torch.float32)
    
    print(f"å¥–åŠ±: {[f'{r:.3f}' for r in all_rewards]}")
    print(f"ä¼˜åŠ¿: {[f'{a:.3f}' for a in advantages.tolist()]}")
    print(f"å¹³å‡å¥–åŠ±: {rewards_tensor.mean().item():.4f}")
    print(f"æˆåŠŸç‡: {sum(ep['success'] for ep in all_episodes) / len(all_episodes):.2f}")
    
    # ç®€åŒ–çš„"è®­ç»ƒ"æ­¥éª¤ï¼ˆå®é™…ä¸Šåªæ˜¯æ¼”ç¤ºæ•°æ®å¤„ç†ï¼‰
    print("\n=== æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤ ===")
    
    # å¤„ç†episodesæ•°æ®ç”¨äºè®­ç»ƒ
    for i, (episode, advantage) in enumerate(zip(all_episodes, advantages)):
        print(f"Episode {i}: advantage={advantage:.3f}, å°†ç”¨äºåŠ æƒè®­ç»ƒ")
        
        # è¿™é‡Œå¯ä»¥åŠ å…¥å®é™…çš„è®­ç»ƒé€»è¾‘
        # æ¯”å¦‚ï¼šè°ƒç”¨ policy.forward() è®¡ç®—æŸå¤±ï¼Œç„¶ååå‘ä¼ æ’­
        # ä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªæ˜¯æ¼”ç¤ºæ•°æ®æµ
        
        if len(episode['actions']) > 0:
            first_action = episode['actions'][0]
            print(f"  ç¬¬ä¸€ä¸ªåŠ¨ä½œå½¢çŠ¶: {first_action.shape}")
    
    print("\n=== è®­ç»ƒå®Œæˆ ===")
    print("è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ¼”ç¤ºç‰ˆæœ¬ï¼Œå±•ç¤ºäº†ï¼š")
    print("1. ç›´æ¥è°ƒç”¨policy.select_action()è¿›è¡Œæ¨ç†")
    print("2. æ”¶é›†è½¨è¿¹æ•°æ®")
    print("3. è®¡ç®—ä¼˜åŠ¿å€¼")
    print("4. å‡†å¤‡è®­ç»ƒæ•°æ®")
    print("5. ä¿å­˜rolloutè§†é¢‘ç”¨äºåˆ†æ")
    print("å®é™…çš„è®­ç»ƒæ­¥éª¤ï¼ˆæ¢¯åº¦æ›´æ–°ï¼‰éœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚å®ç°")
    
    # æ˜¾ç¤ºä¿å­˜çš„è§†é¢‘æ–‡ä»¶
    print(f"\n=== è§†é¢‘æ–‡ä»¶ä¿å­˜åœ¨: {video_dir} ===")
    video_files = list(video_dir.glob("*.mp4"))
    if video_files:
        print(f"å…±ä¿å­˜äº† {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶:")
        for video_file in sorted(video_files):
            print(f"  - {video_file.name}")
    else:
        print("æ²¡æœ‰ä¿å­˜è§†é¢‘æ–‡ä»¶")
    
    # æ¸…ç†
    env.close()

if __name__ == "__main__":
    main()