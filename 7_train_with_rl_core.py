#!/usr/bin/env python3
"""
ç¬¬1é˜¶æ®µï¼šæ ¸å¿ƒRLè®­ç»ƒå¾ªç¯ (7_train_with_rl_core.py)
åŸºäº6_simple_train_direct_runner.pyï¼Œæ·»åŠ çœŸå®çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŠŸèƒ½

æ ¸å¿ƒæ–°å¢åŠŸèƒ½ï¼š
1. ä¼˜åŒ–å™¨åˆ›å»ºå’Œç®¡ç†
2. PI0Policyè®­ç»ƒæ¨¡å¼åˆ‡æ¢ 
3. CFG-styleä¼˜åŠ¿åŠ æƒæŸå¤±è®¡ç®—
4. çœŸå®çš„æ¢¯åº¦æ›´æ–°å¾ªç¯
5. è®­ç»ƒæŒ‡æ ‡ç›‘æ§

ä½¿ç”¨æ–¹æ³•:
    cd /zhaohan/ZJH/openpi_pytorch
    python 7_train_with_rl_core.py
    
    # è‡ªå®šä¹‰é…ç½®
    python 7_train_with_rl_core.py --lr 2e-5 --num_epochs 3 --debug_loss true
"""

import os
import sys
import json
import numpy as np
import torch
from pathlib import Path
from datetime import datetime
import argparse

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from pi0 import PI0Policy
from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
from pi0.ript.reward_function import BinarySuccessReward
from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter

class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»ï¼Œæ›¿ä»£ç¡¬ç¼–ç å‚æ•°"""
    def __init__(self):
        # æ¨¡å‹é…ç½®
        self.model_path = "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch"
        self.norm_stats_path = "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json"
        
        # ç¯å¢ƒé…ç½®
        self.benchmark_name = "libero_goal"
        self.task_id = 2
        self.num_parallel_envs = 1
        self.max_episode_length = 200
        
        # å¼ºåŒ–å­¦ä¹ é…ç½®
        self.num_rollouts = 5
        self.num_epochs = 3  # æ–°å¢ï¼šå¤šè½®è®­ç»ƒ
        self.learning_rate = 1e-5
        self.optimizer_type = "AdamW"
        self.weight_decay = 0.01
        self.max_grad_norm = 1.0  # æ–°å¢ï¼šæ¢¯åº¦è£å‰ª
        
        # è°ƒè¯•é…ç½®
        self.save_videos = True
        self.video_dir = "rollout_videos_rl_core"
        self.debug_loss_computation = True  # æ–°å¢ï¼šæŸå¤±è®¡ç®—è°ƒè¯•
        self.log_training_details = True   # æ–°å¢ï¼šè¯¦ç»†è®­ç»ƒæ—¥å¿—

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="PI0 RL Core Training")
    parser.add_argument("--lr", type=float, default=1e-5, help="Learning rate")
    parser.add_argument("--num_epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument("--num_rollouts", type=int, default=5, help="Number of rollouts")
    parser.add_argument("--debug_loss", type=bool, default=True, help="Enable loss computation debugging")
    parser.add_argument("--save_videos", type=bool, default=True, help="Save rollout videos")
    return parser.parse_args()

def create_optimizer(policy, config):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    print(f"\n=== åˆ›å»ºä¼˜åŒ–å™¨ ===")
    print(f"ä¼˜åŒ–å™¨ç±»å‹: {config.optimizer_type}")
    print(f"å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"æƒé‡è¡°å‡: {config.weight_decay}")
    
    if config.optimizer_type == "AdamW":
        optimizer = torch.optim.AdamW(
            policy.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.999)
        )
    elif config.optimizer_type == "Adam":
        optimizer = torch.optim.Adam(
            policy.parameters(),
            lr=config.learning_rate,
            weight_decay=0.0  # Adamä¸ä½¿ç”¨weight_decay
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {config.optimizer_type}")
    
    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
    total_params = sum(p.numel() for p in policy.parameters())
    trainable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params:.2%}")
    
    return optimizer

def save_episode_video(frames, episode_idx, success, total_reward, task_description, video_dir):
    """ä¿å­˜episodeè§†é¢‘ï¼ˆä¸6è„šæœ¬ä¿æŒä¸€è‡´ï¼‰"""
    if not frames:
        print(f"  è­¦å‘Š: Episode {episode_idx} æ²¡æœ‰å¸§æ•°æ®ï¼Œè·³è¿‡è§†é¢‘ä¿å­˜")
        return None
        
    # å¤„ç†å¯èƒ½çš„ä¸åŒå¸§æ ¼å¼
    processed_frames = []
    for frame in frames:
        if isinstance(frame, np.ndarray):
            # ç¡®ä¿å¸§æ ¼å¼æ­£ç¡® (H, W, C)
            if frame.ndim == 3 and frame.shape[2] == 3:
                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                if frame.dtype != np.uint8:
                    frame = (frame * 255).astype(np.uint8) if frame.max() <= 1.0 else frame.astype(np.uint8)
                processed_frames.append(frame)
    
    if not processed_frames:
        print(f"  è­¦å‘Š: Episode {episode_idx} æ²¡æœ‰æœ‰æ•ˆçš„å¸§æ•°æ®")
        return None
        
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å¤„ç†ä»»åŠ¡æè¿°ï¼Œä½¿å…¶é€‚åˆä½œä¸ºæ–‡ä»¶å
    clean_task = str(task_description).lower().replace(" ", "_").replace(".", "").replace(",", "")[:30]
    
    # åˆ›å»ºæ–‡ä»¶å
    filename = f"episode_{episode_idx:02d}_{timestamp}_success_{success}_reward_{total_reward:.2f}_{clean_task}.mp4"
    video_path = video_dir / filename
    
    print(f"  ä¿å­˜è§†é¢‘: {video_path} (å…±{len(processed_frames)}å¸§)")
    
    # åˆ›å»ºè§†é¢‘
    try:
        import imageio
        writer = imageio.get_writer(str(video_path), fps=10)  # 10 FPS for better visibility
        
        for frame in processed_frames:
            writer.append_data(frame)
        
        writer.close()
        print(f"  æˆåŠŸä¿å­˜è§†é¢‘æ–‡ä»¶")
        return str(video_path)
    except Exception as e:
        print(f"  é”™è¯¯: ä¿å­˜è§†é¢‘å¤±è´¥ - {e}")
        return None

def main():
    print("=== ç¬¬1é˜¶æ®µï¼šæ ¸å¿ƒRLè®­ç»ƒå¾ªç¯ ===")
    print("åŸºäº6è„šæœ¬ï¼Œæ·»åŠ çœŸå®çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŠŸèƒ½")
    print()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°å’Œåˆ›å»ºé…ç½®
    args = parse_args()
    config = TrainingConfig()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    config.learning_rate = args.lr
    config.num_epochs = args.num_epochs
    config.num_rollouts = args.num_rollouts
    config.debug_loss_computation = args.debug_loss
    config.save_videos = args.save_videos
    
    print(f"è®­ç»ƒé…ç½®:")
    print(f"  å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  è®­ç»ƒè½®æ•°: {config.num_epochs}")
    print(f"  æ”¶é›†è½¨è¿¹æ•°: {config.num_rollouts}")
    print(f"  è°ƒè¯•æ¨¡å¼: {config.debug_loss_computation}")
    print("")
    
    # åŠ è½½PI0æ¨¡å‹
    print(f"åŠ è½½PI0æ¨¡å‹: {config.model_path}")
    policy = PI0Policy.from_pretrained(config.model_path)
    device = policy.config.device
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ¨¡å¼ç®¡ç†è¯´æ˜
    print("\n=== æ¨¡å¼ç®¡ç†è¯´æ˜ ===")
    print("PI0Policyæœ‰ä¸¤ç§æ¨¡å¼:")
    print("  - eval() æ¨¡å¼: ç”¨äºè½¨è¿¹æ”¶é›†ï¼Œè°ƒç”¨select_action()")
    print("  - train() æ¨¡å¼: ç”¨äºå‚æ•°æ›´æ–°ï¼Œè°ƒç”¨forward()")
    print("æˆ‘ä»¬å°†åœ¨é€‚å½“çš„æ—¶å€™åˆ‡æ¢æ¨¡å¼")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(policy, config)
    
    # åˆ›å»ºCFGé€‚é…å™¨
    print(f"\n=== åˆ›å»ºCFGé€‚é…å™¨ ===")
    print(f"åŠ è½½å½’ä¸€åŒ–å‚æ•°: {config.norm_stats_path}")
    
    # ç¦ç”¨CFGé€‚é…å™¨çš„è§†é¢‘ä¿å­˜ï¼Œé¿å…é‡å¤ä¿å­˜
    os.environ["PI0_DEBUG_SAVE_VIDEO"] = "false"  # ä¸´æ—¶ç¦ç”¨
    cfg_adapter = PI0_CFG_Adapter(policy, norm_stats_path=config.norm_stats_path)
    print("âœ“ CFGé€‚é…å™¨åˆ›å»ºæˆåŠŸ (å·²ç¦ç”¨é‡å¤è§†é¢‘ä¿å­˜)")
    
    # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
    video_dir = Path(config.video_dir)
    video_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºLIBEROç¯å¢ƒè¿è¡Œå™¨ï¼ˆä¸6è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    print("\n=== åˆå§‹åŒ–ç¯å¢ƒè¿è¡Œå™¨ ===")
    env_runner = LIBEROEnvRunner(
        policy=policy,
        benchmark_name=config.benchmark_name,
        rollouts_per_env=1,
        num_parallel_envs=config.num_parallel_envs,
        max_episode_length=config.max_episode_length,
        task_names_to_use=[config.task_id],
        config=config.__dict__,  # ä¼ é€’é…ç½®å­—å…¸
        rank=0,
        world_size=1,
        norm_stats_path=config.norm_stats_path
    )
    
    # åˆ›å»ºå¥–åŠ±å‡½æ•°
    reward_function = BinarySuccessReward()
    
    # ğŸ¯ æ ¸å¿ƒRLè®­ç»ƒå¾ªç¯å¼€å§‹
    print(f"\n=== å¼€å§‹æ ¸å¿ƒRLè®­ç»ƒå¾ªç¯ ===")
    print(f"å°†è¿›è¡Œ {config.num_rollouts} æ¬¡è½¨è¿¹æ”¶é›†å’Œ {config.num_epochs} è½®ä¼˜åŒ–")
    print("æ³¨æ„: å½“å‰PI0æ¨¡å‹æ€§èƒ½å¾ˆå¥½ï¼Œå¯èƒ½å‡ºç°æˆåŠŸç‡100%çš„æƒ…å†µ")
    print("è¿™ä¼šå¯¼è‡´Leave-One-Outä¼˜åŠ¿è®¡ç®—ç»“æœä¸º0ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œè¡¨æ˜ç­–ç•¥å·²æ”¶æ•›")
    
    all_episodes = []
    all_rewards = []
    training_metrics = []
    
    # ç¬¬1æ­¥ï¼šè½¨è¿¹æ”¶é›†ï¼ˆä¸6è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    print(f"\n--- ç¬¬1æ­¥ï¼šè½¨è¿¹æ”¶é›† ---")
    print("ğŸ”„ åˆ‡æ¢åˆ°evalæ¨¡å¼è¿›è¡Œè½¨è¿¹æ”¶é›†...")
    policy.eval()  # å…³é”®ï¼šæ¨ç†æ¨¡å¼æ”¶é›†è½¨è¿¹
    print("âœ“ PI0Policyç°åœ¨å¤„äºæ¨ç†æ¨¡å¼")
    
    for rollout_idx in range(config.num_rollouts):
        print(f"\nRollout {rollout_idx + 1}/{config.num_rollouts}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„åˆå§‹çŠ¶æ€æ•°æ®
        dummy_init_states = np.zeros((config.num_parallel_envs, 8), dtype=np.float32)
        
        print("æ”¶é›†è½¨è¿¹...")
        
        try:
            # ç›´æ¥è°ƒç”¨env_runnerçš„run_policy_in_envæ–¹æ³•
            rollout_generator_for_batch = env_runner.run_policy_in_env(
                env_name=config.task_id,
                all_init_states=dummy_init_states
            )
            
            # æ”¶é›†ç”Ÿæˆçš„è½¨è¿¹
            batch_rollouts = list(rollout_generator_for_batch)
            
            if not batch_rollouts:
                print("è­¦å‘Š: æœªç”Ÿæˆæœ‰æ•ˆè½¨è¿¹ï¼Œè·³è¿‡æ­¤rollout")
                continue
                
        except Exception as e:
            print(f"ç”Ÿæˆè½¨è¿¹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
        
        # å¤„ç†ç”Ÿæˆçš„rollouts
        for rollout_idx_in_batch, rollout in enumerate(batch_rollouts):
            print(f"å¤„ç†rollout {rollout_idx_in_batch + 1}/{len(batch_rollouts)}")
            
            # rolloutæ ¼å¼: (success, total_reward, episode_data)
            success, total_reward, episode_data = rollout
            
            # åˆ›å»ºepisodeå­—å…¸
            episode = {
                'success': success,
                'total_reward': total_reward,
                **episode_data  # åŒ…å«observations, actions, rewards, taskç­‰
            }
            
            # ä½¿ç”¨å¥–åŠ±å‡½æ•°è®¡ç®—å¥–åŠ±ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
            try:
                computed_reward = reward_function.compute_reward(rollout_idx_in_batch, episode, None)
                episode['computed_reward'] = computed_reward
            except Exception as e:
                print(f"è®¡ç®—å¥–åŠ±æ—¶å‡ºé”™: {e}")
                episode['computed_reward'] = 0.0
            
            # ä¿å­˜episodeæ•°æ®
            all_episodes.append(episode)
            all_rewards.append(total_reward)
            
            # ä¿å­˜è§†é¢‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if config.save_videos and 'video_frames' in episode_data and episode_data['video_frames']:
                save_episode_video(
                    episode_data['video_frames'], 
                    len(all_episodes) - 1, 
                    success, 
                    total_reward,
                    episode_data.get('task', 'unknown_task'),
                    video_dir
                )
            
            print(f"  Episodeå®Œæˆ: å¥–åŠ±={total_reward:.4f}, æˆåŠŸ={success}, è®¡ç®—å¥–åŠ±={episode['computed_reward']:.4f}")
    
    # ç¬¬2æ­¥ï¼šä¼˜åŠ¿è®¡ç®—ï¼ˆæ ‡å‡†Leave-One-Outæ–¹æ³•ï¼‰
    print(f"\n--- ç¬¬2æ­¥ï¼šä¼˜åŠ¿è®¡ç®— ---")
    
    if not all_rewards:
        print("è­¦å‘Š: æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•å¥–åŠ±æ•°æ®ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
        return
    
    rewards_tensor = torch.tensor(all_rewards, dtype=torch.float32)
    advantages = []
    
    # æ ‡å‡†çš„Leave-One-Outä¼˜åŠ¿è®¡ç®—
    for i in range(len(all_rewards)):
        # Leave-One-Out baseline: é™¤äº†å½“å‰è½¨è¿¹å¤–ï¼Œå…¶ä»–è½¨è¿¹çš„å¹³å‡å¥–åŠ±
        other_rewards = torch.cat([rewards_tensor[:i], rewards_tensor[i+1:]])
        baseline = other_rewards.mean() if len(other_rewards) > 0 else 0.0
        advantage = rewards_tensor[i] - baseline
        advantages.append(advantage.item())
    
    advantages = torch.tensor(advantages, dtype=torch.float32, device=device)
    
    print(f"å¥–åŠ±: {[f'{r:.3f}' for r in all_rewards]}")
    print(f"ä¼˜åŠ¿: {[f'{a:.3f}' for a in advantages.tolist()]}")
    print(f"å¹³å‡å¥–åŠ±: {rewards_tensor.mean().item():.4f}")
    print(f"æˆåŠŸç‡: {sum(ep['success'] for ep in all_episodes) / len(all_episodes):.2f}")
    
    # åˆ†æä¼˜åŠ¿æƒ…å†µ
    advantage_variance = advantages.var().item()
    print(f"ä¼˜åŠ¿æ–¹å·®: {advantage_variance:.6f}")
    
    if advantage_variance < 1e-6:
        print("â„¹ï¸  æ‰€æœ‰è½¨è¿¹è¡¨ç°ç›¸åŒï¼Œä¼˜åŠ¿å‡ä¸º0")
        print("è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼šå½“å‰ç­–ç•¥å·²ç»æ”¶æ•›ï¼Œæ‰€æœ‰è½¨è¿¹è´¨é‡ç›¸ç­‰")
        print("åœ¨çœŸå®è®­ç»ƒä¸­ï¼Œè¿™è¡¨æ˜éœ€è¦æ›´å¤šæ¢ç´¢æˆ–æ›´å›°éš¾çš„ä»»åŠ¡")
    else:
        print("âœ“ æ£€æµ‹åˆ°è½¨è¿¹è´¨é‡å·®å¼‚ï¼Œä¼˜åŠ¿è®¡ç®—æœ‰æ•ˆ")
    
    # ğŸš€ ç¬¬3æ­¥ï¼šæ ¸å¿ƒRLè®­ç»ƒå¾ªç¯ï¼ˆå…¨æ–°åŠŸèƒ½ï¼‰
    print(f"\n--- ç¬¬3æ­¥ï¼šæ ¸å¿ƒRLè®­ç»ƒå¾ªç¯ ---")
    print("ğŸ”„ åˆ‡æ¢åˆ°trainæ¨¡å¼è¿›è¡Œå‚æ•°æ›´æ–°...")
    policy.train()  # å…³é”®ï¼šè®­ç»ƒæ¨¡å¼æ›´æ–°å‚æ•°
    print("âœ“ PI0Policyç°åœ¨å¤„äºè®­ç»ƒæ¨¡å¼")
    print(f"å¼€å§‹ {config.num_epochs} è½®CFG-styleä¼˜åŠ¿åŠ æƒè®­ç»ƒ...")
    
    for epoch in range(config.num_epochs):
        print(f"\n=== Epoch {epoch + 1}/{config.num_epochs} ===")
        
        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        try:
            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨CFGé€‚é…å™¨è®¡ç®—åŠ æƒæŸå¤±
            print("è®¡ç®—CFG-styleä¼˜åŠ¿åŠ æƒæŸå¤±...")
            
            if config.debug_loss_computation:
                print(f"  è¾“å…¥episodesæ•°é‡: {len(all_episodes)}")
                print(f"  è¾“å…¥advantageså½¢çŠ¶: {advantages.shape}")
                print(f"  ä¼˜åŠ¿å€¼èŒƒå›´: [{advantages.min().item():.4f}, {advantages.max().item():.4f}]")
            
            # è¿™æ˜¯æ ¸å¿ƒçš„CFG-styleè®­ç»ƒ
            weighted_loss = cfg_adapter.compute_weighted_loss(all_episodes, advantages)
            
            if config.debug_loss_computation:
                print(f"  è®¡ç®—å¾—åˆ°åŠ æƒæŸå¤±: {weighted_loss.item():.6f}")
                print(f"  æŸå¤±å¼ é‡è®¾å¤‡: {weighted_loss.device}")
                print(f"  æŸå¤±requires_grad: {weighted_loss.requires_grad}")
            
            # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
            if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                print(f"âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆæŸå¤±å€¼: {weighted_loss.item()}")
                print("è·³è¿‡æœ¬è½®ä¼˜åŒ–")
                continue
            
            # ğŸ”¥ å…³é”®ï¼šåå‘ä¼ æ’­
            print("æ‰§è¡Œåå‘ä¼ æ’­...")
            weighted_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if config.max_grad_norm > 0:
                grad_norm = torch.nn.utils.clip_grad_norm_(policy.parameters(), config.max_grad_norm)
                if config.debug_loss_computation:
                    print(f"  æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
            
            # ğŸ”¥ å…³é”®ï¼šå‚æ•°æ›´æ–°
            print("æ›´æ–°æ¨¡å‹å‚æ•°...")
            optimizer.step()
            
            # è®°å½•è®­ç»ƒæŒ‡æ ‡
            epoch_metrics = {
                'epoch': epoch + 1,
                'loss': weighted_loss.item(),
                'grad_norm': grad_norm.item() if config.max_grad_norm > 0 else 0.0,
                'mean_advantage': advantages.mean().item(),
                'mean_reward': rewards_tensor.mean().item(),
                'success_rate': sum(ep['success'] for ep in all_episodes) / len(all_episodes)
            }
            
            training_metrics.append(epoch_metrics)
            
            # è¾“å‡ºè®­ç»ƒè¿›åº¦
            print(f"âœ“ Epoch {epoch + 1} å®Œæˆ:")
            print(f"  æŸå¤±: {epoch_metrics['loss']:.6f}")
            print(f"  æ¢¯åº¦èŒƒæ•°: {epoch_metrics['grad_norm']:.6f}")
            print(f"  å¹³å‡ä¼˜åŠ¿: {epoch_metrics['mean_advantage']:.4f}")
            print(f"  å¹³å‡å¥–åŠ±: {epoch_metrics['mean_reward']:.4f}")
            print(f"  æˆåŠŸç‡: {epoch_metrics['success_rate']:.2f}")
            
        except Exception as e:
            print(f"âŒ Epoch {epoch + 1} è®­ç»ƒå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ç¬¬4æ­¥ï¼šè®­ç»ƒæ€»ç»“
    print(f"\n=== è®­ç»ƒæ€»ç»“ ===")
    print("æ ¸å¿ƒRLè®­ç»ƒå¾ªç¯å®Œæˆï¼")
    print("\nâœ… æˆåŠŸå®ç°çš„å…³é”®åŠŸèƒ½:")
    print("1. âœ“ ä¼˜åŒ–å™¨åˆ›å»ºå’Œç®¡ç†")
    print("2. âœ“ PI0Policyè®­ç»ƒæ¨¡å¼åˆ‡æ¢")
    print("3. âœ“ CFG-styleä¼˜åŠ¿åŠ æƒæŸå¤±è®¡ç®—")
    print("4. âœ“ çœŸå®çš„æ¢¯åº¦æ›´æ–°å¾ªç¯")
    print("5. âœ“ è®­ç»ƒæŒ‡æ ‡ç›‘æ§")
    
    if training_metrics:
        print(f"\nğŸ“Š è®­ç»ƒè½¨è¿¹:")
        print("Epoch | Loss      | GradNorm  | MeanAdv   | MeanRew   | Success")
        print("------|-----------|-----------|-----------|-----------|--------")
        for m in training_metrics:
            print(f"{m['epoch']:5d} | {m['loss']:9.6f} | {m['grad_norm']:9.6f} | "
                  f"{m['mean_advantage']:9.4f} | {m['mean_reward']:9.4f} | {m['success_rate']:7.2f}")
        
        # è®­ç»ƒè¶‹åŠ¿åˆ†æ
        first_loss = training_metrics[0]['loss']
        last_loss = training_metrics[-1]['loss']
        loss_change = ((last_loss - first_loss) / abs(first_loss)) * 100
        
        print(f"\nğŸ“ˆ è®­ç»ƒè¶‹åŠ¿:")
        print(f"  åˆå§‹æŸå¤±: {first_loss:.6f}")
        print(f"  æœ€ç»ˆæŸå¤±: {last_loss:.6f}")
        print(f"  æŸå¤±å˜åŒ–: {loss_change:+.2f}%")
        
        if loss_change < -5:
            print("  ğŸ‰ æŸå¤±æ˜¾è‘—ä¸‹é™ï¼Œè®­ç»ƒæ•ˆæœè‰¯å¥½ï¼")
        elif loss_change < 5:
            print("  ğŸ“Š æŸå¤±ç›¸å¯¹ç¨³å®šï¼Œæ¨¡å‹æ”¶æ•›ä¸­...")
        else:
            print("  âš ï¸ æŸå¤±ä¸Šå‡ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡")
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics_file = f"training_metrics_7_rl_core_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(metrics_file, 'w') as f:
        json.dump({
            'config': config.__dict__,
            'training_metrics': training_metrics,
            'final_summary': {
                'total_episodes': len(all_episodes),
                'mean_reward': rewards_tensor.mean().item(),
                'success_rate': sum(ep['success'] for ep in all_episodes) / len(all_episodes),
                'training_epochs': len(training_metrics)
            }
        }, f, indent=2)
    
    print(f"\nğŸ’¾ è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")
    
    # æ˜¾ç¤ºä¿å­˜çš„è§†é¢‘æ–‡ä»¶
    if config.save_videos:
        print(f"\nğŸ¬ è§†é¢‘æ–‡ä»¶ä¿å­˜åœ¨: {video_dir}")
        video_files = list(video_dir.glob("*.mp4"))
        if video_files:
            print(f"å…±ä¿å­˜äº† {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        else:
            print("æ²¡æœ‰ä¿å­˜è§†é¢‘æ–‡ä»¶")
    
    print(f"\nğŸ¯ ç¬¬1é˜¶æ®µæ ¸å¿ƒRLè®­ç»ƒå®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥å¯ä»¥æ·»åŠ æ›´å¤šè®­ç»ƒè½®æ•°ã€æ‰¹å¤„ç†å’Œé…ç½®ç®¡ç†åŠŸèƒ½")

if __name__ == "__main__":
    main()