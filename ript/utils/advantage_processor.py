"""
ä¼˜åŠ¿å€¼å¤„ç†å™¨
å¤„ç†RIPTè®­ç»ƒä¸­çš„ä¼˜åŠ¿å€¼å½’ä¸€åŒ–ã€æˆªæ–­å’Œæ•°å€¼ç¨³å®šæ€§é—®é¢˜
"""

import torch
import numpy as np
import warnings
from typing import Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum


class AdvantageNormalizationMode(Enum):
    """ä¼˜åŠ¿å½’ä¸€åŒ–æ¨¡å¼"""
    NONE = "none"                    # ä¸å½’ä¸€åŒ–
    ZERO_MEAN = "zero_mean"          # é›¶å‡å€¼å½’ä¸€åŒ– (x - mean)
    STANDARD = "standard"            # æ ‡å‡†å½’ä¸€åŒ– (x - mean) / std
    MIN_MAX = "min_max"              # æœ€å°-æœ€å¤§å½’ä¸€åŒ– (x - min) / (max - min)
    ROBUST = "robust"                # é²æ£’å½’ä¸€åŒ–ï¼Œä½¿ç”¨ä¸­ä½æ•°å’ŒMAD


class AdvantageClippingMode(Enum):
    """ä¼˜åŠ¿æˆªæ–­æ¨¡å¼"""
    NONE = "none"                    # ä¸æˆªæ–­
    SYMMETRIC = "symmetric"          # å¯¹ç§°æˆªæ–­ [-clip, +clip]
    QUANTILE = "quantile"            # åŸºäºåˆ†ä½æ•°æˆªæ–­
    SIGMA = "sigma"                  # åŸºäºæ ‡å‡†å·®æˆªæ–­ [mean Â± n*std]


class NegativeHandlingMode(Enum):
    """è´Ÿå€¼å¤„ç†æ¨¡å¼"""
    KEEP = "keep"                    # ä¿æŒè´Ÿå€¼
    SOFTPLUS = "softplus"            # ä½¿ç”¨softplus: log(1 + exp(x))
    RELU = "relu"                    # ä½¿ç”¨ReLU: max(0, x)
    SHIFT_POSITIVE = "shift"         # å¹³ç§»åˆ°æ­£æ•°åŒºé—´
    EXP = "exp"                      # æŒ‡æ•°å˜æ¢: exp(x)


@dataclass
class AdvantageProcessingConfig:
    """ä¼˜åŠ¿å¤„ç†é…ç½®"""
    # å½’ä¸€åŒ–é…ç½®
    normalization_mode: AdvantageNormalizationMode = AdvantageNormalizationMode.STANDARD
    
    # æˆªæ–­é…ç½®
    clipping_mode: AdvantageClippingMode = AdvantageClippingMode.SYMMETRIC
    clip_value: float = 3.0          # æˆªæ–­é˜ˆå€¼
    quantile_range: Tuple[float, float] = (0.05, 0.95)  # åˆ†ä½æ•°èŒƒå›´
    
    # è´Ÿå€¼å¤„ç†é…ç½®
    negative_handling: NegativeHandlingMode = NegativeHandlingMode.KEEP
    softplus_beta: float = 1.0       # softplusçš„betaå‚æ•°
    shift_epsilon: float = 1e-3      # å¹³ç§»çš„æœ€å°æ­£å€¼
    
    # æ•°å€¼ç¨³å®šæ€§é…ç½®
    epsilon: float = 1e-8            # é¿å…é™¤é›¶çš„å°æ•°
    nan_replacement: float = 0.0     # NaNæ›¿æ¢å€¼
    inf_replacement: float = 1.0     # Infæ›¿æ¢å€¼
    enable_validation: bool = True   # å¯ç”¨è¾“å…¥éªŒè¯
    
    # è°ƒè¯•é…ç½®
    verbose: bool = False            # è¯¦ç»†è¾“å‡º
    track_statistics: bool = True    # è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯


class AdvantageProcessor:
    """
    ä¼˜åŠ¿å€¼å¤„ç†å™¨
    
    åŠŸèƒ½ï¼š
    1. å¤šç§å½’ä¸€åŒ–ç­–ç•¥
    2. æ™ºèƒ½æˆªæ–­æœºåˆ¶
    3. è´Ÿå€¼å¤„ç†ç­–ç•¥
    4. æ•°å€¼ç¨³å®šæ€§ä¿éšœ
    5. ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ª
    """
    
    def __init__(self, config: AdvantageProcessingConfig):
        self.config = config
        
        # ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ª
        self.processing_stats = {
            "total_processed": 0,
            "nan_count": 0,
            "inf_count": 0,
            "negative_count": 0,
            "clipped_count": 0,
            "original_stats": {"mean": [], "std": [], "min": [], "max": []},
            "processed_stats": {"mean": [], "std": [], "min": [], "max": []},
        }
        
        if config.verbose:
            print(f"ğŸ”§ AdvantageProcessor åˆå§‹åŒ–")
            print(f"   - å½’ä¸€åŒ–æ¨¡å¼: {config.normalization_mode.value}")
            print(f"   - æˆªæ–­æ¨¡å¼: {config.clipping_mode.value}")
            print(f"   - è´Ÿå€¼å¤„ç†: {config.negative_handling.value}")
    
    def process_advantages(
        self, 
        advantages: Union[torch.Tensor, np.ndarray],
        batch_info: Optional[Dict[str, Any]] = None
    ) -> torch.Tensor:
        """
        å¤„ç†ä¼˜åŠ¿å€¼ï¼Œåº”ç”¨å½’ä¸€åŒ–ã€æˆªæ–­å’Œè´Ÿå€¼å¤„ç†
        
        Args:
            advantages: åŸå§‹ä¼˜åŠ¿å€¼
            batch_info: æ‰¹æ¬¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
            
        Returns:
            å¤„ç†åçš„ä¼˜åŠ¿å€¼
        """
        # è½¬æ¢ä¸ºtorch.Tensor
        if isinstance(advantages, np.ndarray):
            advantages = torch.tensor(advantages, dtype=torch.float32)
        elif not isinstance(advantages, torch.Tensor):
            advantages = torch.tensor(advantages, dtype=torch.float32)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹ä¸ºæµ®ç‚¹å‹
        if not advantages.is_floating_point():
            warnings.warn(f"ä¼˜åŠ¿å€¼æ•°æ®ç±»å‹ä¸º {advantages.dtype}ï¼Œè‡ªåŠ¨è½¬æ¢ä¸ºfloat32")
            advantages = advantages.float()
        
        # è¾“å…¥éªŒè¯
        if self.config.enable_validation:
            self._validate_input(advantages)
        
        # è®°å½•åŸå§‹ç»Ÿè®¡ä¿¡æ¯
        original_advantages = advantages.clone()
        if self.config.track_statistics:
            self._record_original_stats(original_advantages)
        
        if self.config.verbose:
            print(f"ğŸ”„ å¤„ç†ä¼˜åŠ¿å€¼: shape={advantages.shape}")
            print(f"   åŸå§‹ç»Ÿè®¡: mean={advantages.mean():.4f}, std={advantages.std():.4f}")
            print(f"   åŸå§‹èŒƒå›´: [{advantages.min():.4f}, {advantages.max():.4f}]")
        
        # æ­¥éª¤1: å¤„ç†NaNå’ŒInfå€¼
        advantages = self._handle_invalid_values(advantages)
        
        # æ­¥éª¤2: å½’ä¸€åŒ–
        advantages = self._apply_normalization(advantages)
        
        # æ­¥éª¤3: æˆªæ–­
        advantages = self._apply_clipping(advantages)
        
        # æ­¥éª¤4: è´Ÿå€¼å¤„ç†
        advantages = self._handle_negative_values(advantages)
        
        # æ­¥éª¤5: æœ€ç»ˆéªŒè¯
        advantages = self._final_validation(advantages)
        
        # è®°å½•å¤„ç†åç»Ÿè®¡ä¿¡æ¯
        if self.config.track_statistics:
            self._record_processed_stats(advantages)
            self.processing_stats["total_processed"] += 1
        
        if self.config.verbose:
            print(f"   å¤„ç†åç»Ÿè®¡: mean={advantages.mean():.4f}, std={advantages.std():.4f}")
            print(f"   å¤„ç†åèŒƒå›´: [{advantages.min():.4f}, {advantages.max():.4f}]")
        
        return advantages
    
    def _validate_input(self, advantages: torch.Tensor):
        """éªŒè¯è¾“å…¥ä¼˜åŠ¿å€¼"""
        if advantages.numel() == 0:
            raise ValueError("ä¼˜åŠ¿å€¼å¼ é‡ä¸ºç©º")
        
        if not advantages.dtype.is_floating_point:
            warnings.warn(f"ä¼˜åŠ¿å€¼æ•°æ®ç±»å‹ä¸º {advantages.dtype}ï¼Œå»ºè®®ä½¿ç”¨æµ®ç‚¹ç±»å‹")
    
    def _handle_invalid_values(self, advantages: torch.Tensor) -> torch.Tensor:
        """å¤„ç†NaNå’ŒInfå€¼"""
        nan_mask = torch.isnan(advantages)
        inf_mask = torch.isinf(advantages)
        
        if nan_mask.any():
            nan_count = nan_mask.sum().item()
            self.processing_stats["nan_count"] += nan_count
            advantages = torch.where(nan_mask, self.config.nan_replacement, advantages)
            if self.config.verbose:
                print(f"   âš ï¸ å¤„ç†äº† {nan_count} ä¸ªNaNå€¼")
        
        if inf_mask.any():
            inf_count = inf_mask.sum().item()
            self.processing_stats["inf_count"] += inf_count
            # åŒºåˆ†æ­£æ— ç©·å’Œè´Ÿæ— ç©·
            pos_inf_mask = torch.isposinf(advantages)
            neg_inf_mask = torch.isneginf(advantages)
            advantages = torch.where(pos_inf_mask, self.config.inf_replacement, advantages)
            advantages = torch.where(neg_inf_mask, -self.config.inf_replacement, advantages)
            if self.config.verbose:
                print(f"   âš ï¸ å¤„ç†äº† {inf_count} ä¸ªInfå€¼")
        
        return advantages
    
    def _apply_normalization(self, advantages: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨å½’ä¸€åŒ–"""
        mode = self.config.normalization_mode
        
        if mode == AdvantageNormalizationMode.NONE:
            return advantages
        
        elif mode == AdvantageNormalizationMode.ZERO_MEAN:
            # é›¶å‡å€¼å½’ä¸€åŒ–
            mean_val = advantages.mean()
            normalized = advantages - mean_val
            if self.config.verbose:
                print(f"   ğŸ“Š é›¶å‡å€¼å½’ä¸€åŒ–: mean={mean_val:.4f}")
        
        elif mode == AdvantageNormalizationMode.STANDARD:
            # æ ‡å‡†å½’ä¸€åŒ–
            mean_val = advantages.mean()
            std_val = advantages.std()
            if std_val < self.config.epsilon:
                std_val = torch.tensor(1.0)  # é¿å…é™¤é›¶
                if self.config.verbose:
                    print(f"   âš ï¸ æ ‡å‡†å·®å¤ªå°ï¼Œä½¿ç”¨1.0æ›¿ä»£")
            normalized = (advantages - mean_val) / std_val
            if self.config.verbose:
                print(f"   ğŸ“Š æ ‡å‡†å½’ä¸€åŒ–: mean={mean_val:.4f}, std={std_val:.4f}")
        
        elif mode == AdvantageNormalizationMode.MIN_MAX:
            # æœ€å°-æœ€å¤§å½’ä¸€åŒ–
            min_val = advantages.min()
            max_val = advantages.max()
            if (max_val - min_val) < self.config.epsilon:
                normalized = torch.zeros_like(advantages)  # æ‰€æœ‰å€¼ç›¸ç­‰
                if self.config.verbose:
                    print(f"   âš ï¸ å€¼èŒƒå›´å¤ªå°ï¼Œå½’ä¸€åŒ–ä¸ºé›¶")
            else:
                normalized = (advantages - min_val) / (max_val - min_val)
                if self.config.verbose:
                    print(f"   ğŸ“Š æœ€å°-æœ€å¤§å½’ä¸€åŒ–: range=[{min_val:.4f}, {max_val:.4f}]")
        
        elif mode == AdvantageNormalizationMode.ROBUST:
            # é²æ£’å½’ä¸€åŒ–ï¼ˆåŸºäºä¸­ä½æ•°å’ŒMADï¼‰
            median_val = advantages.median()
            mad = torch.median(torch.abs(advantages - median_val))
            if mad < self.config.epsilon:
                mad = torch.tensor(1.0)
                if self.config.verbose:
                    print(f"   âš ï¸ MADå¤ªå°ï¼Œä½¿ç”¨1.0æ›¿ä»£")
            normalized = (advantages - median_val) / mad
            if self.config.verbose:
                print(f"   ğŸ“Š é²æ£’å½’ä¸€åŒ–: median={median_val:.4f}, MAD={mad:.4f}")
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ¨¡å¼: {mode}")
        
        return normalized
    
    def _apply_clipping(self, advantages: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨æˆªæ–­"""
        mode = self.config.clipping_mode
        
        if mode == AdvantageClippingMode.NONE:
            return advantages
        
        original_range = (advantages.min().item(), advantages.max().item())
        
        if mode == AdvantageClippingMode.SYMMETRIC:
            # å¯¹ç§°æˆªæ–­
            clip_val = self.config.clip_value
            clipped = torch.clamp(advantages, min=-clip_val, max=clip_val)
            
        elif mode == AdvantageClippingMode.QUANTILE:
            # åŸºäºåˆ†ä½æ•°æˆªæ–­
            q_low, q_high = self.config.quantile_range
            low_val = advantages.quantile(q_low)
            high_val = advantages.quantile(q_high)
            clipped = torch.clamp(advantages, min=low_val, max=high_val)
            
        elif mode == AdvantageClippingMode.SIGMA:
            # åŸºäºæ ‡å‡†å·®æˆªæ–­
            mean_val = advantages.mean()
            std_val = advantages.std()
            clip_val = self.config.clip_value * std_val
            clipped = torch.clamp(advantages, min=mean_val - clip_val, max=mean_val + clip_val)
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æˆªæ–­æ¨¡å¼: {mode}")
        
        # ç»Ÿè®¡æˆªæ–­ä¿¡æ¯
        clipped_mask = (clipped != advantages)
        if clipped_mask.any():
            clipped_count = clipped_mask.sum().item()
            self.processing_stats["clipped_count"] += clipped_count
            if self.config.verbose:
                clipped_range = (clipped.min().item(), clipped.max().item())
                print(f"   âœ‚ï¸ æˆªæ–­äº† {clipped_count} ä¸ªå€¼")
                print(f"      åŸå§‹èŒƒå›´: [{original_range[0]:.4f}, {original_range[1]:.4f}]")
                print(f"      æˆªæ–­èŒƒå›´: [{clipped_range[0]:.4f}, {clipped_range[1]:.4f}]")
        
        return clipped
    
    def _handle_negative_values(self, advantages: torch.Tensor) -> torch.Tensor:
        """å¤„ç†è´Ÿå€¼"""
        mode = self.config.negative_handling
        
        negative_mask = advantages < 0
        if negative_mask.any():
            negative_count = negative_mask.sum().item()
            self.processing_stats["negative_count"] += negative_count
            
            if self.config.verbose:
                print(f"   â– æ£€æµ‹åˆ° {negative_count} ä¸ªè´Ÿå€¼")
        
        if mode == NegativeHandlingMode.KEEP:
            return advantages
        
        elif mode == NegativeHandlingMode.SOFTPLUS:
            # ä½¿ç”¨softpluså˜æ¢
            beta = self.config.softplus_beta
            transformed = torch.nn.functional.softplus(advantages, beta=beta)
            if self.config.verbose:
                print(f"   ğŸ”„ åº”ç”¨softpluså˜æ¢ (beta={beta})")
        
        elif mode == NegativeHandlingMode.RELU:
            # ä½¿ç”¨ReLU
            transformed = torch.nn.functional.relu(advantages)
            if self.config.verbose:
                print(f"   ğŸ”„ åº”ç”¨ReLUå˜æ¢")
        
        elif mode == NegativeHandlingMode.SHIFT_POSITIVE:
            # å¹³ç§»åˆ°æ­£æ•°åŒºé—´
            min_val = advantages.min()
            if min_val < 0:
                shift_amount = -min_val + self.config.shift_epsilon
                transformed = advantages + shift_amount
                if self.config.verbose:
                    print(f"   ğŸ”„ å¹³ç§»å˜æ¢: +{shift_amount:.6f}")
            else:
                transformed = advantages
        
        elif mode == NegativeHandlingMode.EXP:
            # æŒ‡æ•°å˜æ¢
            # ä¸ºé¿å…æ•°å€¼æº¢å‡ºï¼Œå…ˆè¿›è¡Œæˆªæ–­
            safe_advantages = torch.clamp(advantages, max=10.0)  # exp(10) â‰ˆ 22000
            transformed = torch.exp(safe_advantages)
            if self.config.verbose:
                print(f"   ğŸ”„ åº”ç”¨æŒ‡æ•°å˜æ¢")
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è´Ÿå€¼å¤„ç†æ¨¡å¼: {mode}")
        
        return transformed
    
    def _final_validation(self, advantages: torch.Tensor) -> torch.Tensor:
        """æœ€ç»ˆéªŒè¯å’Œæ¸…ç†"""
        # æœ€åä¸€æ¬¡NaN/Infæ£€æŸ¥
        if torch.isnan(advantages).any() or torch.isinf(advantages).any():
            advantages = torch.nan_to_num(
                advantages, 
                nan=self.config.nan_replacement,
                posinf=self.config.inf_replacement,
                neginf=-self.config.inf_replacement
            )
            if self.config.verbose:
                print(f"   ğŸ§¹ æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤æ®‹ç•™çš„NaN/Inf")
        
        return advantages
    
    def _record_original_stats(self, advantages: torch.Tensor):
        """è®°å½•åŸå§‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.processing_stats["original_stats"]
        stats["mean"].append(advantages.mean().item())
        stats["std"].append(advantages.std().item())
        stats["min"].append(advantages.min().item())
        stats["max"].append(advantages.max().item())
        
        # ä¿æŒåˆ—è¡¨å¤§å°
        for key in stats:
            if len(stats[key]) > 100:
                stats[key] = stats[key][-50:]
    
    def _record_processed_stats(self, advantages: torch.Tensor):
        """è®°å½•å¤„ç†åç»Ÿè®¡ä¿¡æ¯"""
        stats = self.processing_stats["processed_stats"]
        stats["mean"].append(advantages.mean().item())
        stats["std"].append(advantages.std().item())
        stats["min"].append(advantages.min().item())
        stats["max"].append(advantages.max().item())
        
        # ä¿æŒåˆ—è¡¨å¤§å°
        for key in stats:
            if len(stats[key]) > 100:
                stats[key] = stats[key][-50:]
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.processing_stats.copy()
        
        # è®¡ç®—å¹³å‡ç»Ÿè®¡
        for phase in ["original_stats", "processed_stats"]:
            phase_stats = stats[phase]
            for metric in ["mean", "std", "min", "max"]:
                if phase_stats[metric]:
                    stats[f"{phase}_{metric}_avg"] = np.mean(phase_stats[metric])
                else:
                    stats[f"{phase}_{metric}_avg"] = 0.0
        
        return stats
    
    def print_stats(self):
        """æ‰“å°å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_processing_stats()
        
        print("\nğŸ“Š ä¼˜åŠ¿å€¼å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»å¤„ç†æ¬¡æ•°: {stats['total_processed']}")
        print(f"   NaNå¤„ç†æ¬¡æ•°: {stats['nan_count']}")
        print(f"   Infå¤„ç†æ¬¡æ•°: {stats['inf_count']}")
        print(f"   è´Ÿå€¼å¤„ç†æ¬¡æ•°: {stats['negative_count']}")
        print(f"   æˆªæ–­æ¬¡æ•°: {stats['clipped_count']}")
        
        print(f"\nğŸ“ˆ åŸå§‹ä¼˜åŠ¿å€¼ç»Ÿè®¡:")
        print(f"   å¹³å‡mean: {stats['original_stats_mean_avg']:.4f}")
        print(f"   å¹³å‡std: {stats['original_stats_std_avg']:.4f}")
        
        print(f"\nğŸ“ˆ å¤„ç†åä¼˜åŠ¿å€¼ç»Ÿè®¡:")
        print(f"   å¹³å‡mean: {stats['processed_stats_mean_avg']:.4f}")
        print(f"   å¹³å‡std: {stats['processed_stats_std_avg']:.4f}")
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.processing_stats = {
            "total_processed": 0,
            "nan_count": 0,
            "inf_count": 0,
            "negative_count": 0,
            "clipped_count": 0,
            "original_stats": {"mean": [], "std": [], "min": [], "max": []},
            "processed_stats": {"mean": [], "std": [], "min": [], "max": []},
        }
        if self.config.verbose:
            print("ğŸ”„ ä¼˜åŠ¿å¤„ç†ç»Ÿè®¡å·²é‡ç½®")


# ä¾¿æ·å‡½æ•°
def create_advantage_processor(
    normalization: str = "standard",
    clipping: str = "symmetric",
    clip_value: float = 3.0,
    negative_handling: str = "keep",
    verbose: bool = False
) -> AdvantageProcessor:
    """
    åˆ›å»ºä¼˜åŠ¿å¤„ç†å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        normalization: å½’ä¸€åŒ–æ¨¡å¼
        clipping: æˆªæ–­æ¨¡å¼
        clip_value: æˆªæ–­å€¼
        negative_handling: è´Ÿå€¼å¤„ç†æ¨¡å¼
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        
    Returns:
        é…ç½®å¥½çš„ä¼˜åŠ¿å¤„ç†å™¨
    """
    config = AdvantageProcessingConfig(
        normalization_mode=AdvantageNormalizationMode(normalization),
        clipping_mode=AdvantageClippingMode(clipping),
        clip_value=clip_value,
        negative_handling=NegativeHandlingMode(negative_handling),
        verbose=verbose
    )
    
    return AdvantageProcessor(config)


def process_advantages_batch(
    advantages_list: list,
    processor: Optional[AdvantageProcessor] = None,
    **processor_kwargs
) -> list:
    """
    æ‰¹é‡å¤„ç†ä¼˜åŠ¿å€¼åˆ—è¡¨
    
    Args:
        advantages_list: ä¼˜åŠ¿å€¼åˆ—è¡¨
        processor: ä¼˜åŠ¿å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
        **processor_kwargs: å¤„ç†å™¨åˆ›å»ºå‚æ•°
        
    Returns:
        å¤„ç†åçš„ä¼˜åŠ¿å€¼åˆ—è¡¨
    """
    if processor is None:
        processor = create_advantage_processor(**processor_kwargs)
    
    processed_list = []
    for i, advantages in enumerate(advantages_list):
        processed = processor.process_advantages(advantages, batch_info={"batch_idx": i})
        processed_list.append(processed)
    
    return processed_list
