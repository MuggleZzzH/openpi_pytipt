#!/usr/bin/env python3
"""
ç¬¬11é˜¶æ®µï¼šRIPT-VLA Runneré›†æˆ (11_train_with_ript_vla.py) 
åŸºäº9_train_with_config.pyï¼Œå®Œå…¨é‡æ„é›†æˆRIPT-VLAå¹¶è¡Œç¯å¢ƒrunner

æ ¸å¿ƒç‰¹æ€§ï¼š
1. åŸºäºRIPT-VLAçš„çœŸæ­£å¤šè¿›ç¨‹å¹¶è¡Œç¯å¢ƒ
2. æ™ºèƒ½runneré€‰æ‹©æœºåˆ¶ (åŸæœ‰ vs RIPT-VLA)
3. å®Œå…¨å‘åå…¼å®¹çš„é…ç½®ç³»ç»Ÿ
4. ç®€åŒ–çš„è®­ç»ƒæµç¨‹å’Œé”™è¯¯å¤„ç†
5. å•æ¨¡å‹å¤šç¯å¢ƒæ¶æ„

ä½¿ç”¨æ–¹æ³•:
    cd /zhaohan/ZJH/openpi_pytorch
    
    # ä½¿ç”¨åŸæœ‰runner (å‘åå…¼å®¹)
    python 11_train_with_ript_vla.py --config_path pi0/ript/config/debug_train_pi0.yaml
    
    # ä½¿ç”¨RIPT-VLA runner (æ–°åŠŸèƒ½)
    python 11_train_with_ript_vla.py --config_path pi0/ript/config/debug_train_ript_vla.yaml
    
"""

import os
import sys
import json
import numpy as np
import torch
from pathlib import Path
from datetime import datetime
import argparse
from typing import List, Dict, Any, Tuple, Optional, Union
import yaml
import traceback

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(f"=== ç¬¬11é˜¶æ®µï¼šRIPT-VLA Runneré›†æˆè®­ç»ƒ ===")
print(f"è„šæœ¬ä½ç½®: {current_file}")
print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print()

# å¯¼å…¥é…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ OmegaConfé…ç½®ç®¡ç†å·²å¯ç”¨")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€YAMLåŠ è½½")

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    print("æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
    
    # PI0ç­–ç•¥
    from pi0.modeling_pi0 import PI0Policy
    print("âœ“ PI0ç­–ç•¥æ¨¡å—")
    
    # RIPTç»„ä»¶
    from pi0.ript.reward_function import BinarySuccessReward
    from pi0.ript.algos.rl_optimizers.rl_optimizer_pi0_cfg import RLOptimizerPI0_CFG
    from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter
    from pi0.ript.algos.rl_optimizers.rollout_generator import RolloutGenerator
    print("âœ“ RIPTæ ¸å¿ƒç»„ä»¶")
    
    # ğŸš€ æ™ºèƒ½Runnerå¯¼å…¥
    def import_runners():
        """æ™ºèƒ½å¯¼å…¥runnerç±»"""
        runners = {}
        
        # åŸæœ‰runner
        try:
            from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
            runners['original'] = LIBEROEnvRunner
            print("âœ“ åŸæœ‰LIBEROEnvRunner")
        except ImportError as e:
            print(f"âš ï¸ åŸæœ‰runnerå¯¼å…¥å¤±è´¥: {e}")
            runners['original'] = None
            
        # # RIPT-VLA runner
        # try:
        #     from pi0.ript.env.pi0_libero_runner_ript_vla import PI0LiberoRunner
        #     runners['ript_vla'] = PI0LiberoRunner
        #     print("âœ“ RIPT-VLA Runner")
        # except ImportError as e:
        #     print(f"âš ï¸ RIPT-VLA runnerå¯¼å…¥å¤±è´¥: {e}")
        #     runners['ript_vla'] = None
            
        return runners
    
    RUNNERS = import_runners()
    
    print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥å®Œæˆ")
    
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def load_config(config_path: str, overrides: List[str] = None) -> Dict[str, Any]:
    """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
    print(f"æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    if OMEGACONF_AVAILABLE:
        # ä½¿ç”¨OmegaConf
        config = OmegaConf.load(config_path)
        
        # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
        if overrides:
            for override in overrides:
                key, value = override.split('=', 1)
                # å°è¯•è½¬æ¢æ•°æ®ç±»å‹
                try:
                    if value.lower() in ['true', 'false']:
                        value = value.lower() == 'true'
                    elif '.' in value:
                        value = float(value)
                    else:
                        value = int(value)
                except ValueError:
                    pass  # ä¿æŒå­—ç¬¦ä¸²
                
                OmegaConf.set(config, key, value)
        
        # è½¬æ¢ä¸ºæ™®é€šå­—å…¸ä»¥ä¾¿å…¼å®¹æ€§
        config = OmegaConf.to_yaml(config)
        config = yaml.safe_load(config)
    else:
        # ä½¿ç”¨åŸºç¡€YAML
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    
    # é…ç½®éªŒè¯å’Œé»˜è®¤å€¼
    config = validate_and_set_defaults(config)
    
    print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    return config

def validate_and_set_defaults(config: Dict[str, Any]) -> Dict[str, Any]:
    """éªŒè¯é…ç½®å¹¶è®¾ç½®é»˜è®¤å€¼"""
    
    # å¿…éœ€çš„é…ç½®é¡¹
    required_keys = [
        'policy_path', 'task.benchmark_name', 
        'algo.rloo_batch_size', 'training.num_train_steps'
    ]
    
    for key in required_keys:
        keys = key.split('.')
        current = config
        for k in keys[:-1]:
            if k not in current:
                current[k] = {}
            current = current[k]
        if keys[-1] not in current:
            raise ValueError(f"é…ç½®ç¼ºå°‘å¿…éœ€é¡¹: {key}")
    
    # è®¾ç½®é»˜è®¤å€¼
    defaults = {
        'output_dir': './output',
        'training.seed': 42,
        'task.num_parallel_envs': 1,
        'task.max_episode_length': 200,
        'algo.gradient_accumulation_steps': 1,
        'logging.use_wandb': False,
        'features.use_ript_vla_runner': False,
    }
    
    for key, default_value in defaults.items():
        keys = key.split('.')
        current = config
        for k in keys[:-1]:
            if k not in current:
                current[k] = {}
            current = current[k]
        if keys[-1] not in current:
            current[keys[-1]] = default_value
    
    return config

def create_initial_state_dataset(config: Dict[str, Any]):
    """åˆ›å»ºåˆå§‹çŠ¶æ€æ•°æ®é›†ï¼ˆä¸Stage 9å…¼å®¹ï¼‰"""
    from torch.utils.data import Dataset
    
    class InitialStateDataset(Dataset):
        def __init__(self, config):
            self.config = config
            self.states = []
            
            # è·å–æ•°æ®é›†å¤§å°å’ŒçŠ¶æ€ç»´åº¦
            num_states = config.get('dataset', {}).get('num_init_states', 50)
            state_dim = config.get('dataset', {}).get('state_dim', 8)
            
            np.random.seed(config.get('training', {}).get('seed', 42))
            
            # ç”Ÿæˆå¤šæ ·åŒ–çš„åˆå§‹çŠ¶æ€
            for i in range(num_states):
                base_state = np.zeros(state_dim, dtype=np.float32)
                
                # æ·»åŠ ä¸åŒç¨‹åº¦çš„å™ªå£°ä»¥å¢åŠ å¤šæ ·æ€§
                if i < num_states // 3:
                    noise = np.random.normal(0, 0.05, state_dim).astype(np.float32)
                elif i < 2 * num_states // 3:
                    noise = np.random.normal(0, 0.15, state_dim).astype(np.float32)
                else:
                    noise = np.random.normal(0, 0.25, state_dim).astype(np.float32)
                
                state = base_state + noise
                self.states.append(state)
            
            print(f"âœ“ åˆ›å»ºäº†åŒ…å« {len(self.states)} ä¸ªå¤šæ ·åŒ–åˆå§‹çŠ¶æ€çš„æ•°æ®é›†")
        
        def __len__(self):
            return len(self.states)
        
        def __getitem__(self, idx):
            return torch.from_numpy(self.states[idx]).float()
        
        def sample_batch(self, batch_size=1):
            """å…¼å®¹æ—§ç‰ˆæœ¬çš„é‡‡æ ·æ¥å£"""
            if batch_size > len(self.states):
                indices = np.random.choice(len(self.states), batch_size, replace=True)
            else:
                indices = np.random.choice(len(self.states), batch_size, replace=False)
            
            sampled_states = np.array([self.states[i] for i in indices])
            return sampled_states
    
    return InitialStateDataset(config)

def create_env_runner(config: Dict[str, Any], policy, rank: int = 0, world_size: int = 1):
    """ğŸš€ æ™ºèƒ½runneré€‰æ‹©å‡½æ•°"""
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨RIPT-VLA runner
    use_ript_vla = config.get('features', {}).get('use_ript_vla_runner', False)
    
    if rank == 0:
        print(f"ğŸ” Runneré€‰æ‹©åˆ†æ:")
        print(f"   é…ç½®ä¸­features: {config.get('features', {})}")
        print(f"   use_ript_vla_runner: {use_ript_vla}")
        print(f"   å¯ç”¨runners: {[k for k, v in RUNNERS.items() if v is not None]}")
    
    if use_ript_vla and RUNNERS['ript_vla'] is not None:
        if rank == 0:
            print("ğŸš€ ä½¿ç”¨RIPT-VLAé£æ ¼çš„ç¯å¢ƒrunner")
        
        return RUNNERS['ript_vla'](
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            rank=rank
        )
    
    elif RUNNERS['original'] is not None:
        if rank == 0:
            print("ğŸ”„ ä½¿ç”¨åŸæœ‰çš„ç¯å¢ƒrunner")
        
        # ç¡®ä¿norm_stats_pathå­˜åœ¨
        norm_stats_path = config.get('norm_stats_path')
        if not norm_stats_path:
            # æ¨æ–­é»˜è®¤è·¯å¾„
            policy_path = config['policy_path']
            if '/checkpoints/' in policy_path:
                norm_stats_path = f"{policy_path}/norm_stats.json"
            else:
                norm_stats_path = "./lerobot_dataset/norm_stats.json"
            config['norm_stats_path'] = norm_stats_path
            
        return RUNNERS['original'](
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            norm_stats_path=norm_stats_path,
            config=config,
            rank=rank,
            world_size=world_size
        )
    
    else:
        raise RuntimeError("âŒ æ— å¯ç”¨çš„ç¯å¢ƒrunnerï¼")

def setup_training(config: Dict[str, Any]):
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    
    # è®¾ç½®éšæœºç§å­
    seed = config['training']['seed']
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(config['output_dir'])
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_name = config.get('exp_name', 'ript_vla_train')
    output_dir = output_dir / f"{exp_name}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    config['output_dir'] = str(output_dir)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    return device, output_dir

def load_policy(config: Dict[str, Any], device):
    """åŠ è½½PI0ç­–ç•¥"""
    policy_path = config['policy_path']
    print(f"æ­£åœ¨åŠ è½½ç­–ç•¥: {policy_path}")
    
    if not Path(policy_path).exists():
        raise FileNotFoundError(f"ç­–ç•¥è·¯å¾„ä¸å­˜åœ¨: {policy_path}")
    
    policy = PI0Policy.from_pretrained(policy_path)

    # ğŸ”§ æ ¹æ®é…ç½®æ§åˆ¶CFGåŠŸèƒ½
    policy_config = config.get('policy', {})
    cfg_enabled = policy_config.get('cfg_enabled', True)  # é»˜è®¤å¯ç”¨ä»¥ä¿æŒå…¼å®¹æ€§

    print(f"ğŸ”§ é…ç½®CFGåŠŸèƒ½: {'å¯ç”¨' if cfg_enabled else 'ç¦ç”¨'}")
    policy.model.cfg_enabled = cfg_enabled
    if hasattr(policy, 'config'):
        policy.config.cfg_enabled = cfg_enabled

    policy = policy.to(device)
    policy.eval()

    if cfg_enabled:
        print("âœ… ç­–ç•¥åŠ è½½æˆåŠŸï¼ŒCFGå·²å¯ç”¨")
    else:
        print("âœ… ç­–ç•¥åŠ è½½æˆåŠŸï¼ŒCFGå·²ç¦ç”¨")
    return policy

def create_trainer_components(config: Dict[str, Any], policy, env_runner, device):
    """åˆ›å»ºè®­ç»ƒç»„ä»¶ï¼ˆå®Œæ•´çš„RolloutGeneratorç‰ˆæœ¬ï¼‰"""
    import torch
    from torch.utils.data import DataLoader
    
    # åˆ›å»ºå¥–åŠ±å‡½æ•°
    reward_function = BinarySuccessReward()
    
    # ğŸ”§ åˆ›å»ºCFGé€‚é…å™¨ï¼ˆå¿…éœ€çš„model_adapterï¼‰
    print("æ­£åœ¨åˆ›å»ºCFGé€‚é…å™¨...")

    # ğŸ”¥ Phase 3: SO100æ•°æ®å¤„ç†é…ç½®æ”¯æŒ
    dataset_config = config.get('dataset', {})
    use_so100_processing = dataset_config.get('use_so100_processing', False)

    print(f"æ•°æ®å¤„ç†æ¨¡å¼: {'SO100æ ·æœ¬å¤„ç†' if use_so100_processing else 'Legacyçª—å£åŒ–'}")

    cfg_adapter = PI0_CFG_Adapter(
        policy=policy,
        norm_stats_path=f"{config['policy_path']}/norm_stats.json",
        use_so100_processing=use_so100_processing  # ğŸ”¥ Phase 3: æ–°å¢SO100æ”¯æŒ
    )
    
    # ğŸ”§ ä¸ºpolicyæ·»åŠ ä¼˜åŒ–å™¨ï¼ˆRLOptimizerPI0_CFGéœ€è¦ï¼‰
    print("æ­£åœ¨ä¸ºpolicyé…ç½®ä¼˜åŒ–å™¨...")
    lr = config['algo'].get('lr', 1e-5)
    # ç¡®ä¿å­¦ä¹ ç‡æ˜¯floatç±»å‹
    if isinstance(lr, str):
        lr = float(lr)
    
    optimizer = torch.optim.AdamW(
        policy.parameters(),
        lr=lr,
        weight_decay=0.01
    )
    policy.optimizer = optimizer  # é™„åŠ åˆ°policyä¸Šä¾›RLä¼˜åŒ–å™¨ä½¿ç”¨
    
    # ğŸ”§ åˆ›å»ºåˆå§‹çŠ¶æ€æ•°æ®é›†ï¼ˆä¸Stage 9å…¼å®¹ï¼‰
    print("æ­£åœ¨åˆ›å»ºåˆå§‹çŠ¶æ€æ•°æ®é›†...")
    init_dataset = create_initial_state_dataset(config)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    init_state_dataloader = DataLoader(
        init_dataset,
        batch_size=config['task'].get('num_parallel_envs', 1),
        shuffle=True,
        drop_last=True
    )
    
    # ğŸ”§ æ­£ç¡®åˆå§‹åŒ–RolloutGenerator
    print("æ­£åœ¨åˆ›å»ºRolloutGenerator...")
    rollout_generator = RolloutGenerator(
        env_runner=env_runner,
        rollouts_per_env=1,  # æ¯ä¸ªç¯å¢ƒç”Ÿæˆ1ä¸ªrollout
        num_envs=config['task'].get('num_parallel_envs', 1),
        max_steps=config['task']['max_episode_length'],
        agent_gpus=[0],  # ä½¿ç”¨å•GPU
        init_state_dataloader=init_state_dataloader,
        init_state_dataset=init_dataset,
        enable_dynamic_sampling=config.get('algo', {}).get('enable_dynamic_sampling', True),
        rollout_skip_threshold=config.get('algo', {}).get('rollout_skip_threshold', 3),
        enable_rollout_stats_tracking=config.get('algo', {}).get('enable_rollout_stats_tracking', True),
        rollout_stats_path=config.get('algo', {}).get('rollout_stats_path', './rollout_stats.json'),
        use_val_init=config.get('algo', {}).get('use_val_init', False)
    )
    
    # ğŸ”§ æ­£ç¡®åˆå§‹åŒ–RLOptimizerPI0_CFGï¼ˆä½¿ç”¨æ­£ç¡®çš„å‚æ•°ï¼‰
    print("æ­£åœ¨åˆ›å»ºRLä¼˜åŒ–å™¨...")
    rl_optimizer = RLOptimizerPI0_CFG(
        rollout_generator=rollout_generator,
        reward_function=reward_function,
        num_epochs=5,  # é»˜è®¤epochæ•°
        batch_size=config['algo']['rloo_batch_size'],
        gradient_accumulation_steps=config['algo']['gradient_accumulation_steps'],
        grad_norm_clip=1.0  # é»˜è®¤æ¢¯åº¦è£å‰ª
    )
    
    print("âœ“ è®­ç»ƒç»„ä»¶åˆ›å»ºæˆåŠŸï¼ˆä½¿ç”¨å®Œæ•´RolloutGeneratoré€»è¾‘ï¼‰")
    return cfg_adapter, rl_optimizer

def train_loop(config: Dict[str, Any], cfg_adapter, rl_optimizer, output_dir):
    """ä¸»è®­ç»ƒå¾ªç¯ï¼ˆä½¿ç”¨å®Œæ•´RolloutGeneratoré€»è¾‘ï¼‰"""
    
    num_train_steps = config['training']['num_train_steps']
    log_freq = config.get('logging', {}).get('log_freq', 1)
    save_freq = config['training'].get('save_freq', 10)
    
    print(f"å¼€å§‹è®­ç»ƒ: {num_train_steps} æ­¥")
    print(f"æ—¥å¿—é¢‘ç‡: {log_freq}, ä¿å­˜é¢‘ç‡: {save_freq}")
    print(f"RolloutGeneratoré…ç½®:")
    print(f"  - åŠ¨æ€é‡‡æ ·: {rl_optimizer.rollout_generator.enable_dynamic_sampling}")
    print(f"  - ç»Ÿè®¡è·Ÿè¸ª: {rl_optimizer.rollout_generator.enable_rollout_stats_tracking}")
    print(f"  - è·³è¿‡é˜ˆå€¼: {rl_optimizer.rollout_generator.rollout_skip_threshold}")
    
    all_training_metrics = []
    
    for step in range(num_train_steps):
        try:
            print(f"\n=== è®­ç»ƒæ­¥éª¤ {step + 1}/{num_train_steps} ===")
            
            # ğŸ”§ ä½¿ç”¨RLOptimizerPI0_CFGçš„train_on_rolloutsæ–¹æ³•
            print("æ­£åœ¨æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå‘¨æœŸï¼ˆrollout + ä¼˜åŠ¿è®¡ç®— + ä¼˜åŒ–ï¼‰...")
            
            training_metrics = rl_optimizer.train_on_rollouts(
                model_adapter=cfg_adapter,
                lr=config['algo'].get('lr', 1e-5),
                scaler=None,  # æš‚ä¸ä½¿ç”¨æ··åˆç²¾åº¦
                use_amp=False  # æš‚ä¸ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
            )
            
            if not training_metrics:
                print("âš ï¸ è®­ç»ƒæœªè¿”å›æŒ‡æ ‡ï¼Œè·³è¿‡æ­¤æ­¥")
                continue
            
            print(f"è®­ç»ƒæŒ‡æ ‡: {training_metrics}")
            
            # æå–å…³é”®æŒ‡æ ‡
            step_metrics = {
                'step': step + 1,
                **training_metrics  # åŒ…å«æ‰€æœ‰è¿”å›çš„æŒ‡æ ‡
            }
            all_training_metrics.append(step_metrics)
            
            # æ—¥å¿—è®°å½•
            if (step + 1) % log_freq == 0:
                print(f"\nğŸ“Š æ­¥éª¤ {step + 1} è®­ç»ƒæŒ‡æ ‡:")
                for key, value in step_metrics.items():
                    if key != 'step':
                        print(f"  {key}: {value}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (step + 1) % save_freq == 0:
                checkpoint_path = output_dir / f"checkpoint_step_{step + 1}.pt"
                policy = cfg_adapter.get_policy_model()
                torch.save({
                    'step': step + 1,
                    'policy_state_dict': policy.state_dict(),
                    'optimizer_state_dict': policy.optimizer.state_dict(),
                    'config': config,
                    'training_metrics': all_training_metrics,
                }, checkpoint_path)
                
                print(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        except KeyboardInterrupt:
            print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"âŒ è®­ç»ƒæ­¥éª¤ {step + 1} å‡ºé”™: {e}")
            traceback.print_exc()
            continue
    
    # ä¿å­˜æœ€ç»ˆè®­ç»ƒç»“æœ
    final_results_path = output_dir / "final_training_results.json"
    with open(final_results_path, 'w') as f:
        json.dump({
            'config': config,
            'training_metrics': all_training_metrics,
            'total_steps': len(all_training_metrics)
        }, f, indent=2)
    
    print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š å®Œæ•´è®­ç»ƒç»“æœå·²ä¿å­˜: {final_results_path}")
    print(f"ğŸ”§ ä½¿ç”¨äº†å®Œæ•´çš„RolloutGeneratoré€»è¾‘ï¼ŒåŒ…æ‹¬æ™ºèƒ½é‡‡æ ·å’Œç»Ÿè®¡è·Ÿè¸ª")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç¬¬11é˜¶æ®µï¼šRIPT-VLA Runneré›†æˆè®­ç»ƒ")
    parser.add_argument(
        "--config_path", 
        type=str, 
        required=True,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--override",
        type=str,
        nargs="*",
        default=[],
        help="é…ç½®è¦†ç›– (æ ¼å¼: key=value)"
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config_path, args.override)
        
        # æ˜¾ç¤ºé…ç½®
        print("\n====== ä½¿ç”¨é…ç½® ======")
        print(yaml.dump(config, default_flow_style=False, allow_unicode=True))
        print("====================\n")
        
        # è®¾ç½®è®­ç»ƒç¯å¢ƒ
        device, output_dir = setup_training(config)
        
        # åŠ è½½ç­–ç•¥
        policy = load_policy(config, device)
        
        # åˆ›å»ºç¯å¢ƒrunner
        env_runner = create_env_runner(config, policy, rank=0, world_size=1)
        
        # åˆ›å»ºè®­ç»ƒç»„ä»¶
        cfg_adapter, rl_optimizer = create_trainer_components(
            config, policy, env_runner, device
        )
        
        # å¼€å§‹è®­ç»ƒ
        train_loop(config, cfg_adapter, rl_optimizer, output_dir)
        
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()