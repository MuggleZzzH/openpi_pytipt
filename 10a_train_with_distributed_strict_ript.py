#!/usr/bin/env python3
"""
ä¸¥æ ¼æŒ‰ç…§RIPT-VLAåŸç‰ˆé€»è¾‘çš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ

ğŸ”¥ æ ¸å¿ƒåŸç‰ˆé€»è¾‘ï¼š
1. ä»»åŠ¡åˆ†é…ï¼šæ¯ä¸ªGPUåˆ†é…å›ºå®šä»»åŠ¡åˆ—è¡¨ï¼Œè®­ç»ƒæœŸé—´ä¸è½®è¯¢
2. æ•°æ®é©±åŠ¨ï¼šé€šè¿‡dataloaderçš„task_idå­—æ®µç¡®å®šå¤„ç†å“ªä¸ªä»»åŠ¡
3. éšæœºé‡‡æ ·ï¼šä»dataloaderä¸­éšæœºè·å–åˆå§‹çŠ¶æ€ï¼Œæ— ç¯å½¢ç´¢å¼•
4. Rolloutç”Ÿæˆï¼šåŸºäºRolloutGenerator.generate_rollouts()

åŸºäº /zhaohan/ZJH/ript-vla/train_ript_openvla_oft.py çš„ä¸¥æ ¼ç§»æ¤
"""

import os
import sys
import json
import numpy as np
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from pathlib import Path
from datetime import datetime, timedelta
import argparse
import hashlib
from typing import List, Dict, Any, Tuple, Optional, Union
import yaml
import time
import gc
from tqdm import tqdm

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["NCCL_TIMEOUT"] = "108000"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥OmegaConfç”¨äºé…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ ä½¿ç”¨OmegaConfè¿›è¡Œé…ç½®ç®¡ç†")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfæœªå®‰è£…ï¼Œå›é€€åˆ°åŸºç¡€YAMLè§£æ")

from pi0 import PI0Policy
from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
from pi0.ript.reward_function import BinarySuccessReward
from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter

def collate_fn_state(batch):
    """å¤„ç†æ‰¹æ¬¡æ•°æ®ï¼Œä¸RIPT-VLAåŸç‰ˆä¿æŒä¸€è‡´"""
    from torch.utils.data.dataloader import default_collate
    
    # Process the special case first
    states = [item['init_state']['states'] for item in batch]
    max_len = max(s.shape[-1] for s in states)
    
    padded_states = []
    masks = []
    modified_batch = []
    
    for item in batch:
        # Pad states and create mask
        tensor = torch.as_tensor(item['init_state']['states']).float()
        pad_size = max_len - tensor.shape[-1]
        padded = torch.nn.functional.pad(tensor, (0, pad_size))
        padded_states.append(padded)
        
        mask = torch.ones(tensor.shape[-1], dtype=torch.bool)
        mask = torch.nn.functional.pad(mask, (0, pad_size), value=False)
        masks.append(mask)
        
        # Create a modified item without the special field
        modified_item = {key: item[key] for key in item.keys() if key != 'init_state'}
        modified_batch.append(modified_item)

    # Collate all other fields normally
    collated_batch = default_collate(modified_batch)
    
    # Add our processed states and mask back in
    collated_batch['init_state'] = {}
    collated_batch['init_state']['states'] = torch.stack(padded_states)
    collated_batch['init_state']['pad_mask'] = torch.stack(masks)
    
    return collated_batch

class StrictRIPTDataset:
    """ä¸¥æ ¼æŒ‰ç…§RIPT-VLAåŸç‰ˆçš„æ•°æ®é›†å®ç°"""
    
    def __init__(self, task_names_to_use, init_states_per_task=50):
        self.task_names_to_use = task_names_to_use
        self.init_states_per_task = init_states_per_task
        
        # ç”Ÿæˆæ¨¡æ‹Ÿåˆå§‹çŠ¶æ€æ•°æ®ï¼ˆå®é™…åº”è¯¥ä»LIBERO benchmarkåŠ è½½ï¼‰
        self.data = []
        for task_idx, task_name in enumerate(task_names_to_use):
            for state_idx in range(init_states_per_task):
                # æ¨¡æ‹Ÿåˆå§‹çŠ¶æ€ - å®é™…åº”è¯¥ä»LIBEROæ•°æ®é›†åŠ è½½
                state_data = np.random.randn(10).astype(np.float32)  # æ¨¡æ‹Ÿ10ç»´çŠ¶æ€
                
                sample = {
                    'task_id': torch.tensor(task_idx, dtype=torch.long),
                    'task_name': task_name,
                    'init_state': {
                        'states': torch.tensor(state_data, dtype=torch.float32).unsqueeze(0),
                    }
                }
                self.data.append(sample)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

class StrictRIPTRolloutGenerator:
    """ä¸¥æ ¼æŒ‰ç…§RIPT-VLAåŸç‰ˆRolloutGeneratoré€»è¾‘çš„å®ç°"""
    
    def __init__(
        self, 
        env_runner, 
        task_names_to_use, 
        rloo_batch_size=2,
        demo_batch_size=1,
        enable_rollout_stats_tracking=False
    ):
        self.env_runner = env_runner
        self.task_names_to_use = task_names_to_use
        self.rloo_batch_size = rloo_batch_size
        self.demo_batch_size = demo_batch_size
        self.enable_rollout_stats_tracking = enable_rollout_stats_tracking
        self.rollout_stats = {}
        self.rollout_skip_cnt = {}
        self.rollout_skip_threshold = 3
    
    def compute_hash_from_state(self, state, bidx):
        """è®¡ç®—çŠ¶æ€å“ˆå¸Œå€¼"""
        state_data = state['states'][bidx][0]
        state_mask = state['pad_mask'][bidx] if 'pad_mask' in state else torch.ones_like(state_data, dtype=torch.bool)
        return hashlib.sha256(state_data[state_mask].cpu().numpy().tobytes()).hexdigest()[:8]
    
    def generate_rollouts(self, model_adapter, batch, data_iterator, dataloader):
        """
        ğŸ”¥ ä¸¥æ ¼æŒ‰ç…§RIPT-VLAåŸç‰ˆé€»è¾‘ç”Ÿæˆrollouts
        
        å…³é”®åŸç‰ˆé€»è¾‘ï¼š
        1. ä»batchä¸­æå–task_idç¡®å®šä»»åŠ¡
        2. åŸºäºbatchä¸­çš„init_stateè¿›è¡Œrollout
        3. ä½¿ç”¨RLOOæ‰¹æ¬¡å¤§å°å¤åˆ¶çŠ¶æ€
        4. è¿”å›episodesåˆ—è¡¨ç”¨äºåç»­RLOOè®¡ç®—
        """
        rank = dist.get_rank() if dist.is_initialized() else 0
        
        all_episodes = []
        all_task_ids = []
        
        # åŸç‰ˆé€»è¾‘ï¼šå¤„ç†batchä¸­çš„æ¯ä¸ªæ ·æœ¬
        demo_batch_size = batch['task_id'].shape[0]
        
        print(f"ğŸ”„ Rank {rank} å¼€å§‹ç”Ÿæˆrollouts: {demo_batch_size} ä¸ªæ ·æœ¬")
        
        for batch_idx in range(demo_batch_size):
            # ğŸ”¥ åŸç‰ˆé€»è¾‘ï¼šä»batchä¸­æå–ä»»åŠ¡ä¿¡æ¯
            sample_task_id = batch['task_id'][batch_idx].item()
            task_name = self.task_names_to_use[sample_task_id]
            
            print(f"   å¤„ç†æ ·æœ¬ {batch_idx}: ä»»åŠ¡ {task_name} (ID: {sample_task_id})")
            
            # ğŸ”¥ åŸç‰ˆé€»è¾‘ï¼šä»batchä¸­æå–åˆå§‹çŠ¶æ€
            sample_states = batch['init_state']
            if 'pad_mask' in sample_states:
                init_state = sample_states['states'][batch_idx, 0][sample_states['pad_mask'][batch_idx]]
            else:
                init_state = sample_states['states'][batch_idx, 0]
            
            # ğŸ”¥ åŸç‰ˆé€»è¾‘ï¼šå¤åˆ¶çŠ¶æ€åˆ°RLOOæ‰¹æ¬¡å¤§å°
            env_init_states = init_state.unsqueeze(0).repeat(self.rloo_batch_size, 1).cpu().numpy()
            
            # è®¡ç®—çŠ¶æ€å“ˆå¸Œç”¨äºç»Ÿè®¡è·Ÿè¸ª
            init_hash = self.compute_hash_from_state(batch['init_state'], batch_idx)
            
            # ğŸ”¥ åŸç‰ˆé€»è¾‘ï¼šè·³è¿‡å…¨æˆåŠŸçš„çŠ¶æ€ï¼ˆå¦‚æœå¯ç”¨ç»Ÿè®¡è·Ÿè¸ªï¼‰
            if self.enable_rollout_stats_tracking and init_hash in self.rollout_stats:
                recent_successes = self.rollout_stats[init_hash][-self.rloo_batch_size:]
                if len(recent_successes) >= self.rloo_batch_size and all(s == 1 for s in recent_successes):
                    print(f"   è·³è¿‡å…¨æˆåŠŸçŠ¶æ€: {init_hash}")
                    self.rollout_skip_cnt[init_hash] += 1
                    if self.rollout_skip_cnt[init_hash] > self.rollout_skip_threshold:
                        del self.rollout_stats[init_hash]
                    continue
            
            print(f"   ğŸ¯ è¿è¡Œ {self.rloo_batch_size} ä¸ªrollouts for ä»»åŠ¡: {task_name}")
            
            # ğŸ”¥ ä½¿ç”¨env_runnerè¿è¡Œç­–ç•¥æ”¶é›†è½¨è¿¹
            try:
                rollout_results = self.env_runner.run_policy_in_env(
                    env_name=task_name,
                    all_init_states=env_init_states,
                    debug_save_video=False
                )
                
                # æ”¶é›†rolloutç»“æœ
                batch_episodes = []
                batch_successes = []
                
                for success, total_reward, episode_data in rollout_results:
                    episode = {
                        'success': success,
                        'total_reward': total_reward,
                        'task_name': task_name,
                        'task_id': sample_task_id,
                        'init_hash': init_hash,
                        **episode_data
                    }
                    batch_episodes.append(episode)
                    batch_successes.append(1 if success else 0)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if self.enable_rollout_stats_tracking:
                    if init_hash not in self.rollout_stats:
                        self.rollout_stats[init_hash] = []
                        self.rollout_skip_cnt[init_hash] = 0
                    self.rollout_stats[init_hash].extend(batch_successes)
                
                all_episodes.extend(batch_episodes)
                all_task_ids.extend([sample_task_id] * len(batch_episodes))
                
                success_count = sum(batch_successes)
                print(f"   âœ“ æ”¶é›†åˆ° {len(batch_episodes)} ä¸ªepisodes, æˆåŠŸç‡: {success_count}/{len(batch_episodes)}")
                
            except Exception as e:
                print(f"   âŒ Rolloutå¤±è´¥: {e}")
                continue
        
        print(f"ğŸ‰ Rank {rank} æ€»å…±æ”¶é›† {len(all_episodes)} ä¸ªepisodes")
        
        # ğŸ”¥ åŸç‰ˆè¿”å›æ ¼å¼ï¼š(episodes, task_ids, valid_mask, samples_checked)
        valid_mask = torch.ones(len(all_episodes), dtype=torch.bool)
        
        return all_episodes, all_task_ids, valid_mask, len(all_episodes)

class StrictRIPTPI0DistributedTrainer:
    """ä¸¥æ ¼æŒ‰ç…§RIPT-VLAåŸç‰ˆé€»è¾‘çš„PI0åˆ†å¸ƒå¼è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        self._init_distributed()
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        self.rank = dist.get_rank()
        self.world_size = dist.get_world_size()
        self.device_id = self.rank % torch.cuda.device_count()
        self.device = f'cuda:{self.device_id}'
        torch.cuda.set_device(self.device)
        
        if self.rank == 0:
            print(f"ğŸš€ ä¸¥æ ¼RIPT-VLAé€»è¾‘åˆ†å¸ƒå¼è®­ç»ƒ")
            print(f"   World size: {self.world_size}")
            print(f"   Config: {config_path}")
    
    def _init_distributed(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
        dist.init_process_group(
            backend='nccl', 
            timeout=timedelta(seconds=10800)
        )
    
    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            if OMEGACONF_AVAILABLE:
                config = OmegaConf.load(f)
                return OmegaConf.to_container(config, resolve=True)
            else:
                return yaml.safe_load(f)
    
    def _split_tasks_across_gpus(self, all_tasks: List[str]) -> List[str]:
        """ğŸ”¥ ä¸¥æ ¼æŒ‰ç…§RIPT-VLAåŸç‰ˆä»»åŠ¡åˆ†é…é€»è¾‘"""
        # åŸç‰ˆä»£ç ï¼štrain_ript_openvla_oft.py:88-91
        rank_to_tasks = {rank_i: [] for rank_i in range(self.world_size)}
        for task_i, task_name in enumerate(all_tasks):
            rank_to_tasks[task_i % self.world_size].append(task_name)
        
        local_tasks = rank_to_tasks[self.rank]
        
        if self.rank == 0:
            print(f"ğŸ“‹ ä»»åŠ¡åˆ†é… (ä¸¥æ ¼åŸç‰ˆé€»è¾‘):")
            for rank_i in range(self.world_size):
                print(f"   Rank {rank_i}: {rank_to_tasks[rank_i]}")
        
        print(f"ğŸ¯ Rank {self.rank} åˆ†é…ä»»åŠ¡: {local_tasks}")
        return local_tasks
    
    def train(self):
        """ğŸ”¥ ä¸¥æ ¼æŒ‰ç…§RIPT-VLAåŸç‰ˆè®­ç»ƒå¾ªç¯"""
        
        # ğŸ”¥ æ­¥éª¤1ï¼šä»»åŠ¡åˆ†é… (åŸç‰ˆé€»è¾‘)
        all_tasks = self.config['task'].get('task_names_to_use', [
            "LIBERO_SPATIAL_pick_the_black_bowl_on_table_center_and_place_it_on_the_plate"
        ])
        local_tasks = self._split_tasks_across_gpus(all_tasks)
        
        if not local_tasks:
            print(f"âš ï¸ Rank {self.rank} æ²¡æœ‰åˆ†é…ä»»åŠ¡ï¼Œé€€å‡ºè®­ç»ƒ")
            return
        
        # ğŸ”¥ æ­¥éª¤2ï¼šåˆå§‹åŒ–æ¨¡å‹ (PI0ç­–ç•¥)
        model_adapter = self._setup_model()
        
        # ğŸ”¥ æ­¥éª¤3ï¼šåˆå§‹åŒ–æ•°æ®é›†å’Œdataloader (åŸç‰ˆé€»è¾‘)
        dataset = StrictRIPTDataset(
            task_names_to_use=local_tasks,
            init_states_per_task=20
        )
        
        # ğŸ”¥ åŸç‰ˆé€»è¾‘ï¼šä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨
        train_dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.config['algo']['data_batch_size'] // self.world_size,
            sampler=DistributedSampler(dataset),
            collate_fn=collate_fn_state,
            num_workers=0,
            pin_memory=True
        )
        
        # ğŸ”¥ æ­¥éª¤4ï¼šåˆå§‹åŒ–ç¯å¢ƒrunner
        env_runner = LIBEROEnvRunner(
            policy_model=model_adapter.get_policy_model(),
            task_names_to_use=local_tasks,
            num_parallel_envs=self.config['task']['num_parallel_envs'],
            rank=self.rank,
            world_size=self.world_size
        )
        
        # ğŸ”¥ æ­¥éª¤5ï¼šåˆå§‹åŒ–rollout generator (ä¸¥æ ¼åŸç‰ˆé€»è¾‘)
        rollout_generator = StrictRIPTRolloutGenerator(
            env_runner=env_runner,
            task_names_to_use=local_tasks,
            rloo_batch_size=self.config['algo']['rloo_batch_size'],
            demo_batch_size=self.config['algo']['data_batch_size'] // self.world_size,
            enable_rollout_stats_tracking=self.config['algo'].get('enable_rollout_stats_tracking', False)
        )
        
        # ğŸ”¥ æ­¥éª¤6ï¼šåˆå§‹åŒ–RLä¼˜åŒ–å™¨å’Œå¥–åŠ±å‡½æ•°
        reward_function = BinarySuccessReward()
        
        # ğŸ”¥ æ­¥éª¤7ï¼šä¸¥æ ¼æŒ‰ç…§åŸç‰ˆè®­ç»ƒå¾ªç¯
        print(f"ğŸ”„ å¼€å§‹è®­ç»ƒå¾ªç¯ (Rank {self.rank})")
        
        data_iter = iter(train_dataloader)
        total_steps = self.config['training']['num_train_steps']
        
        for global_step in tqdm(range(total_steps), desc=f'Strict RIPT Training (Rank {self.rank})'):
            
            # ğŸ”¥ åŸç‰ˆé€»è¾‘ï¼šè·å–æ•°æ®æ‰¹æ¬¡
            try:
                batch = next(data_iter)
            except StopIteration:
                data_iter = iter(train_dataloader)
                batch = next(data_iter)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch = self._map_to_device(batch)
            
            if self.rank == 0:
                print(f"\\nğŸ”„ è®­ç»ƒæ­¥éª¤ {global_step + 1}/{total_steps}")
                print(f"   æ‰¹æ¬¡å¤§å°: {batch['task_id'].shape[0]}")
                print(f"   ä»»åŠ¡IDs: {batch['task_id'].tolist()}")
            
            # ğŸ”¥ æ ¸å¿ƒï¼šç”Ÿæˆrollouts (ä¸¥æ ¼åŸç‰ˆé€»è¾‘)
            all_episodes, all_task_ids, valid_mask, samples_checked = rollout_generator.generate_rollouts(
                model_adapter, batch, data_iter, train_dataloader
            )
            
            if not all_episodes:
                print(f"âš ï¸ Rank {self.rank} æ­¥éª¤ {global_step + 1}: æœªæ”¶é›†åˆ°æœ‰æ•ˆepisodesï¼Œè·³è¿‡")
                continue
            
            # ğŸ”¥ è®¡ç®—å¥–åŠ±å’Œä¼˜åŠ¿ (RLOOé€»è¾‘)
            rewards = self._compute_rloo_advantages(all_episodes, reward_function)
            
            # ğŸ”¥ ç­–ç•¥ä¼˜åŒ– (ç®€åŒ–ç‰ˆPPO)
            loss_metrics = self._optimize_policy(model_adapter, all_episodes, rewards)
            
            # ç»Ÿè®¡å’Œæ—¥å¿—
            if self.rank == 0:
                success_count = sum(1 for ep in all_episodes if ep.get('success', False))
                print(f"   âœ“ æ”¶é›† {len(all_episodes)} episodes, æˆåŠŸç‡: {success_count}/{len(all_episodes)}")
                print(f"   ğŸ“Š æŸå¤±: {loss_metrics.get('loss', 0.0):.6f}")
        
        if self.rank == 0:
            print(f"ğŸ‰ ä¸¥æ ¼RIPT-VLAè®­ç»ƒå®Œæˆ!")
    
    def _setup_model(self):
        """åˆå§‹åŒ–PI0æ¨¡å‹"""
        policy_path = self.config['policy_path']
        policy = PI0Policy.from_pretrained(policy_path)
        policy = policy.to(self.device)
        
        # ä½¿ç”¨DDPåŒ…è£…
        if self.world_size > 1:
            policy = DDP(policy, device_ids=[self.device_id])
        
        # åˆ›å»ºé€‚é…å™¨
        model_adapter = PI0_CFG_Adapter(policy, device=self.device)
        
        return model_adapter
    
    def _map_to_device(self, batch):
        """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡"""
        def _map_tensor(obj):
            if torch.is_tensor(obj):
                return obj.to(self.device)
            elif isinstance(obj, dict):
                return {k: _map_tensor(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [_map_tensor(item) for item in obj]
            else:
                return obj
        
        return _map_tensor(batch)
    
    def _compute_rloo_advantages(self, episodes, reward_function):
        """è®¡ç®—RLOOä¼˜åŠ¿"""
        # ç®€åŒ–ç‰ˆRLOOè®¡ç®—
        scores = []
        for i, episode in enumerate(episodes):
            reward = reward_function.compute_reward(episode)
            scores.append(reward)
        
        scores = torch.tensor(scores, device=self.device, dtype=torch.float32)
        
        # Leave-One-Out baseline
        batch_size = len(episodes)
        if batch_size > 1:
            total_sum = scores.sum()
            baseline = (total_sum.unsqueeze(0) - scores) / (batch_size - 1)
            advantages = scores - baseline
        else:
            advantages = scores
        
        return advantages
    
    def _optimize_policy(self, model_adapter, episodes, advantages):
        """ä¼˜åŒ–ç­–ç•¥ (ç®€åŒ–ç‰ˆ)"""
        # è¿™é‡Œåº”è¯¥å®ç°å®Œæ•´çš„PPOä¼˜åŒ–é€»è¾‘
        # ä¸ºäº†æ¼”ç¤ºï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        
        policy = model_adapter.get_policy_model()
        
        # ç®€å•çš„ç›‘ç£å­¦ä¹ æŸå¤±ä½œä¸ºæ¼”ç¤º
        total_loss = 0.0
        
        for i, (episode, advantage) in enumerate(zip(episodes, advantages)):
            if advantage > 0:  # åªå¯¹æ­£ä¼˜åŠ¿çš„episodeè¿›è¡Œå­¦ä¹ 
                # è¿™é‡Œåº”è¯¥å®ç°å®Œæ•´çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°
                # å½“å‰ä»…ä½œæ¼”ç¤º
                total_loss += advantage.item()
        
        avg_loss = total_loss / len(episodes) if episodes else 0.0
        
        return {'loss': avg_loss}

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¸¥æ ¼RIPT-VLAé€»è¾‘çš„PI0åˆ†å¸ƒå¼è®­ç»ƒ')
    parser.add_argument('--config_path', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    trainer = StrictRIPTPI0DistributedTrainer(args.config_path)
    trainer.train()

if __name__ == "__main__":
    main()