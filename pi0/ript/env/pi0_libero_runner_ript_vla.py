"""
PI0 + LIBERO ç¯å¢ƒè¿è¡Œå™¨ - åŸºäºRIPT-VLAçš„æˆåŠŸå®ç°
ç›´æ¥å¤ç”¨ /zhaohan/ZJH/ript-vla/ript/env_runner/openvla_oft_libero_runner.py çš„æ ¸å¿ƒé€»è¾‘
"""
import os
import numpy as np
import gc
import multiprocessing
from collections import deque
from pathlib import Path
import torch
from libero.libero import benchmark
from libero.libero.envs import OffScreenRenderEnv, SubprocVectorEnv
import robosuite.utils.transform_utils as T
from typing import List, Dict, Any


class BatchProcessingWrapper:
    """
    æ‰¹å¤„ç†ç¯å¢ƒåŒ…è£…å™¨
    åœ¨ä¸»è¿›ç¨‹ä¸­ä½¿ç”¨å•ä¸ªç¯å¢ƒï¼Œé€šè¿‡é¡ºåºæ‰§è¡Œæ¨¡æ‹Ÿå¹¶è¡Œç¯å¢ƒ
    é¿å…SubprocVectorEnvçš„æ¨¡å‹å¤åˆ¶é—®é¢˜
    """
    
    def __init__(self, single_env, num_parallel_envs):
        self.single_env = single_env
        self.num_parallel_envs = num_parallel_envs
        self.current_states = [None] * num_parallel_envs
        
    def reset(self):
        """é‡ç½®æ‰€æœ‰å¹¶è¡Œç¯å¢ƒ"""
        # åªé‡ç½®ä¸€æ¬¡çœŸå®ç¯å¢ƒï¼Œç„¶åå¤åˆ¶çŠ¶æ€
        obs = self.single_env.reset()
        # ä¸ºæ¯ä¸ªæ¨¡æ‹Ÿç¯å¢ƒä¿å­˜ç›¸åŒçš„åˆå§‹çŠ¶æ€
        self.current_states = [obs] * self.num_parallel_envs
        return self.current_states
    
    def set_init_state(self, init_states):
        """è®¾ç½®åˆå§‹çŠ¶æ€"""
        # ä¸ºæ¯ä¸ªç¯å¢ƒè®¾ç½®ä¸åŒçš„åˆå§‹çŠ¶æ€
        if len(init_states) >= self.num_parallel_envs:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªçŠ¶æ€è®¾ç½®ç¯å¢ƒ
            self.single_env.set_init_state(init_states[0])
        
    def step(self, actions):
        """æ‰¹å¤„ç†æ‰§è¡ŒåŠ¨ä½œ"""
        batch_obs = []
        batch_rewards = []
        batch_dones = []
        batch_infos = []
        
        # é¡ºåºæ‰§è¡Œæ¯ä¸ªç¯å¢ƒçš„åŠ¨ä½œ
        for i, action in enumerate(actions):
            if i < self.num_parallel_envs:
                # æ‰§è¡Œç¬¬iä¸ªåŠ¨ä½œ
                obs, reward, done, info = self.single_env.step(action)
                batch_obs.append(obs)
                batch_rewards.append(reward)
                batch_dones.append(done)
                batch_infos.append(info)
            else:
                # å¦‚æœåŠ¨ä½œæ•°é‡è¶…è¿‡ç¯å¢ƒæ•°ï¼Œå¤åˆ¶æœ€åç»“æœ
                batch_obs.append(batch_obs[-1] if batch_obs else {})
                batch_rewards.append(0.0)
                batch_dones.append(False)
                batch_infos.append({})
        
        return batch_obs, batch_rewards, batch_dones, batch_infos
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if hasattr(self.single_env, 'close'):
            self.single_env.close()

class PI0LiberoRunner:
    """PI0 + LIBERO ç¯å¢ƒè¿è¡Œå™¨ - ç›´æ¥å¤ç”¨RIPT-VLAçš„æˆåŠŸæ¨¡å¼"""
    
    def __init__(self,
                 policy,
                 benchmark_name,
                 rollouts_per_env,
                 num_parallel_envs,
                 max_episode_length=None,
                 task_names_to_use=[],
                 rank=0):
        self.policy = policy
        self.benchmark_name = benchmark_name.lower()
        self.rollouts_per_env = rollouts_per_env  
        self.num_parallel_envs = num_parallel_envs
        self.rank = rank
        
        # LIBERO benchmark setup
        benchmark_dict = benchmark.get_benchmark_dict()
        self.benchmark = benchmark_dict[benchmark_name.lower()]()
        self.env_names = self.benchmark.get_task_names()
        
        if task_names_to_use is not None and len(task_names_to_use) > 0:
            self.env_names_to_run = [name for name in self.env_names if name in task_names_to_use]
        else:
            self.env_names_to_run = self.env_names
            
        # è®¾ç½®multiprocessingæ–¹æ³• - å…³é”®ï¼
        if num_parallel_envs > 1:
            if multiprocessing.get_start_method(allow_none=True) != "spawn":  
                multiprocessing.set_start_method("spawn", force=True)
                
        # Episode length
        task_max_steps = {
            "libero_spatial": 220,
            "libero_object": 280, 
            "libero_goal": 300,
            "libero_10": 520,
            "libero_90": 400,
        }
        
        if max_episode_length is None:
            self.max_episode_length = task_max_steps.get(benchmark_name.lower(), 300)
        else:
            self.max_episode_length = max_episode_length
            
        self.num_steps_wait = 20  # LIBERO simulator warmup - ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸å‚è€ƒè„šæœ¬ç›¸åŒçš„20æ­¥
        
        # ğŸ”§ é»˜è®¤å¼ºåˆ¶æ‰¹å¤„ç†æ¨¡å¼ï¼Œé¿å…SubprocVectorEnvçš„æ¨¡å‹å¤åˆ¶é—®é¢˜
        self._force_batch_mode = True  # è®¾ä¸ºFalseå¯å¯ç”¨çœŸæ­£å¹¶è¡Œ
        
        # åŠ è½½å½’ä¸€åŒ–å‚æ•°
        self._load_norm_stats()
        
    def create_env(self, env_name):
        """åˆ›å»ºå¹¶è¡Œç¯å¢ƒ - ä¿®å¤ç‰ˆæœ¬ï¼Œé¿å…æ¨¡å‹å¤åˆ¶é—®é¢˜"""
        # å¦‚æœenv_nameæ˜¯æ•°å­—ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åœ¨ä»»åŠ¡åˆ—è¡¨ä¸­æŸ¥æ‰¾
        if isinstance(env_name, int):
            task_id = env_name
        elif isinstance(env_name, str) and env_name.isdigit():
            task_id = int(env_name)
        elif env_name in self.env_names:
            task_id = self.env_names.index(env_name)
        else:
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
            task_id = 0
            if self.rank == 0:
                print(f"âš ï¸ ä»»åŠ¡åç§° '{env_name}' æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤task_id=0")
        
        task = self.benchmark.get_task(task_id)
        
        env_num = min(self.num_parallel_envs, self.rollouts_per_env)
        
        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨çœŸæ­£çš„å¹¶è¡Œç¯å¢ƒ
        use_true_parallel = env_num > 1 and hasattr(self, '_force_batch_mode') and not self._force_batch_mode
        
        if use_true_parallel:
            # ğŸ”‘ å…³é”®ï¼šç®€å•çš„ç¯å¢ƒå·¥å‚ï¼Œæ— å¤–éƒ¨ä¾èµ–
            def get_env(task):
                task_description = task.language
                # ä½¿ç”¨liberoçš„æ ‡å‡†è·¯å¾„è·å–æ–¹æ³•
                from libero.libero import get_libero_path
                task_bddl_file = os.path.join(
                    get_libero_path("bddl_files"), 
                    task.problem_folder, 
                    task.bddl_file
                )
                env_args = {
                    "bddl_file_name": task_bddl_file, 
                    "camera_heights": 224, 
                    "camera_widths": 224
                }
                env = OffScreenRenderEnv(**env_args)
                env.seed(0)
                return env
                
            env_factory = lambda: get_env(task)
            
            if self.rank == 0:
                print(f"ğŸš€ åˆ›å»º {env_num} ä¸ªå¹¶è¡Œç¯å¢ƒ (çœŸæ­£å¹¶è¡Œ)")
                
            env = SubprocVectorEnv([env_factory for _ in range(env_num)])
        else:
            # ğŸ”§ ä¿®å¤ï¼šæ‰¹å¤„ç†æ¨¡å¼ - åœ¨ä¸»è¿›ç¨‹åˆ›å»ºå•ä¸ªç¯å¢ƒï¼Œæ¨¡æ‹Ÿå¹¶è¡Œ
            if self.rank == 0:
                print(f"ğŸ”§ ä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼æ¨¡æ‹Ÿ {env_num} ä¸ªå¹¶è¡Œç¯å¢ƒï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰")
            
            # åˆ›å»ºå•ä¸ªç¯å¢ƒ
            task_description = task.language
            from libero.libero import get_libero_path
            task_bddl_file = os.path.join(
                get_libero_path("bddl_files"), 
                task.problem_folder, 
                task.bddl_file
            )
            env_args = {
                "bddl_file_name": task_bddl_file, 
                "camera_heights": 224, 
                "camera_widths": 224
            }
            single_env = OffScreenRenderEnv(**env_args)
            single_env.seed(0)
            
            # åŒ…è£…æˆæ‰¹å¤„ç†ç¯å¢ƒ
            env = BatchProcessingWrapper(single_env, env_num)
        
        return env, task_id, env_num
        
    def run_policy_in_env(self, env_name, all_init_states=None, created_env=None):
        """è¿è¡Œç­–ç•¥ - å®Œå…¨å¤ç”¨RIPT-VLAçš„é€»è¾‘"""
        if created_env is None:
            env, env_id, env_num = self.create_env(env_name) 
        else:
            env, env_id, env_num = created_env
            
        if all_init_states is None:
            all_init_states = self.benchmark.get_task_init_states(env_id)
            
        count = 0
        eval_loop_num = (self.rollouts_per_env + self.num_parallel_envs - 1) // self.num_parallel_envs
        
        if self.rank == 0:
            print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ: {eval_loop_num} è½®æ¬¡, æ¯è½® {env_num} ä¸ªç¯å¢ƒ")
            
        while count < eval_loop_num:
            indices = np.arange(count * env_num, (count + 1) * env_num) % all_init_states.shape[0]
            init_states_ = all_init_states[indices]
            
            success, total_reward, episode = self.run_episode(env, env_name, init_states_, env_num)
            
            # åˆ†ç¦»æ¯ä¸ªç¯å¢ƒçš„ç»“æœ - ğŸ”§ å®Œå…¨é‡å†™æ•°æ®åˆ†ç¦»é€»è¾‘
            step_num = len(episode['actions'])
            for k in range(env_num):
                episode_k = {}
                
                # å¤„ç†æ ‡é‡å­—æ®µï¼ˆæ‰€æœ‰ç¯å¢ƒå…±äº«ï¼‰
                for key in ['task', 'task_id']:
                    if key in episode:
                        episode_k[key] = episode[key]
                
                # ğŸ”‘ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†observationså­—æ®µ
                if 'observations' in episode:
                    # episode['observations']æ˜¯æ¯æ­¥çš„obsåˆ—è¡¨ï¼Œæ¯ä¸ªobsåˆ—è¡¨åŒ…å«env_numä¸ªç¯å¢ƒçš„è§‚æµ‹
                    episode_k['observations'] = []
                    for step_obs_list in episode['observations']:
                        # step_obs_listæ˜¯ä¸€ä¸ªé•¿åº¦ä¸ºenv_numçš„åˆ—è¡¨ï¼ŒåŒ…å«æ¯ä¸ªç¯å¢ƒçš„åŸå§‹è§‚æµ‹
                        if isinstance(step_obs_list, list) and len(step_obs_list) > k:
                            episode_k['observations'].append(step_obs_list[k])  # ç¬¬kä¸ªç¯å¢ƒçš„è§‚æµ‹
                        else:
                            # å¦‚æœä¸æ˜¯åˆ—è¡¨æˆ–ç´¢å¼•è¶…ç•Œï¼Œå¯èƒ½æ˜¯å•ç¯å¢ƒæ•°æ®
                            episode_k['observations'].append(step_obs_list)
                
                # å¤„ç†å…¶ä»–æ•°ç»„å­—æ®µï¼ˆactions, rewards, dones, infosï¼‰
                for key in ['actions', 'rewards', 'dones', 'infos']:
                    if key in episode:
                        episode_k[key] = []
                        for step_data in episode[key]:
                            if isinstance(step_data, list) and len(step_data) > k:
                                episode_k[key].append(step_data[k])  # ç¬¬kä¸ªç¯å¢ƒçš„æ•°æ®
                            else:
                                # å¤‡ç”¨ï¼šå¯èƒ½æ˜¯å•ç¯å¢ƒæˆ–å…¶ä»–æ ¼å¼
                                episode_k[key].append(step_data)
                
                yield success[k], total_reward[k], episode_k
                
            count += 1
            
        if created_env is None:
            # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨å…³é—­ç¯å¢ƒï¼Œé¿å…EGLé”™è¯¯
            try:
                env.close()
                del env
                gc.collect()
            except Exception as e:
                if self.rank == 0:
                    print(f"âš ï¸ ç¯å¢ƒå…³é—­æ—¶å‡ºç°è­¦å‘Š: {e}")
                # å¼ºåˆ¶æ¸…ç†ï¼Œå¿½ç•¥EGLé”™è¯¯
                import os
                os.environ["EGL_LOG_LEVEL"] = "fatal"
            
    def run_episode(self, env, env_name, init_states_, env_num):
        """è¿è¡Œå•ä¸ªepisode - æ ¸å¿ƒå¹¶è¡Œé€»è¾‘"""
        if self.rank == 0:
            print(f"ğŸ”„ é‡ç½®ç¯å¢ƒå¹¶å¼€å§‹episode")
            
        env.reset()
        env.set_init_state(init_states_)
        
        # ğŸ”‘ è·å–åˆå§‹è§‚æµ‹ - é€šè¿‡stepè·å¾—
        dummy_actions = [np.array([0, 0, 0, 0, 0, 0, -1]) for _ in range(env_num)]
        obs, _, _, _ = env.step(dummy_actions)
        
        task_id = self.env_names.index(env_name) 
        task = self.benchmark.get_task(task_id)
        task_description = task.language
        
        t = 0
        success = [False] * env_num
        total_reward = [0] * env_num
        
        episode = {
            'actions': [],
            'observations': [],
            'rewards': [],
            'dones': [],
            'infos': [],
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ PI0éœ€è¦çš„taskå­—æ®µ
            'task': task_description,
            'task_id': task_id,
        }
        
        # ğŸ¬ è§†é¢‘æ”¶é›†ï¼šæ¯ä¸ªç¯å¢ƒç‹¬ç«‹çš„å¸§ç¼“å­˜
        video_frames = [[] for _ in range(env_num)]
        save_video = True  # å¯ç”¨è§†é¢‘ä¿å­˜
        
        # ğŸ”‘ å…³é”®ï¼šåŠ¨ä½œé˜Ÿåˆ— - æ¯ä¸ªç¯å¢ƒç‹¬ç«‹çš„åŠ¨ä½œç¼“å­˜
        batch_action_queue = [deque(maxlen=50) for _ in range(env_num)]
        
        while t < self.max_episode_length + self.num_steps_wait:
            # Simulator warmup
            if t < self.num_steps_wait:
                dummy_actions = [np.array([0, 0, 0, 0, 0, 0, -1]) for _ in range(env_num)]
                obs, _, done, _ = env.step(dummy_actions)
                t += 1
                continue
                
            step_actions = [np.array([0, 0, 0, 0, 0, 0, -1]) for _ in range(env_num)]
            step_observations = [None] * env_num
            
            # æ”¶é›†éœ€è¦æ¨ç†çš„ç¯å¢ƒ
            step_input_obs_ids = []
            step_input_obs_list = []
            
            for bidx in range(env_num):
                if not success[bidx]:
                    step_input_obs_ids.append(bidx)
                    
                    # å‡†å¤‡PI0è§‚æµ‹æ ¼å¼
                    observation = self.prepare_pi0_observation(obs[bidx], task_description)
                    step_observations[bidx] = observation
                    step_input_obs_list.append(observation)
                    
            # ğŸ”‘ å…³é”®ï¼šæ‰¹é‡æ¨ç†æ¡ä»¶ - å½“æ‰€æœ‰éœ€è¦çš„ç¯å¢ƒçš„åŠ¨ä½œé˜Ÿåˆ—éƒ½ç©ºäº†
            conduct_inference = all(len(batch_action_queue[i]) == 0 for i in step_input_obs_ids)
            
            if conduct_inference and step_input_obs_list:
                # ğŸš€ æ‰¹é‡æ¨ç† - ä¸€æ¬¡å¤„ç†å¤šä¸ªç¯å¢ƒçš„è§‚æµ‹
                if self.rank == 0:
                    print(f"   ğŸ§  æ‰¹é‡æ¨ç†: {len(step_input_obs_list)} ä¸ªç¯å¢ƒ (æ­¥éª¤ {t-self.num_steps_wait+1})")
                    
                with torch.inference_mode():
                    batch_actions = self.get_pi0_action_batch(step_input_obs_list)
                    
                    # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºæ¨ç†ç»“æœ
                    if self.rank == 0:
                        for act_idx, actions in enumerate(batch_actions):
                            print(f"     ç¯å¢ƒ {step_input_obs_ids[act_idx]}: ç”Ÿæˆäº† {len(actions)} ä¸ªåŠ¨ä½œ")
                            if len(actions) > 0:
                                first_action = actions[0]
                                print(f"       é¦–ä¸ªåŠ¨ä½œ: [{first_action[0]:.3f}, {first_action[1]:.3f}, {first_action[2]:.3f}, ...]")
                    
                    # åˆ†å‘åŠ¨ä½œåˆ°å„ä¸ªç¯å¢ƒçš„é˜Ÿåˆ—
                    for act_idx, obs_id in enumerate(step_input_obs_ids):
                        batch_action_queue[obs_id].extend(batch_actions[act_idx])
                        if self.rank == 0:
                            print(f"     ç¯å¢ƒ {obs_id}: åŠ¨ä½œé˜Ÿåˆ—é•¿åº¦ {len(batch_action_queue[obs_id])}")
                        
            # ä»é˜Ÿåˆ—ä¸­å–å‡ºåŠ¨ä½œæ‰§è¡Œ
            for i, bidx in enumerate(step_input_obs_ids):
                if len(batch_action_queue[bidx]) > 0:
                    action = batch_action_queue[bidx].popleft()
                    step_actions[bidx] = action
                    
            # ç¯å¢ƒæ­¥è¿›
            obs, reward, done, info = env.step(step_actions)
            
            # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºç¯å¢ƒæ‰§è¡Œä¿¡æ¯
            if self.rank == 0 and t % 10 == 0:  # æ¯10æ­¥æ‰“å°ä¸€æ¬¡
                print(f"   ğŸ“Š æ­¥éª¤ {t-self.num_steps_wait+1}: å¥–åŠ± {reward}, å®Œæˆ {done}")
                for k in range(env_num):
                    has_action = len(step_input_obs_ids) > k and step_input_obs_ids[k] < len(step_actions)
                    action_info = f"åŠ¨ä½œ: [{step_actions[k][0]:.2f}, {step_actions[k][1]:.2f}, ...]" if has_action else "æ— åŠ¨ä½œ"
                    print(f"     ç¯å¢ƒ {k}: {action_info}, å¥–åŠ± {reward[k]:.3f}, æ€»å¥–åŠ± {total_reward[k]:.3f}")
            
            # æ›´æ–°çŠ¶æ€
            for k in range(env_num):
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®çš„æˆåŠŸåˆ¤æ–­é€»è¾‘ - ä½¿ç”¨infoä¸­çš„successå­—æ®µè€Œä¸æ˜¯done
                old_success = success[k]
                if hasattr(info[k], '__getitem__') and 'success' in info[k]:
                    success[k] = success[k] or info[k]['success']
                elif hasattr(info[k], 'success'):
                    success[k] = success[k] or info[k].success
                # å¤‡ç”¨ï¼šå¦‚æœæ²¡æœ‰successä¿¡æ¯ä¸”å¥–åŠ±>0.5ï¼Œä¹Ÿè®¤ä¸ºæˆåŠŸ
                elif reward[k] > 0.5:
                    success[k] = success[k] or True
                total_reward[k] += reward[k]
                
                # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºæˆåŠŸçŠ¶æ€å˜åŒ–
                if not old_success and success[k] and self.rank == 0:
                    print(f"   ğŸ‰ ç¯å¢ƒ {k} æˆåŠŸå®Œæˆä»»åŠ¡ï¼æ€»å¥–åŠ±: {total_reward[k]:.3f}")
                
            if all(success):
                break
                
            t += 1
            
            # è®°å½•æ•°æ® - ğŸ”§ ä¿®å¤ï¼šä¿å­˜åŸå§‹ç¯å¢ƒè§‚æµ‹è€Œä¸æ˜¯PI0æ ¼å¼è§‚æµ‹
            if conduct_inference:
                episode['actions'].append(step_actions)
                # ä¿å­˜åŸå§‹ç¯å¢ƒè§‚æµ‹æ•°æ®ï¼Œè€Œä¸æ˜¯PI0è½¬æ¢åçš„æ ¼å¼
                episode['observations'].append(obs)
                episode['rewards'].append(reward)
                episode['dones'].append(done)
                episode['infos'].append(info)
                
                # ğŸ¬ ä¿å­˜è§†é¢‘å¸§ï¼ˆæ¯éš”5æ­¥ä¿å­˜ä¸€æ¬¡ä»¥å‡å°‘å­˜å‚¨ï¼‰
                if save_video and (t - self.num_steps_wait) % 5 == 0:
                    for k in range(env_num):
                        if isinstance(obs, list) and len(obs) > k:
                            frame = obs[k].get("agentview_image")
                            if frame is not None:
                                video_frames[k].append(frame)
                
        if self.rank == 0:
            total_steps = t - self.num_steps_wait
            inference_count = len(episode.get('actions', []))
            print(f"âœ… Episodeå®Œæˆ: æˆåŠŸ {success}")
            print(f"   ğŸ“ˆ æ€»æ­¥æ•°: {total_steps}, æ¨ç†æ¬¡æ•°: {inference_count}, æœ€ç»ˆå¥–åŠ±: {total_reward}")
            print(f"   ğŸ“Š å¹³å‡å¥–åŠ±/ç¯å¢ƒ: {[f'{r:.3f}' for r in total_reward]}")
            
            # ğŸ¬ ä¿å­˜è§†é¢‘æ–‡ä»¶
            if save_video:
                self._save_episode_videos(video_frames, env_name, total_reward, success)
            
        return success, total_reward, episode
    
    def _save_episode_videos(self, video_frames, env_name, total_rewards, successes):
        """ä¿å­˜episodeè§†é¢‘"""
        try:
            import imageio
            from datetime import datetime
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # ä½¿ç”¨pi0/ript/debug_images/videosä½œä¸ºè§†é¢‘ç›®å½•ï¼Œä¸å…¶ä»–éƒ¨åˆ†ä¿æŒä¸€è‡´
            video_dir = Path("pi0/ript/debug_images/videos")
            video_dir.mkdir(parents=True, exist_ok=True)
            
            for env_id, frames in enumerate(video_frames):
                if len(frames) > 0:
                    success_tag = "success" if successes[env_id] else "fail"
                    reward_tag = f"r{total_rewards[env_id]:.2f}"
                    video_filename = f"{env_name}_env{env_id}_{success_tag}_{reward_tag}_{timestamp}.mp4"
                    video_path = video_dir / video_filename
                    
                    # BGR to RGB conversion for imageio
                    rgb_frames = [frame[:, :, ::-1] for frame in frames]
                    imageio.mimsave(video_path, rgb_frames, fps=10)
                    print(f"   ğŸ¬ è§†é¢‘å·²ä¿å­˜: {video_path}")
                    
        except Exception as e:
            print(f"   âš ï¸ è§†é¢‘ä¿å­˜å¤±è´¥: {e}")
        
    def prepare_pi0_observation(self, obs, task_description):
        """å‡†å¤‡PI0è§‚æµ‹æ ¼å¼"""
        # å›¾åƒå¤„ç†  
        base_0_rgb = obs["agentview_image"][:, :, ::-1].copy()
        left_wrist_0_rgb = obs["robot0_eye_in_hand_image"][:, :, ::-1].copy() 
        
        # çŠ¶æ€å¤„ç† - ä¸åŸå®ç°ä¿æŒä¸€è‡´
        axis_angle = T.quat2axisangle(obs["robot0_eef_quat"])
        unnorm_state = np.concatenate([
            obs["robot0_eef_pos"],
            axis_angle, 
            obs["robot0_gripper_qpos"],
        ], dtype=np.float32)
        
        # çŠ¶æ€å½’ä¸€åŒ–
        state = (unnorm_state - self.state_mean) / (self.state_std + 1e-6)
        
        observation = {
            "image": {
                "base_0_rgb": torch.from_numpy(base_0_rgb).cuda()[None],
                "left_wrist_0_rgb": torch.from_numpy(left_wrist_0_rgb).cuda()[None],
            },
            "state": torch.from_numpy(state).cuda()[None],
            "prompt": [task_description],
        }
        
        return observation
        
    def get_pi0_action_batch(self, obs_batch):
        """æ‰¹é‡æ¨ç†PI0æ¨¡å‹ - å…³é”®ä¼˜åŒ–"""
        batch_actions = []
        
        for obs in obs_batch:
            if self.policy is None:
                # æµ‹è¯•æ¨¡å¼ï¼šç”ŸæˆéšæœºåŠ¨ä½œ
                action = np.random.randn(50, 7).astype(np.float32) * 0.01  # å°å¹…éšæœºåŠ¨ä½œ
                if self.rank == 0:
                    print("âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨éšæœºåŠ¨ä½œ")
            else:
                # çœŸå®æ¨ç†æ¨¡å¼
                raw_action = self.policy.select_action(obs)
                action = raw_action[0, :, :7]  # (50, 7)
                
                if isinstance(action, torch.Tensor):
                    action = action.cpu().numpy()
            
            # åå½’ä¸€åŒ–åŠ¨ä½œ - ä¸åŸå®ç°ä¿æŒä¸€è‡´
            if hasattr(self, 'action_mean') and hasattr(self, 'action_std'):
                action = action * (self.action_std + 1e-6) + self.action_mean
                
            batch_actions.append(action)
            
        return batch_actions
    
    def _load_norm_stats(self):
        """åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        import json
        from pathlib import Path
        
        # å°è¯•åœ¨å¸¸è§ä½ç½®æ‰¾åˆ°norm_stats.json
        possible_paths = [
            "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch/norm_stats.json",
            "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json", 
            "./checkpoints/pi0_libero_pytorch/norm_stats.json",
            "./norm_stats.json"
        ]
        
        norm_stats_path = None
        for path in possible_paths:
            if Path(path).exists():
                norm_stats_path = path
                break
        
        if norm_stats_path and Path(norm_stats_path).exists():
            if self.rank == 0:
                print(f"ğŸ“Š åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡: {norm_stats_path}")
            with open(norm_stats_path) as f:
                norm_stats = json.load(f)
                
            # æå–çŠ¶æ€å’ŒåŠ¨ä½œçš„å½’ä¸€åŒ–å‚æ•°
            self.state_mean = np.array(norm_stats["norm_stats"]["state"]["mean"][:8], dtype=np.float32)
            self.state_std = np.array(norm_stats["norm_stats"]["state"]["std"][:8], dtype=np.float32)
            self.action_mean = np.array(norm_stats["norm_stats"]["actions"]["mean"][:7], dtype=np.float32)
            self.action_std = np.array(norm_stats["norm_stats"]["actions"]["std"][:7], dtype=np.float32)
            
            if self.rank == 0:
                print("âœ… å½’ä¸€åŒ–å‚æ•°åŠ è½½æˆåŠŸ")
        else:
            if self.rank == 0:
                print("âš ï¸ æœªæ‰¾åˆ°norm_stats.jsonï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            # ä½¿ç”¨é»˜è®¤å€¼
            self.state_mean = np.zeros(8, dtype=np.float32)
            self.state_std = np.ones(8, dtype=np.float32)
            self.action_mean = np.zeros(7, dtype=np.float32)
            self.action_std = np.ones(7, dtype=np.float32)