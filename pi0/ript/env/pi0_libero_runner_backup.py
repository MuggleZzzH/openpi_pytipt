import os
import sys
import torch
import numpy as np
from typing import List, Dict, Any, Tuple, Optional, Union, Callable
from datetime import datetime
import imageio
from pathlib import Path
import json

# å¯¼å…¥LIBEROç›¸å…³çš„å˜æ¢å‡½æ•° â€”â€” è‹¥å¤±è´¥ç›´æ¥æŠ›å¼‚å¸¸ï¼Œä¸å®˜æ–¹è„šæœ¬ä¿æŒä¸€è‡´
try:
    import robosuite.utils.transform_utils as T
except ImportError as e:
    raise ImportError("robosuite is required for LIBEROEnvRunner but not found") from e

# è°ƒè¯•è®¾ç½®
DEBUG_SAVE_IMAGES = False  # ç¦ç”¨å•ç‹¬å›¾åƒä¿å­˜ï¼Œåªä¿ç•™è§†é¢‘
DEBUG_SAVE_VIDEO = os.environ.get("PI0_DEBUG_SAVE_VIDEO", "true").lower() in ("true", "1", "yes")
DEBUG_IMAGE_DIR = os.environ.get("PI0_DEBUG_IMAGE_DIR", "ript/debug_images")

class LIBEROEnvRunner:
    """LIBEROç¯å¢ƒè¿è¡Œå™¨ï¼Œæä¾›PI0ç­–ç•¥åœ¨LIBEROç¯å¢ƒä¸­çš„è¿è¡Œæœºåˆ¶"""
    
    def __init__(self, policy=None, benchmark_name=None, rollouts_per_env=1, 
                 num_parallel_envs=1, max_episode_length=200, task_names_to_use=None,
                 config=None, rank=0, world_size=1, norm_stats_path=None):
        """åˆå§‹åŒ–LIBEROç¯å¢ƒè¿è¡Œå™¨"""
        # å­˜å‚¨å‚æ•°
        self.policy = policy
        self.benchmark_name = benchmark_name
        self.rollouts_per_env = rollouts_per_env
        self.num_parallel_envs = num_parallel_envs
        self.max_episode_length = max_episode_length
        self.task_names_to_use = task_names_to_use or []
        self.task_names = task_names_to_use or []  # å…¼å®¹æ€§åˆ«åï¼Œä¾›rollout_generatorä½¿ç”¨
        self.max_steps = max_episode_length if max_episode_length is not None else 500
        
        # âœ… å­˜å‚¨åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
        self.config = config
        self.rank = rank
        self.world_size = world_size
        
        # è°ƒè¯•é€‰é¡¹
        self.debug_save_images = DEBUG_SAVE_IMAGES
        self.debug_save_video = DEBUG_SAVE_VIDEO
        self.debug_image_dir = DEBUG_IMAGE_DIR
        
        # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
        self._load_norm_stats(norm_stats_path)
        
    def _load_norm_stats(self, norm_stats_path: Optional[str] = None):
        """Load normalization statistics from norm_stats.json"""
        if norm_stats_path is None:
            # å°è¯•åœ¨å¸¸è§ä½ç½®æ‰¾åˆ°norm_stats.json
            possible_paths = [
                "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch/norm_stats.json",
                "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json", 
                "./checkpoints/pi0_libero_pytorch/norm_stats.json",
                "./norm_stats.json"
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    norm_stats_path = path
                    break
        
        if norm_stats_path and Path(norm_stats_path).exists():
            print(f"LIBEROEnvRunner: Loading norm_stats from: {norm_stats_path}")
            with open(norm_stats_path) as f:
                norm_stats = json.load(f)
                
            # æå–çŠ¶æ€å’ŒåŠ¨ä½œçš„å½’ä¸€åŒ–å‚æ•°
            self.state_mean = np.array(norm_stats["norm_stats"]["state"]["mean"][:8], dtype=np.float32)
            self.state_std = np.array(norm_stats["norm_stats"]["state"]["std"][:8], dtype=np.float32)
            self.action_mean = np.array(norm_stats["norm_stats"]["actions"]["mean"][:7], dtype=np.float32)
            self.action_std = np.array(norm_stats["norm_stats"]["actions"]["std"][:7], dtype=np.float32)
            
            print(f"âœ… LIBEROEnvRunner: Loaded normalization stats for action post-processing")
        else:
            raise FileNotFoundError("norm_stats.json not found in expected locations; cannot proceed")


    
    def construct_pi0_observation(self, obs, task_description):
        """æ„é€ PI0éœ€è¦çš„è§‚æµ‹æ ¼å¼ï¼Œå®Œå…¨æ¨¡ä»¿2_test_pi0_on_libero.pyçš„åšæ³•"""
        # è·å–è®¾å¤‡
        device = self.policy.config.device if hasattr(self.policy, 'config') else 'cuda:0'
        
        # çŠ¶æ€å¤„ç†
        import robosuite.utils.transform_utils as T
        
        axis_angle = T.quat2axisangle(obs["robot0_eef_quat"])
            
        unnorm_state = np.concatenate([
            obs["robot0_eef_pos"],                    # 3D: end-effector position
            axis_angle, # 3D: rotation as axis-angle  
            obs["robot0_gripper_qpos"],               # 2D: gripper joint positions
        ], dtype=np.float32)
        print(">> gripper_qpos shape:", np.asarray(obs["robot0_gripper_qpos"]).shape)
        # çŠ¶æ€å½’ä¸€åŒ–
        state = (unnorm_state - self.state_mean) / (self.state_std + 1e-6)
        
        # å›¾åƒå¤„ç†
        base_0_rgb = obs["agentview_image"][:, :, ::-1].copy()
        left_wrist_0_rgb = obs["robot0_eye_in_hand_image"][:, :, ::-1].copy()
        
        # æ„é€ è§‚æµ‹æ ¼å¼
        observation = {
            "image": {
                "base_0_rgb": torch.from_numpy(base_0_rgb).to(device)[None],
                "left_wrist_0_rgb": torch.from_numpy(left_wrist_0_rgb).to(device)[None],
            },
            "state": torch.from_numpy(state).to(device)[None],
            "prompt": [task_description],
        }
        
        return observation
    
    def denormalize_action(self, action: np.ndarray) -> np.ndarray:
        """Denormalize action using loaded statistics"""
        return action * (self.action_std + 1e-6) + self.action_mean
    
    def get_unnormalized_state(self, obs) -> np.ndarray:
        """ä»è§‚æµ‹ä¸­æå–æœªå½’ä¸€åŒ–çš„çŠ¶æ€ï¼ˆç”¨äºåŠ¨ä½œåç§»ï¼‰"""
        try:
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            
            if not isinstance(obs, dict) or not obs:
                raise RuntimeError("Required observation dict missing or empty")
            
            # ä½¿ç”¨ end-effector ä¿¡æ¯
            if "robot0_eef_pos" in obs and "robot0_eef_quat" in obs:
                eef_pos = np.array(obs["robot0_eef_pos"], dtype=np.float32)
                eef_quat = np.array(obs["robot0_eef_quat"], dtype=np.float32)
                
                if eef_pos.size != 3 or eef_quat.size != 4:
                    raise RuntimeError("Observation fields robot0_eef_pos or robot0_eef_quat have incorrect dimensions")
                
                # è½¬æ¢å››å…ƒæ•°ä¸ºè½´è§’
                try:
                    axis_angle = T.quat2axisangle(eef_quat).astype(np.float32)
                except Exception as e:
                    raise RuntimeError(f"Failed to convert quaternion to axis-angle: {e}")
                
                # è·å– gripper çŠ¶æ€
                if "robot0_gripper_qpos" not in obs:
                    raise RuntimeError("Observation missing required field robot0_gripper_qpos")
                try:
                    gripper_qpos = float(obs["robot0_gripper_qpos"][0])
                except (IndexError, TypeError, ValueError) as e:
                    raise RuntimeError(f"Invalid robot0_gripper_qpos value: {e}")
                
                # æ„é€ æœªå½’ä¸€åŒ–çš„çŠ¶æ€
                unnorm_state = np.concatenate([eef_pos[:3], axis_angle[:3], [gripper_qpos]]).astype(np.float32)
                return unnorm_state
                
            # å®˜æ–¹è„šæœ¬è¦æ±‚ä¸Šè¿°å…³é”®å­—æ®µå‡å­˜åœ¨ï¼Œè‹¥ç¼ºå¤±åˆ™ç«‹å³æŠ¥é”™
            else:
                raise RuntimeError("Observation missing required end-effector fields")
                
        except Exception as e:
            raise
    
    def make_env(self, env_name: str):
        """åˆ›å»ºLIBEROç¯å¢ƒ"""
        try:
            import gym
            from cleandiffuser.env import libero  # ç¡®ä¿ç¯å¢ƒæ³¨å†Œ
            
            # ä½¿ç”¨ä¸å‚è€ƒå®ç°2_test_pi0_on_libero.pyå®Œå…¨ç›¸åŒçš„ç¯å¢ƒé…ç½®
            benchmark_to_env_id = {
                'libero_spatial': 'libero-spatial-v0',
                'libero_object': 'libero-object-v0',
                'libero_goal': 'libero-goal-v0',
                'libero_10': 'libero-10-v0',
                'libero_90': 'libero-90-v0'
            }

            if self.benchmark_name not in benchmark_to_env_id:
                raise ValueError(f"Unknown benchmark_name: {self.benchmark_name}")

            env_id = benchmark_to_env_id[self.benchmark_name]
            # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®ä»»åŠ¡åç§°åŠ¨æ€ç¡®å®štask_idï¼Œä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´
            # å¯¹äºlibero_goalï¼Œä½¿ç”¨task_id=1 (ä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´)
            if self.benchmark_name == "libero_goal":
                task_id = 1  # ä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´
            elif self.benchmark_name == "libero_spatial":
                task_id = 0  # ç¬¬ä¸€ä¸ªspatialä»»åŠ¡
            else:
                task_id = 0  # å…¶ä»–benchmarkçš„é»˜è®¤ä»»åŠ¡
            
            # åˆ›å»ºç¯å¢ƒ
            env = gym.make(
                env_id,
                task_id=task_id,
                image_size=224,  # åŒ¹é…PI0çš„è¾“å…¥å°ºå¯¸
                camera_names=["agentview", "robot0_eye_in_hand"],  # åŒ¹é…åŸå§‹demo
                seed=0,  # ä½¿ç”¨ä¸å‚è€ƒå®ç°ç›¸åŒçš„éšæœºç§å­
            )
            
            # è·å–ä»»åŠ¡æè¿°
            if hasattr(env, 'task_description'):
                task_description = env.task_description
            else:
                task_description = env_name
                
            return env, task_description
            
        except Exception as e:
            raise RuntimeError(f"åˆ›å»ºç¯å¢ƒå¤±è´¥: {e}") from e
        
    def run_policy_in_env(self, env_name, all_init_states=None, debug_save_video=None):
        """åœ¨ç¯å¢ƒä¸­è¿è¡Œç­–ç•¥ï¼Œç”Ÿæˆè½¨è¿¹"""
        # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–å‚æ•°æ§åˆ¶è°ƒè¯•
        save_video = debug_save_video if debug_save_video is not None else self.debug_save_video
        
        # å¯¹æ¯ä¸ªåˆå§‹çŠ¶æ€è¿è¡Œä¸€æ¬¡episode
        for i, init_state in enumerate(all_init_states):
            rollout_images = []
            
            # åˆ›å»ºç¯å¢ƒ
            env, task_description = self.make_env(env_name)
            
            # è®¾ç½®åˆå§‹çŠ¶æ€å¹¶è·å–åˆå§‹è§‚æµ‹
            obs = env.reset()
            
            # çƒ­æœºæ­¥éª¤
            dummy_action = np.array([0, 0, 0, 0, 0, 0, -1])
            for warmup_step in range(20):
                obs, _, _, _ = env.step(dummy_action)
            
            step = 0
            done = False
            total_reward = 0
            success = False
            
            # æ”¶é›†åˆå§‹è§‚æµ‹å›¾åƒç”¨äºè§†é¢‘
            if save_video:
                initial_img = obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                rollout_images.append(initial_img)
            
            # æ”¶é›†è½¨è¿¹
            observations = [obs]
            actions = []
            rewards = []
            dones = []
            infos = []
            
            try:
                action_buffer = None
                action_index = 0
                
                # æ‰§è¡Œç­–ç•¥ç›´åˆ°å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°
                while not done and step < self.max_steps:
                    # åªåœ¨éœ€è¦æ—¶æ¨ç†ï¼ˆåŠ¨ä½œé˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼‰
                    if action_buffer is None or action_index >= action_buffer.shape[0]:
                        # ä½¿ç”¨æ­£ç¡®çš„PI0è§‚æµ‹æ ¼å¼
                        pi0_observation = self.construct_pi0_observation(obs, task_description)
                        
                        # é€‰æ‹©åŠ¨ä½œ
                        raw_action = self.policy.select_action(pi0_observation)
                        action = raw_action[0, :, :7]  # shape: (50, 7)
                        
                        # è½¬æ¢ä¸ºnumpyå¹¶å¤„ç†ç»´åº¦
                        if isinstance(action, torch.Tensor):
                            action_after_cpu = action.cpu().numpy()
                        else:
                            action_after_cpu = action
                        
                        # åå½’ä¸€åŒ–åŠ¨ä½œ
                        action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                        
                        # è·å–å½“å‰æœªå½’ä¸€åŒ–çŠ¶æ€ç”¨äºåç§»
                        import robosuite.utils.transform_utils as T
                        unnorm_state = np.concatenate([
                            obs["robot0_eef_pos"],
                            T.quat2axisangle(obs["robot0_eef_quat"]),
                            obs["robot0_gripper_qpos"],
                        ], dtype=np.float32)
                        
                        # åº”ç”¨çŠ¶æ€åç§»ï¼ˆå‰6ç»´ï¼‰
                        action_buffer[:, :6] += unnorm_state[None, :6]
                        
                        # é‡ç½®åŠ¨ä½œç´¢å¼•
                        action_index = 0
                    
                    # ä»åŠ¨ä½œé˜Ÿåˆ—ä¸­å–å‡ºå½“å‰åŠ¨ä½œæ‰§è¡Œ
                    current_action = action_buffer[action_index, :7]
                    
                    # æ‰§è¡Œå•æ­¥åŠ¨ä½œ
                    next_obs, reward, done, info = env.step(current_action)
                    
                    # è®°å½•è½¨è¿¹æ•°æ®
                    observations.append(next_obs)
                    actions.append(current_action)
                    rewards.append(reward)
                    dones.append(done)
                    infos.append(info)
                    
                    # æ›´æ–°ç´¯è®¡å¥–åŠ±
                    total_reward += reward
                    
                    # æ”¶é›†å›¾åƒç”¨äºè§†é¢‘
                    if save_video:
                        frame_img = next_obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                        rollout_images.append(frame_img)
                    
                    # æ›´æ–°çŠ¶æ€å’Œè®¡æ•°å™¨
                    obs = next_obs
                    step += 1
                    action_index += 1
                    
                    # æ£€æŸ¥æˆåŠŸçŠ¶æ€ - ä¼˜å…ˆä½¿ç”¨infoä¸­çš„successï¼Œå¦åˆ™åŸºäºå¥–åŠ±åˆ¤æ–­ï¼ˆä¸4è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
                    if info.get("success", False):
                        success = True
                    # å¦‚æœinfoä¸­æ²¡æœ‰successå­—æ®µï¼Œä½¿ç”¨ä¸4_simple_train_ript.pyç›¸åŒçš„åˆ¤æ–­é€»è¾‘
                    elif total_reward > 0.5:
                        success = True
                    
                    # å¦‚æœä»»åŠ¡å®Œæˆï¼Œæå‰é€€å‡º
                    if done:
                        break
            
            except Exception as e:
                print(f"æ‰§è¡Œç¯å¢ƒæ­¥éª¤æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                # å…³é—­ç¯å¢ƒå¹¶é‡Šæ”¾èµ„æº
                try:
                    env.close()
                except:
                    pass
            
            # ä¿å­˜æ•´ä¸ªè½¨è¿¹çš„è§†é¢‘
            if save_video and rollout_images:
                try:
                    # åˆ›å»ºè§†é¢‘ç›®å½•
                    video_dir = Path("pi0/ript/debug_images/videos")
                    video_dir.mkdir(parents=True, exist_ok=True)
                    
                    # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    task_str = task_description.replace(" ", "_")[:30] if isinstance(task_description, str) else f"episode_{i}"
                    success_str = "success" if success else "failure"
                    video_path = video_dir / f"{timestamp}_{task_str}_{success_str}.mp4"
                    
                    # ä¿å­˜è§†é¢‘
                    writer = imageio.get_writer(str(video_path), fps=30)
                    for frame in rollout_images:
                        writer.append_data(frame)
                    writer.close()
                    
                    print(f"âœ… å·²ä¿å­˜è§†é¢‘: {video_path}")
                except Exception as e:
                    print(f"ä¿å­˜è§†é¢‘å‡ºé”™: {e}")
            
            # æ”¶é›†episodeæ•°æ®
            episode_data = {
                "observations": observations,
                "actions": actions,
                "rewards": rewards,
                "dones": dones,
                "infos": infos,
                "task": task_description,
            }
            
            # è¿”å›è½¨è¿¹ç»“æœ
            yield (success, total_reward, episode_data)