import os
import sys
import torch
import numpy as np
from typing import List, Dict, Any, Tuple, Optional, Union, Callable
from datetime import datetime
import imageio
from pathlib import Path
import json

# å¯¼å…¥LIBEROç›¸å…³çš„å˜æ¢å‡½æ•° â€”â€” è‹¥å¤±è´¥ç›´æ¥æŠ›å¼‚å¸¸ï¼Œä¸å®˜æ–¹è„šæœ¬ä¿æŒä¸€è‡´
try:
    import robosuite.utils.transform_utils as T
except ImportError as e:
    raise ImportError("robosuite is required for LIBEROEnvRunner but not found") from e

# å¯¼å…¥å¹¶è¡Œç¯å¢ƒæ”¯æŒ (ä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨)
try:
    from libero.libero.envs import SubprocVectorEnv
    import multiprocessing
    import gc
    VECTOR_ENV_AVAILABLE = True
except ImportError:
    VECTOR_ENV_AVAILABLE = False

# å¯¼å…¥ç‹¬ç«‹ç¯å¢ƒå·¥å‚ (ç”¨äºçœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œ)
try:
    from .parallel_env_factory import create_env_factory
    TRUE_PARALLEL_AVAILABLE = True
except ImportError:
    TRUE_PARALLEL_AVAILABLE = False

# è°ƒè¯•è®¾ç½®
DEBUG_SAVE_IMAGES = False  # ç¦ç”¨å•ç‹¬å›¾åƒä¿å­˜ï¼Œåªä¿ç•™è§†é¢‘
DEBUG_SAVE_VIDEO = os.environ.get("PI0_DEBUG_SAVE_VIDEO", "false").lower() in ("true", "1", "yes")
DEBUG_IMAGE_DIR = os.environ.get("PI0_DEBUG_IMAGE_DIR", "ript/debug_images")

class LIBEROEnvRunner:
    """LIBEROç¯å¢ƒè¿è¡Œå™¨ï¼Œæä¾›PI0ç­–ç•¥åœ¨LIBEROç¯å¢ƒä¸­çš„è¿è¡Œæœºåˆ¶"""
    
    def __init__(self, policy=None, benchmark_name=None, rollouts_per_env=1, 
                 num_parallel_envs=1, max_episode_length=200, task_names_to_use=None,
                 config=None, rank=0, world_size=1, norm_stats_path=None):
        """åˆå§‹åŒ–LIBEROç¯å¢ƒè¿è¡Œå™¨"""
        # å­˜å‚¨å‚æ•°
        self.policy = policy
        self.benchmark_name = benchmark_name
        self.rollouts_per_env = rollouts_per_env
        self.num_parallel_envs = num_parallel_envs
        self.max_episode_length = max_episode_length
        self.task_names_to_use = task_names_to_use or []
        self.task_names = task_names_to_use or []  # å…¼å®¹æ€§åˆ«åï¼Œä¾›rollout_generatorä½¿ç”¨
        self.max_steps = max_episode_length if max_episode_length is not None else 500
        
        # âœ… å­˜å‚¨åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
        self.config = config
        self.rank = rank
        self.world_size = world_size
        
        # ğŸ”¥ æ–°å¢ï¼šåŠŸèƒ½å¼€å…³æ§åˆ¶ (å®‰å…¨é›†æˆå¤æ‚åŠŸèƒ½)
        # ä»é…ç½®æ–‡ä»¶çš„featureséƒ¨åˆ†è¯»å–å¼€å…³è®¾ç½®
        if config and hasattr(config, 'features'):
            features = config.features
            self.enable_task_polling = getattr(features, 'enable_task_polling', False)
            self.enable_parallel_envs = getattr(features, 'enable_parallel_envs', False)
            self.enable_smart_sampling = getattr(features, 'enable_smart_sampling', False)
            # ğŸ†• æ–°å¢ï¼šçœŸæ­£å¤šè¿›ç¨‹å¹¶è¡Œå¼€å…³ï¼ˆé»˜è®¤å…³é—­ä»¥ä¿æŒå…¼å®¹æ€§ï¼‰
            self.enable_true_parallel_envs = getattr(features, 'enable_true_parallel_envs', False)
        else:
            # å›é€€åˆ°ç›´æ¥ä»configè¯»å–æˆ–ä½¿ç”¨é»˜è®¤å€¼
            self.enable_task_polling = getattr(config, 'enable_task_polling', False) if config else False
            self.enable_parallel_envs = getattr(config, 'enable_parallel_envs', False) if config else False
            self.enable_smart_sampling = getattr(config, 'enable_smart_sampling', False) if config else False
            # ğŸ†• æ–°å¢ï¼šçœŸæ­£å¤šè¿›ç¨‹å¹¶è¡Œå¼€å…³ï¼ˆé»˜è®¤å…³é—­ä»¥ä¿æŒå…¼å®¹æ€§ï¼‰
            self.enable_true_parallel_envs = getattr(config, 'enable_true_parallel_envs', False) if config else False
        
        if self.rank == 0:
            print(f"ğŸ”§ LIBEROEnvRunneråŠŸèƒ½å¼€å…³:")
            print(f"   ä»»åŠ¡è½®è¯¢: {'âœ…' if self.enable_task_polling else 'âŒ'}")
            print(f"   å¹¶è¡Œç¯å¢ƒ: {'âœ…' if self.enable_parallel_envs else 'âŒ'}")
            print(f"   çœŸæ­£å¤šè¿›ç¨‹å¹¶è¡Œ: {'âœ…' if self.enable_true_parallel_envs else 'âŒ'}")
            print(f"   æ™ºèƒ½é‡‡æ ·: {'âœ…' if self.enable_smart_sampling else 'âŒ'}")
        
        # ğŸ”¥ æ–°å¢ï¼šä»»åŠ¡è½®è¯¢æœºåˆ¶ (ä»…åœ¨å¼€å…³å¯ç”¨æ—¶åˆå§‹åŒ–)
        if self.enable_task_polling:
            self.assigned_tasks = task_names_to_use or []  
            self.task_cursor = 0                          
            self.current_task = None                      
            
            if self.assigned_tasks:
                self.current_task = self.assigned_tasks[0]
                if self.rank == 0:
                    print(f"ğŸ¯ ä»»åŠ¡è½®è¯¢åˆå§‹åŒ–: åˆ†é…ä»»åŠ¡ {self.assigned_tasks}, å½“å‰ä»»åŠ¡: {self.current_task}")
            else:
                if self.rank == 0:
                    print(f"âš ï¸ ä»»åŠ¡è½®è¯¢: Rank {self.rank} æ²¡æœ‰åˆ†é…ä»»åŠ¡")
        else:
            # ç®€å•æ¨¡å¼ï¼šä¸ä½¿ç”¨ä»»åŠ¡è½®è¯¢
            self.assigned_tasks = task_names_to_use or []
            self.current_task = task_names_to_use[0] if task_names_to_use else None
        
        # è°ƒè¯•é€‰é¡¹
        self.debug_save_images = DEBUG_SAVE_IMAGES
        # YAMLå¼€å…³ä¼˜å…ˆï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡
        yaml_save_video = False
        try:
            if config and hasattr(config, 'features'):
                yaml_save_video = bool(getattr(config.features, 'save_video', False))
        except Exception:
            yaml_save_video = False
        self.debug_save_video = bool(yaml_save_video) or DEBUG_SAVE_VIDEO
        self.debug_image_dir = DEBUG_IMAGE_DIR
        
        # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
        self._load_norm_stats(norm_stats_path)

    def _ensure_list_of_dict_obs(self, obs_any, env_num: int):
        """Normalize vector-env observation to List[Dict] per environment.

        Supports formats:
        - dict of batched arrays -> split along first dim
        - numpy array of objects (each a dict) -> list(obs)
        - single dict (non-batched) -> broadcast to env_num
        - fallback: wrap unknown type by broadcasting
        """
        try:
            import numpy as _np
        except Exception:
            _np = None

        # dict of batched arrays
        if isinstance(obs_any, dict):
            result = []
            for i in range(env_num):
                per_env = {}
                for k, v in obs_any.items():
                    try:
                        if hasattr(v, '__len__') and len(v) >= env_num:
                            per_env[k] = v[i]
                        else:
                            per_env[k] = v
                    except Exception:
                        per_env[k] = v
                result.append(per_env)
            return result

        # numpy array of objects (each is a dict)
        if _np is not None and isinstance(obs_any, _np.ndarray):
            if obs_any.dtype == object:
                return [obs_any[i] for i in range(min(len(obs_any), env_num))]
            # unknown numeric array; broadcast
            return [obs_any for _ in range(env_num)]

        # single dict -> broadcast
        if isinstance(obs_any, dict):
            return [obs_any for _ in range(env_num)]

        # list already -> assume per-env
        if isinstance(obs_any, list):
            return obs_any[:env_num] if len(obs_any) >= env_num else (obs_any + [obs_any[-1]] * (env_num - len(obs_any)))

        # fallback: broadcast
        return [obs_any for _ in range(env_num)]

    def _ensure_list_generic(self, value, env_num: int, fill_default, expect_dict: bool = False):
        """Normalize rewards/dones/infos to python list of length env_num.

        - If numpy array: convert to list; if too short, pad by last; if too long, slice.
        - If object array and expect_dict, ensure dict per item; otherwise cast as-is.
        - If single scalar/value, broadcast.
        """
        try:
            import numpy as _np
        except Exception:
            _np = None

        result = None
        if isinstance(value, list):
            result = value
        elif _np is not None and isinstance(value, _np.ndarray):
            # object array (e.g., dicts)
            if value.dtype == object:
                result = list(value)
            else:
                # numeric/bool array
                try:
                    result = value.tolist()
                except Exception:
                    result = [value]
        else:
            # single scalar/value -> broadcast
            result = [value for _ in range(env_num)]

        # adjust length
        if len(result) < env_num:
            pad_val = result[-1] if result else fill_default
            result = (result + [pad_val] * env_num)[:env_num]
        else:
            result = result[:env_num]

        if expect_dict:
            # ensure each item is a dict
            result = [item if isinstance(item, dict) else {} for item in result]

        return result
        
    # ğŸ”¥ æ–°å¢ï¼šä»»åŠ¡è½®è¯¢æ ¸å¿ƒæ–¹æ³• (ä»…åœ¨åŠŸèƒ½å¼€å…³å¯ç”¨æ—¶å¯ç”¨)
    def set_current_task_by_cursor(self):
        """æ ¹æ®cursorè®¾ç½®å½“å‰ä»»åŠ¡"""
        if not self.enable_task_polling:
            return self.current_task  # ç®€å•æ¨¡å¼ï¼šç›´æ¥è¿”å›å½“å‰ä»»åŠ¡
            
        if not self.assigned_tasks:
            self.current_task = None
            return None
        
        # ä½¿ç”¨æ¨¡è¿ç®—å®ç°å¾ªç¯è½®è¯¢
        self.current_task = self.assigned_tasks[self.task_cursor % len(self.assigned_tasks)]
        return self.current_task
    
    def advance_task_cursor(self):
        """æ¨è¿›ä»»åŠ¡cursoråˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡"""
        if not self.enable_task_polling:
            return self.current_task  # ç®€å•æ¨¡å¼ï¼šä¸æ¨è¿›ï¼Œè¿”å›å½“å‰ä»»åŠ¡
            
        if not self.assigned_tasks:
            return None
        
        self.task_cursor += 1
        new_task = self.set_current_task_by_cursor()
        
        # è°ƒè¯•è¾“å‡º
        if self.rank == 0 or len(self.assigned_tasks) > 1:  # ä¸»è¿›ç¨‹æˆ–å¤šä»»åŠ¡æ—¶è¾“å‡º
            print(f"ğŸ”„ Rank {self.rank}: ä»»åŠ¡cursoræ¨è¿›åˆ° {self.task_cursor}, å½“å‰ä»»åŠ¡: {new_task}")
        
        return new_task
    
    def get_current_task(self):
        """è·å–å½“å‰ä»»åŠ¡"""
        return self.current_task
    
    def has_tasks(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é…çš„ä»»åŠ¡"""
        return len(self.assigned_tasks) > 0
    
    # ğŸ”¥ æ–°å¢ï¼šä»»åŠ¡ç»Ÿè®¡å’Œé«˜çº§åŠŸèƒ½ (æ¨¡ä»¿åŸç‰ˆRIPT-VLA)
    def update_task_stats(self, task_name: str, success: bool, reward: float):
        """æ›´æ–°ä»»åŠ¡å®Œæˆç»Ÿè®¡"""
        if not hasattr(self, 'task_completion_stats'):
            self.task_completion_stats = {}
        
        if task_name not in self.task_completion_stats:
            self.task_completion_stats[task_name] = {
                'attempts': 0,
                'successes': 0,
                'total_reward': 0.0,
                'success_rate': 0.0
            }
        
        stats = self.task_completion_stats[task_name]
        stats['attempts'] += 1
        stats['total_reward'] += reward
        if success:
            stats['successes'] += 1
        stats['success_rate'] = stats['successes'] / stats['attempts'] if stats['attempts'] > 0 else 0.0
    
    def get_task_stats(self) -> dict:
        """è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        if not hasattr(self, 'task_completion_stats'):
            return {}
        return self.task_completion_stats.copy()
    
    def get_best_performing_task(self) -> Optional[str]:
        """è·å–è¡¨ç°æœ€å¥½çš„ä»»åŠ¡ï¼ˆç”¨äºæ™ºèƒ½é‡‡æ ·ï¼‰"""
        if not hasattr(self, 'task_completion_stats') or not self.task_completion_stats:
            return None
        
        best_task = None
        best_score = -1.0
        
        for task, stats in self.task_completion_stats.items():
            # ç»¼åˆè€ƒè™‘æˆåŠŸç‡å’Œå¹³å‡å¥–åŠ±
            if stats['attempts'] > 0:
                avg_reward = stats['total_reward'] / stats['attempts']
                score = stats['success_rate'] * 0.7 + min(avg_reward, 1.0) * 0.3
                if score > best_score:
                    best_score = score
                    best_task = task
        
        return best_task
    
    def setup_file_counter(self, counter_name: str = "rollout", work_dir: str = "./") -> Optional[object]:
        """è®¾ç½®æ–‡ä»¶è®¡æ•°å™¨ç”¨äºåˆ†å¸ƒå¼åè°ƒ"""
        try:
            from pi0.ript.algos.rl_optimizers.file_counter import setup_global_counter
            counter = setup_global_counter(counter_name, work_dir=work_dir)
            if self.rank == 0:
                print(f"âœ… æ–‡ä»¶è®¡æ•°å™¨è®¾ç½®æˆåŠŸ: {counter_name}")
            return counter
        except ImportError as e:
            if self.rank == 0:
                print(f"âš ï¸ æ–‡ä»¶è®¡æ•°å™¨ä¸å¯ç”¨: {e}")
            return None
        except Exception as e:
            if self.rank == 0:
                print(f"âš ï¸ æ–‡ä»¶è®¡æ•°å™¨åˆ›å»ºå¤±è´¥: {e}")
            return None
        
    def _load_norm_stats(self, norm_stats_path: Optional[str] = None):
        """Load normalization statistics from norm_stats.json"""
        if norm_stats_path is None:
            # å°è¯•åœ¨å¸¸è§ä½ç½®æ‰¾åˆ°norm_stats.json
            possible_paths = [
                "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch/norm_stats.json",
                "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json", 
                "./checkpoints/pi0_libero_pytorch/norm_stats.json",
                "./norm_stats.json"
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    norm_stats_path = path
                    break
        
        if norm_stats_path and Path(norm_stats_path).exists():
            print(f"LIBEROEnvRunner: Loading norm_stats from: {norm_stats_path}")
            with open(norm_stats_path) as f:
                norm_stats = json.load(f)
                
            # æå–çŠ¶æ€å’ŒåŠ¨ä½œçš„å½’ä¸€åŒ–å‚æ•°
            self.state_mean = np.array(norm_stats["norm_stats"]["state"]["mean"][:8], dtype=np.float32)
            self.state_std = np.array(norm_stats["norm_stats"]["state"]["std"][:8], dtype=np.float32)
            self.action_mean = np.array(norm_stats["norm_stats"]["actions"]["mean"][:7], dtype=np.float32)
            self.action_std = np.array(norm_stats["norm_stats"]["actions"]["std"][:7], dtype=np.float32)
            
            print(f"âœ… LIBEROEnvRunner: Loaded normalization stats for action post-processing")
        else:
            raise FileNotFoundError("norm_stats.json not found in expected locations; cannot proceed")


    
    def construct_pi0_observation(self, obs, task_description):
        """æ„é€ PI0éœ€è¦çš„è§‚æµ‹æ ¼å¼ï¼Œå®Œå…¨æ¨¡ä»¿2_test_pi0_on_libero.pyçš„åšæ³•"""
        # è·å–è®¾å¤‡
        device = self.policy.config.device if hasattr(self.policy, 'config') else 'cuda:0'
        
        # çŠ¶æ€å¤„ç†
        import robosuite.utils.transform_utils as T
        
        axis_angle = T.quat2axisangle(obs["robot0_eef_quat"])
            
        unnorm_state = np.concatenate([
            obs["robot0_eef_pos"],                    # 3D: end-effector position
            axis_angle, # 3D: rotation as axis-angle  
            obs["robot0_gripper_qpos"],               # 2D: gripper joint positions
        ], dtype=np.float32)
        print(">> gripper_qpos shape:", np.asarray(obs["robot0_gripper_qpos"]).shape)
        # çŠ¶æ€å½’ä¸€åŒ–
        state = (unnorm_state - self.state_mean) / (self.state_std + 1e-6)
        
        # å›¾åƒå¤„ç†
        base_0_rgb = obs["agentview_image"][:, :, ::-1].copy()
        left_wrist_0_rgb = obs["robot0_eye_in_hand_image"][:, :, ::-1].copy()
        
        # æ„é€ è§‚æµ‹æ ¼å¼
        observation = {
            "image": {
                "base_0_rgb": torch.from_numpy(base_0_rgb).to(device)[None],
                "left_wrist_0_rgb": torch.from_numpy(left_wrist_0_rgb).to(device)[None],
            },
            "state": torch.from_numpy(state).to(device)[None],
            # å§‹ç»ˆä½¿ç”¨è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ä½œä¸ºpromptï¼Œé¿å…ä½¿ç”¨env_nameç­‰çŸ­æ ‡è¯†
            "prompt": [task_description],
        }
        
        return observation
    
    def denormalize_action(self, action: np.ndarray) -> np.ndarray:
        """Denormalize action using loaded statistics"""
        return action * (self.action_std + 1e-6) + self.action_mean
    
    def get_unnormalized_state(self, obs) -> np.ndarray:
        """ä»è§‚æµ‹ä¸­æå–æœªå½’ä¸€åŒ–çš„çŠ¶æ€ï¼ˆç”¨äºåŠ¨ä½œåç§»ï¼‰"""
        try:
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            
            if not isinstance(obs, dict) or not obs:
                raise RuntimeError("Required observation dict missing or empty")
            
            # ä½¿ç”¨ end-effector ä¿¡æ¯
            if "robot0_eef_pos" in obs and "robot0_eef_quat" in obs:
                eef_pos = np.array(obs["robot0_eef_pos"], dtype=np.float32)
                eef_quat = np.array(obs["robot0_eef_quat"], dtype=np.float32)
                
                if eef_pos.size != 3 or eef_quat.size != 4:
                    raise RuntimeError("Observation fields robot0_eef_pos or robot0_eef_quat have incorrect dimensions")
                
                # è½¬æ¢å››å…ƒæ•°ä¸ºè½´è§’
                try:
                    axis_angle = T.quat2axisangle(eef_quat).astype(np.float32)
                except Exception as e:
                    raise RuntimeError(f"Failed to convert quaternion to axis-angle: {e}")
                
                # è·å– gripper çŠ¶æ€
                if "robot0_gripper_qpos" not in obs:
                    raise RuntimeError("Observation missing required field robot0_gripper_qpos")
                try:
                    gripper_qpos = float(obs["robot0_gripper_qpos"][0])
                except (IndexError, TypeError, ValueError) as e:
                    raise RuntimeError(f"Invalid robot0_gripper_qpos value: {e}")
                
                # æ„é€ æœªå½’ä¸€åŒ–çš„çŠ¶æ€
                unnorm_state = np.concatenate([eef_pos[:3], axis_angle[:3], [gripper_qpos]]).astype(np.float32)
                return unnorm_state
                
            # å®˜æ–¹è„šæœ¬è¦æ±‚ä¸Šè¿°å…³é”®å­—æ®µå‡å­˜åœ¨ï¼Œè‹¥ç¼ºå¤±åˆ™ç«‹å³æŠ¥é”™
            else:
                raise RuntimeError("Observation missing required end-effector fields")
                
        except Exception as e:
            raise
    
    def make_env(self, env_name: str):
        """åˆ›å»ºLIBEROç¯å¢ƒ"""
        try:
            import gym
            from cleandiffuser.env import libero  # ç¡®ä¿ç¯å¢ƒæ³¨å†Œ
            
            # ä½¿ç”¨ä¸å‚è€ƒå®ç°2_test_pi0_on_libero.pyå®Œå…¨ç›¸åŒçš„ç¯å¢ƒé…ç½®
            benchmark_to_env_id = {
                'libero_spatial': 'libero-spatial-v0',
                'libero_object': 'libero-object-v0',
                'libero_goal': 'libero-goal-v0',
                'libero_10': 'libero-10-v0',
                'libero_90': 'libero-90-v0'
            }

            if self.benchmark_name not in benchmark_to_env_id:
                raise ValueError(f"Unknown benchmark_name: {self.benchmark_name}")

            env_id = benchmark_to_env_id[self.benchmark_name]
            # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®ä»»åŠ¡åç§°åŠ¨æ€ç¡®å®štask_idï¼Œä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´
            # å¯¹äºlibero_goalï¼Œä½¿ç”¨task_id=1 (ä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´)
            if self.benchmark_name == "libero_goal":
                task_id = 1  # ä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´
            elif self.benchmark_name == "libero_spatial":
                task_id = 0  # ç¬¬ä¸€ä¸ªspatialä»»åŠ¡
            else:
                task_id = 0  # å…¶ä»–benchmarkçš„é»˜è®¤ä»»åŠ¡
            
            # åˆ›å»ºç¯å¢ƒ
            env = gym.make(
                env_id,
                task_id=task_id,
                image_size=224,  # åŒ¹é…PI0çš„è¾“å…¥å°ºå¯¸
                camera_names=["agentview", "robot0_eye_in_hand"],  # åŒ¹é…åŸå§‹demo
                seed=0,  # ä½¿ç”¨ä¸å‚è€ƒå®ç°ç›¸åŒçš„éšæœºç§å­
            )
            
            # è·å–ä»»åŠ¡æè¿°
            if hasattr(env, 'task_description'):
                task_description = env.task_description
            else:
                task_description = env_name
                
            return env, task_description
            
        except Exception as e:
            raise RuntimeError(f"åˆ›å»ºç¯å¢ƒå¤±è´¥: {e}") from e
        
    def run_policy_in_env(self, env_name, all_init_states=None, debug_save_video=None, created_env=None):
        """åœ¨ç¯å¢ƒä¸­è¿è¡Œç­–ç•¥ï¼Œç”Ÿæˆè½¨è¿¹ - æ”¯æŒå¹¶è¡Œå’Œä¸²è¡Œç¯å¢ƒ"""
        save_video = debug_save_video if debug_save_video is not None else self.debug_save_video
        
        # ğŸ”¥ æ ¹æ®åŠŸèƒ½å¼€å…³é€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œæ¨¡å¼
        if self.enable_parallel_envs and created_env is None:
            if self.rank == 0:
                print(f"ğŸš€ ä½¿ç”¨å¹¶è¡Œç¯å¢ƒæ¨¡å¼ (num_parallel_envs={self.num_parallel_envs})")
            # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
            env, env_id, env_num = self.create_parallel_envs(env_name)
            created_env = (env, env_id, env_num)
        elif created_env is None:
            if self.rank == 0:
                print(f"ğŸ”„ ä½¿ç”¨ä¸²è¡Œç¯å¢ƒæ¨¡å¼")
            # ä½¿ç”¨ä¸²è¡Œç¯å¢ƒ
            env, task_description = self.make_env(env_name)
            created_env = (env, env_name, 1)
        
        env, env_id, env_num = created_env
        
        if all_init_states is None:
            # å¦‚æœæ²¡æœ‰æä¾›åˆå§‹çŠ¶æ€ï¼Œç”Ÿæˆé»˜è®¤çŠ¶æ€
            all_init_states = np.zeros((self.rollouts_per_env, 8), dtype=np.float32)
        
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„å¹¶è¡Œç¯å¢ƒ
            is_vector_env = hasattr(env, 'num_envs') or 'VectorEnv' in str(type(env))
            
            if self.enable_parallel_envs and env_num > 1 and is_vector_env:
                # ä½¿ç”¨çœŸæ­£çš„å¹¶è¡Œç¯å¢ƒ
                if self.rank == 0:
                    print(f"ğŸš€ å¹¶è¡Œç¯å¢ƒæ¨¡å¼: {env_num} ä¸ªç¯å¢ƒ")
                yield from self._run_parallel_episodes(env, env_name, all_init_states, env_num, save_video)
            else:
                # ä½¿ç”¨å•ç¯å¢ƒæ¨¡å¼
                if self.rank == 0:
                    print(f"ğŸ”„ å•ç¯å¢ƒæ¨¡å¼")
                yield from self._run_serial_episodes(env, env_name, all_init_states, save_video)
        
        finally:
            # æ¸…ç†ç¯å¢ƒ
            try:
                if hasattr(env, 'close'):
                    env.close()
                if VECTOR_ENV_AVAILABLE:
                    import gc
                    gc.collect()
            except Exception as e:
                if self.rank == 0:
                    print(f"ç¯å¢ƒæ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def _run_serial_episodes(self, env, env_name, all_init_states, save_video):
        """è¿è¡Œä¸²è¡Œ episodesï¼ˆå¤„ç†å•ä¸ªç¯å¢ƒå’Œå•ä¸ªVectorEnvï¼‰"""
        if self.rank == 0:
            print(f"ä½¿ç”¨ä¸²è¡Œæ¨¡å¼æ‰§è¡Œ {len(all_init_states)} ä¸ªepisodes")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºVectorEnvç±»å‹
        is_vector_env = hasattr(env, 'num_envs') or 'VectorEnv' in str(type(env))
        if is_vector_env:
            if self.rank == 0:
                print(f"æ£€æµ‹åˆ°VectorEnvï¼Œä½¿ç”¨å•ä¸ªç¯å¢ƒæ¨¡å¼")
        
        for i, init_state in enumerate(all_init_states):
            rollout_images = []
            
            # é‡ç”¨ä¼ å…¥çš„ç¯å¢ƒæˆ–åˆ›å»ºæ–°ç¯å¢ƒ
            if hasattr(env, 'task_description'):
                task_description = env.task_description
            else:
                task_description = env_name
            
            # è®¾ç½®åˆå§‹çŠ¶æ€å¹¶è·å–åˆå§‹è§‚æµ‹
            obs = env.reset()
            
            # å¦‚æœæ˜¯VectorEnvï¼Œå–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            if is_vector_env and isinstance(obs, list):
                obs = obs[0]
            
            # çƒ­æœºæ­¥éª¤
            dummy_action = np.array([0, 0, 0, 0, 0, 0, -1])
            for warmup_step in range(20):
                if is_vector_env:
                    # VectorEnvæœŸæœ›åŠ¨ä½œåˆ—è¡¨
                    step_results = env.step([dummy_action])
                    obs, _, _, _ = step_results
                    if isinstance(obs, list):
                        obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„ç»“æœ
                else:
                    obs, _, _, _ = env.step(dummy_action)
            
            step = 0
            done = False
            total_reward = 0
            success = False
            
            # æ”¶é›†åˆå§‹è§‚æµ‹å›¾åƒç”¨äºè§†é¢‘
            if save_video:
                initial_img = obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                rollout_images.append(initial_img)
            
            # æ”¶é›†è½¨è¿¹
            observations = [obs]
            actions = []
            rewards = []
            dones = []
            infos = []
            
            try:
                action_buffer = None
                action_index = 0
                
                # æ‰§è¡Œç­–ç•¥ç›´åˆ°å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°
                while not done and step < self.max_steps:
                    # åªåœ¨éœ€è¦æ—¶æ¨ç†ï¼ˆåŠ¨ä½œé˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼‰
                    if action_buffer is None or action_index >= action_buffer.shape[0]:
                        # ä½¿ç”¨æ­£ç¡®çš„PI0è§‚æµ‹æ ¼å¼
                        pi0_observation = self.construct_pi0_observation(obs, task_description)
                        
                        # é€‰æ‹©åŠ¨ä½œ
                        raw_action = self.policy.select_action(pi0_observation)
                        action = raw_action[0, :, :7]  # shape: (50, 7)
                        
                        # è½¬æ¢ä¸ºnumpyå¹¶å¤„ç†ç»´åº¦
                        if isinstance(action, torch.Tensor):
                            action_after_cpu = action.cpu().numpy()
                        else:
                            action_after_cpu = action
                        
                        # åå½’ä¸€åŒ–åŠ¨ä½œ
                        action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                        
                        # è·å–å½“å‰æœªå½’ä¸€åŒ–çŠ¶æ€ç”¨äºåç§»
                        import robosuite.utils.transform_utils as T
                        unnorm_state = np.concatenate([
                            obs["robot0_eef_pos"],
                            T.quat2axisangle(obs["robot0_eef_quat"]),
                            obs["robot0_gripper_qpos"],
                        ], dtype=np.float32)
                        
                        # åº”ç”¨çŠ¶æ€åç§»ï¼ˆå‰6ç»´ï¼‰
                        action_buffer[:, :6] += unnorm_state[None, :6]
                        
                        # é‡ç½®åŠ¨ä½œç´¢å¼•
                        action_index = 0
                    
                    # ä»åŠ¨ä½œé˜Ÿåˆ—ä¸­å–å‡ºå½“å‰åŠ¨ä½œæ‰§è¡Œ
                    current_action = action_buffer[action_index, :7]
                    
                    # æ‰§è¡Œå•æ­¥åŠ¨ä½œ
                    if is_vector_env:
                        # VectorEnvæœŸæœ›åŠ¨ä½œåˆ—è¡¨
                        step_results = env.step([current_action])
                        next_obs, reward, done, info = step_results
                        # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„ç»“æœ
                        if isinstance(next_obs, list):
                            next_obs = next_obs[0]
                        if isinstance(reward, list):
                            reward = reward[0]
                        if isinstance(done, list):
                            done = done[0]
                        if isinstance(info, list):
                            info = info[0]
                    else:
                        next_obs, reward, done, info = env.step(current_action)
                    
                    # è®°å½•è½¨è¿¹æ•°æ®
                    observations.append(next_obs)
                    actions.append(current_action)
                    rewards.append(reward)
                    dones.append(done)
                    infos.append(info)
                    
                    # æ›´æ–°ç´¯è®¡å¥–åŠ±
                    total_reward += reward
                    
                    # æ”¶é›†å›¾åƒç”¨äºè§†é¢‘
                    if save_video:
                        frame_img = next_obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                        rollout_images.append(frame_img)
                    
                    # æ›´æ–°çŠ¶æ€å’Œè®¡æ•°å™¨
                    obs = next_obs
                    step += 1
                    action_index += 1
                    
                    # æ£€æŸ¥æˆåŠŸçŠ¶æ€ - ä¼˜å…ˆä½¿ç”¨infoä¸­çš„successï¼Œå¦åˆ™åŸºäºå¥–åŠ±åˆ¤æ–­ï¼ˆä¸4è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
                    if info.get("success", False):
                        success = True
                    # å¦‚æœinfoä¸­æ²¡æœ‰successå­—æ®µï¼Œä½¿ç”¨ä¸4_simple_train_ript.pyç›¸åŒçš„åˆ¤æ–­é€»è¾‘
                    elif total_reward > 0.5:
                        success = True
                    
                    # å¦‚æœä»»åŠ¡å®Œæˆï¼Œæå‰é€€å‡º
                    if done:
                        break
            
            except Exception as e:
                print(f"æ‰§è¡Œç¯å¢ƒæ­¥éª¤æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                # å…³é—­ç¯å¢ƒå¹¶é‡Šæ”¾èµ„æº
                try:
                    env.close()
                except:
                    pass
            
            # ä¿å­˜æ•´ä¸ªè½¨è¿¹çš„è§†é¢‘
            if save_video and rollout_images:
                try:
                    # åˆ›å»ºè§†é¢‘ç›®å½•
                    video_dir = Path("pi0/ript/debug_images/videos")
                    video_dir.mkdir(parents=True, exist_ok=True)
                    
                    # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    task_str = task_description.replace(" ", "_")[:30] if isinstance(task_description, str) else f"episode_{i}"
                    success_str = "success" if success else "failure"
                    video_path = video_dir / f"{timestamp}_{task_str}_{success_str}.mp4"
                    
                    # ä¿å­˜è§†é¢‘
                    writer = imageio.get_writer(str(video_path), fps=30)
                    for frame in rollout_images:
                        writer.append_data(frame)
                    writer.close()
                    
                    print(f"âœ… å·²ä¿å­˜è§†é¢‘: {video_path}")
                except Exception as e:
                    print(f"ä¿å­˜è§†é¢‘å‡ºé”™: {e}")
            
            # æ”¶é›†episodeæ•°æ®
            episode_data = {
                "observations": observations,
                "actions": actions,
                "rewards": rewards,
                "dones": dones,
                "infos": infos,
                "task": task_description,
            }
            
            # è¿”å›è½¨è¿¹ç»“æœ
            yield (success, total_reward, episode_data)
    
    # ğŸ”¥ æ–°å¢ï¼šå¹¶è¡Œç¯å¢ƒæ”¯æŒ (ä»…åœ¨åŠŸèƒ½å¼€å…³å¯ç”¨æ—¶å¯ç”¨)
    def create_env(self, env_name: str):
        """åˆ›å»ºå•ä¸ªç¯å¢ƒï¼ˆç”¨äºå¹¶è¡Œç¯å¢ƒï¼‰"""
        if not self.enable_parallel_envs:
            return self.make_env(env_name)  # ç®€å•æ¨¡å¼ï¼šç›´æ¥è¿”å›å•ä¸ªç¯å¢ƒ
            
        env, task_description = self.make_env(env_name)
        return env, env_name, 1
    
    def create_parallel_envs(self, env_name: str):
        """åˆ›å»ºå¹¶è¡Œç¯å¢ƒ"""
        if not self.enable_parallel_envs or not VECTOR_ENV_AVAILABLE or self.num_parallel_envs <= 1:
            # å¦‚æœä¸æ”¯æŒå¹¶è¡Œç¯å¢ƒæˆ–åªéœ€è¦1ä¸ªç¯å¢ƒï¼Œä½¿ç”¨å•ä¸ªç¯å¢ƒ
            if self.rank == 0:
                print(f"ä½¿ç”¨å•ä¸ªç¯å¢ƒ (num_parallel_envs={self.num_parallel_envs})")
            env, task_description = self.make_env(env_name)
            return env, env_name, 1
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨çœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œ
        if self.enable_true_parallel_envs:
            return self._create_true_parallel_envs(env_name)
        else:
            # å›é€€åˆ°å•ç¯å¢ƒ
            return self._create_single_env(env_name)
    
    def _create_single_env(self, env_name: str):
        """åˆ›å»ºå•ä¸ªç¯å¢ƒ - ç®€å•ç›´æ¥"""
        if self.rank == 0:
            print(f"ğŸ”„ ä½¿ç”¨å•ç¯å¢ƒæ¨¡å¼")
        
        env, task_description = self.make_env(env_name) 
        return env, env_name, 1
    
    def _create_true_parallel_envs(self, env_name: str):
        """ğŸ†• åˆ›å»ºçœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œç¯å¢ƒ - ä½¿ç”¨ç‹¬ç«‹ç¯å¢ƒå·¥å‚è§£å†³åºåˆ—åŒ–é—®é¢˜"""
        if not TRUE_PARALLEL_AVAILABLE:
            if self.rank == 0:
                print(f"âš ï¸ ç‹¬ç«‹ç¯å¢ƒå·¥å‚ä¸å¯ç”¨ï¼Œå›é€€åˆ°å•ç¯å¢ƒæ¨¡å¼")
            return self._create_single_env(env_name)
        
        # è®¡ç®—GPUå†…å­˜éœ€æ±‚
        model_size_gb = 3.5  # PI0æ¨¡å‹å¤§å°
        required_memory_gb = self.num_parallel_envs * model_size_gb
        
        # æ£€æŸ¥GPUå†…å­˜
        available_memory_gb = 0
        current_usage = 0
        if torch.cuda.is_available():
            available_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            current_usage = torch.cuda.memory_allocated(0) / (1024**3)
            available_memory = available_memory_gb - current_usage
        
        if self.rank == 0:
            print(f"ğŸš€ å°è¯•åˆ›å»ºçœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œç¯å¢ƒ:")
            print(f"ğŸ§  å†…å­˜åˆ†æ:")
            print(f"   {self.num_parallel_envs}ä¸ªå¹¶è¡Œç¯å¢ƒç†è®ºéœ€è¦: {required_memory_gb:.1f}GB")
            print(f"   å½“å‰GPUæ€»å†…å­˜: {available_memory_gb:.1f}GB") 
            print(f"   å½“å‰GPUå·²ä½¿ç”¨: {current_usage:.1f}GB")
            print(f"   å¯ç”¨å†…å­˜: {available_memory:.1f}GB")
        
        # å†…å­˜å®‰å…¨æ£€æŸ¥
        if available_memory < required_memory_gb * 1.2:
            if self.rank == 0:
                print(f"âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°å•ç¯å¢ƒæ¨¡å¼")
            return self._create_single_env(env_name)
        
        try:
            # ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ç‹¬ç«‹ç¯å¢ƒå·¥å‚ï¼Œé¿å…åºåˆ—åŒ–selfå¯¹è±¡
            env_factory = create_env_factory(
                benchmark_name=self.benchmark_name,
                env_name=env_name,
                task_id=None  # è‡ªåŠ¨æ¨æ–­
            )
            
            # åˆ›å»ºå¤šä¸ªç¯å¢ƒå·¥å‚å®ä¾‹
            env_factories = [env_factory for _ in range(self.num_parallel_envs)]
            
            if self.rank == 0:
                print(f"ğŸ”§ åˆ›å»º {self.num_parallel_envs} ä¸ªç‹¬ç«‹å¹¶è¡Œç¯å¢ƒ...")
            
            # è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•
            if multiprocessing.get_start_method(allow_none=True) != 'spawn':
                multiprocessing.set_start_method('spawn', force=True)
            
            # åˆ›å»ºSubprocVectorEnv
            parallel_env = SubprocVectorEnv(env_factories)
            
            # ğŸ” ä»…è®°å½•resetè¿”å›ä¿¡æ¯ï¼Œä¸åšå¼ºæ ¡éªŒä»¥é€‚é…ä¸åŒå®ç°
            try:
                test_obs = parallel_env.reset()
                if self.rank == 0:
                    if isinstance(test_obs, list):
                        print(f"âœ… SubprocVectorEnvå·²åˆ›å»ºï¼Œresetè¿”å›listï¼Œé•¿åº¦: {len(test_obs)}")
                    else:
                        print(f"âœ… SubprocVectorEnvå·²åˆ›å»ºï¼Œresetè¿”å›ç±»å‹: {type(test_obs)}")
            except Exception as e:
                if self.rank == 0:
                    print(f"âš ï¸ SubprocVectorEnv.reset() è°ƒç”¨å¼‚å¸¸: {e}")
            
            if self.rank == 0:
                print(f"   ğŸ”„ {self.num_parallel_envs} ä¸ªç‹¬ç«‹å­è¿›ç¨‹")
                print(f"   ğŸ§  æ¯ä¸ªå­è¿›ç¨‹æ— æ¨¡å‹ï¼Œæ€»å†…å­˜èŠ‚çœ ~{(self.num_parallel_envs-1)*3.5:.1f}GB")
                print(f"   âš¡ çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œï¼Œæ€§èƒ½æå‡ ~{self.num_parallel_envs}x")
            
            # ğŸ§  è·å–æ¯ä¸ªå­è¿›ç¨‹çš„ä»»åŠ¡æè¿°ï¼ˆè‡ªç„¶è¯­è¨€promptï¼‰ï¼Œç”¨äºæ„é€ æ¨¡å‹è¾“å…¥
            try:
                vector_prompts = []
                for w in getattr(parallel_env, 'workers', []):
                    desc = None
                    try:
                        desc = w.get_env_attr('task_description')
                    except Exception:
                        desc = None
                    vector_prompts.append(desc if isinstance(desc, str) and len(desc) > 0 else env_name)
                self._vector_env_prompts = vector_prompts
            except Exception:
                self._vector_env_prompts = [env_name for _ in range(self.num_parallel_envs)]

            return parallel_env, env_name, self.num_parallel_envs
            
        except Exception as e:
            if self.rank == 0:
                print(f"âŒ çœŸæ­£å¹¶è¡Œç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
                print(f"ğŸ”„ å›é€€åˆ°å•ç¯å¢ƒæ¨¡å¼")
                import traceback
                traceback.print_exc()
            
            # æ¸…ç†å¯èƒ½çš„èµ„æº
            try:
                if 'parallel_env' in locals():
                    parallel_env.close()
            except:
                pass
            
            # å›é€€åˆ°å•ç¯å¢ƒ
            return self._create_single_env(env_name)
    
    # âŒ è½»é‡çº§å¹¶è¡Œç¯å¢ƒæ–¹æ¡ˆå·²è¢«ç§»é™¤
    # åŸå› ï¼šCloudPickleåºåˆ—åŒ–æ—¶ä¼šåŒ…å«åŒ…å«Policyçš„selfå¯¹è±¡å¼•ç”¨
    # æ— è®ºå¦‚ä½•æ„é€ ç¯å¢ƒå‡½æ•°ï¼Œéƒ½ä¼šå¯¼è‡´æ¨¡å‹åœ¨æ¯ä¸ªå­è¿›ç¨‹ä¸­é‡å¤åŠ è½½
    # è¿™æ˜¯SubprocVectorEnv + PyTorchæ¨¡å‹çš„å›ºæœ‰é™åˆ¶
    
    def _deprecated_lightweight_parallel_note(self):
        """è®°å½•è½»é‡çº§å¹¶è¡Œæ–¹æ¡ˆå¤±è´¥çš„åŸå› """
        return """
        è½»é‡çº§å¹¶è¡Œæ–¹æ¡ˆå¤±è´¥åŸå› åˆ†æï¼š
        
        1. CloudPickleåºåˆ—åŒ–é—®é¢˜ï¼š
           - SubprocVectorEnvä½¿ç”¨CloudPickleåºåˆ—åŒ–ç¯å¢ƒæ„é€ å‡½æ•°
           - å³ä½¿create_env_without_modelä¸ç›´æ¥å¼•ç”¨self.policy
           - ä½†å‡½æ•°é—­åŒ…ä¸­åŒ…å«selfå¯¹è±¡çš„å¼•ç”¨
           - CloudPickleä¼šé€’å½’åºåˆ—åŒ–selfåŠå…¶åŒ…å«çš„policyå¯¹è±¡
        
        2. GPUå†…å­˜é—®é¢˜ï¼š
           - æ¯ä¸ªå­è¿›ç¨‹ååºåˆ—åŒ–æ—¶éƒ½ä¼šé‡å»ºå®Œæ•´çš„PI0æ¨¡å‹
           - 3ä¸ªè¿›ç¨‹ Ã— 3.5GB = 10.5GB GPUå†…å­˜éœ€æ±‚
           - è¶…è¿‡å•å¡å†…å­˜é™åˆ¶å¯¼è‡´CUDA OOM
        
        3. è§£å†³æ–¹æ¡ˆï¼š
           - ä½¿ç”¨æ‰¹å¤„ç†æ¨¡æ‹Ÿå¹¶è¡Œç­–ç•¥
           - å•ç¯å¢ƒ + æ™ºèƒ½æ‰¹é‡å¤„ç†
           - å†…å­˜å®‰å…¨ä¸”æ€§èƒ½æ¥è¿‘çœŸå®å¹¶è¡Œ
        """
    
    def _run_parallel_episodes(self, env, env_name, all_init_states, env_num, save_video):
        """è¿è¡ŒçœŸæ­£çš„å¹¶è¡Œ episodes"""
        if self.rank == 0:
            print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {env_num} ä¸ªç¯å¢ƒ")
        
        # è®¡ç®—éœ€è¦çš„è½®æ¬¡
        eval_loop_num = (self.rollouts_per_env + env_num - 1) // env_num
        count = 0
        
        while count < eval_loop_num:
            # é€‰æ‹©å½“å‰è½®æ¬¡çš„åˆå§‹çŠ¶æ€
            start_idx = count * env_num
            end_idx = min(start_idx + env_num, len(all_init_states))
            indices = np.arange(start_idx, end_idx) % len(all_init_states)
            current_init_states = all_init_states[indices]
            
            if self.rank == 0:
                print(f"å¹¶è¡Œè½®æ¬¡ {count+1}/{eval_loop_num}, çŠ¶æ€ç´¢å¼•: {indices}")
            
            # å¹¶è¡Œæ‰§è¡Œ episodes
            try:
                results = self._run_single_parallel_batch(env, env_name, current_init_states, env_num, save_video)
                
                # è¿”å›ç»“æœ
                for result in results:
                    yield result
                    
            except Exception as e:
                if self.rank == 0:
                    print(f"å¹¶è¡Œæ‰¹æ¬¡æ‰§è¡Œå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            
            count += 1
    
    def _run_single_parallel_batch(self, env, env_name, init_states, env_num, save_video):
        """æ‰§è¡Œå•ä¸ªå¹¶è¡Œæ‰¹æ¬¡ - æ”¯æŒè½»é‡çº§å¹¶è¡Œç¯å¢ƒ"""
        # æ£€æŸ¥ç¯å¢ƒç±»å‹
        is_vector_env = hasattr(env, 'num_envs') or 'VectorEnv' in str(type(env))
        
        if not is_vector_env or env_num == 1:
            # ä½¿ç”¨æ‰¹å¤„ç†æ¨¡æ‹Ÿå¹¶è¡Œï¼ˆæ¨èç­–ç•¥ï¼‰
            return self._run_batch_simulated_parallel(env, env_name, init_states, save_video)
        
        # çœŸå®å¹¶è¡Œç¯å¢ƒå¤„ç†
        if self.rank == 0:
            print(f"ğŸš€ æ‰§è¡ŒçœŸå®å¹¶è¡Œå¤„ç† ({env_num} ä¸ªç¯å¢ƒ)")
        
        # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
        obs_any = env.reset()
        # é»˜è®¤ä¸åœ¨å¹¶è¡Œæ¨¡å¼ä½¿ç”¨ set_init_stateï¼ˆé¿å… MuJoCo qpos ç»´åº¦ä¸åŒ¹é…å¯¼è‡´å­è¿›ç¨‹å´©æºƒï¼‰
        use_parallel_init = False
        try:
            if getattr(self, 'config', None) and hasattr(self.config, 'features'):
                use_parallel_init = bool(getattr(self.config.features, 'use_parallel_init_state', False))
        except Exception:
            use_parallel_init = False
        if use_parallel_init and init_states is not None:
            try:
                obs_any = env.set_init_state(init_states)
            except Exception as e:
                if self.rank == 0:
                    print(f"âš ï¸ set_init_state è°ƒç”¨å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨é»˜è®¤åˆå§‹çŠ¶æ€: {e}")
        else:
            if self.rank == 0 and init_states is not None:
                print("â„¹ï¸ å¹¶è¡Œæ¨¡å¼ä¸‹å·²è·³è¿‡ set_init_stateï¼ˆuse_parallel_init_state=falseï¼‰")
        obs_list = self._ensure_list_of_dict_obs(obs_any, env_num)
        
        if self.rank == 0:
            print(f"ğŸ”§ åˆå§‹åŒ– {len(obs_list)} ä¸ªå¹¶è¡Œç¯å¢ƒ")
        
        # å¯¹æ¯ä¸ªç¯å¢ƒè¿›è¡Œçƒ­èº«
        dummy_action = np.array([0, 0, 0, 0, 0, 0, -1])
        for warmup_step in range(20):
            # ğŸ”‘ ç¡®ä¿actionsæ•°ç»„é•¿åº¦ä¸ç¯å¢ƒæ•°é‡å®Œå…¨åŒ¹é…
            actions = [dummy_action.copy() for _ in range(env_num)]
            if self.rank == 0 and warmup_step == 0:
                print(f"ğŸ”§ çƒ­èº«åŠ¨ä½œ: {len(actions)} ä¸ªåŠ¨ä½œ for {env_num} ä¸ªç¯å¢ƒ")
            step_out = env.step(actions)
            obs_any = step_out[0] if isinstance(step_out, (list, tuple)) and len(step_out) >= 1 else step_out
            obs_list = self._ensure_list_of_dict_obs(obs_any, env_num)
        
        # åˆå§‹åŒ–episodeæ•°æ®
        episodes_data = []
        for i in range(len(obs_list)):
            episodes_data.append({
                'observations': [obs_list[i]],
                'actions': [],
                'rewards': [],
                'dones': [],
                'infos': [],
                'success': False,
                'total_reward': 0.0,
                'step': 0,
                'action_buffer': None,
                'action_index': 0,
                'rollout_images': [] if save_video else None,  # ğŸ¬ è§†é¢‘å¸§å­˜å‚¨
                'completed': False
            })
            
            # ğŸ¬ æ”¶é›†åˆå§‹è§‚æµ‹å›¾åƒç”¨äºè§†é¢‘
            if save_video:
                try:
                    initial_img = obs_list[i]["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                    episodes_data[i]['rollout_images'].append(initial_img)
                except Exception as e:
                    if self.rank == 0:
                        print(f"âš ï¸ æ”¶é›†åˆå§‹å›¾åƒå¤±è´¥ (ç¯å¢ƒ{i}): {e}")
                    episodes_data[i]['rollout_images'] = None
        
        max_steps = self.max_steps
        all_done = False
        
        # å¹¶è¡Œæ‰§è¡Œsteps - å…³é”®ä¼˜åŒ–ï¼šæ¨¡å‹æ¨ç†åœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œ
        while not all_done:
            # ğŸ”¥ æ‰¹é‡å¤„ç†æ‰€æœ‰ç¯å¢ƒçš„è§‚æµ‹ (é¿å…é€ä¸ªæ¨ç†)
            need_inference_indices = []
            observations_to_infer = []
            
            # ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«éœ€è¦æ¨ç†çš„ç¯å¢ƒ
            for i, obs in enumerate(obs_list):
                episode = episodes_data[i]
                
                if episode['dones'] and len(episode['dones']) > 0 and episode['dones'][-1]:
                    # ç¯å¢ƒå·²å®Œæˆï¼Œè·³è¿‡
                    continue
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¨ç†
                if (episode['action_buffer'] is None or 
                    episode['action_index'] >= episode['action_buffer'].shape[0]):
                    need_inference_indices.append(i)
                    observations_to_infer.append(obs)
            
            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡æ¨ç† (å…³é”®ä¼˜åŒ–: ä¸€æ¬¡æ¨ç†å¤šä¸ªè§‚æµ‹)
            if observations_to_infer:
                prompts_for_obs = None
                if hasattr(self, '_vector_env_prompts'):
                    prompts_for_obs = [self._vector_env_prompts[i] if i < len(self._vector_env_prompts) else env_name for i in need_inference_indices]
                batch_actions = self._batch_policy_inference(observations_to_infer, env_name, prompts_for_obs)
                
                # åˆ†é…æ¨ç†ç»“æœåˆ°å¯¹åº”çš„episode
                for batch_idx, env_idx in enumerate(need_inference_indices):
                    episode = episodes_data[env_idx]
                    obs = obs_list[env_idx]
                    
                    # å¤„ç†åŠ¨ä½œ
                    action_buffer = batch_actions[batch_idx]
                    
                    # è·å–çŠ¶æ€åç§»
                    import robosuite.utils.transform_utils as T
                    unnorm_state = np.concatenate([
                        obs["robot0_eef_pos"],
                        T.quat2axisangle(obs["robot0_eef_quat"]),
                        obs["robot0_gripper_qpos"],
                    ], dtype=np.float32)
                    
                    action_buffer[:, :6] += unnorm_state[None, :6]
                    
                    episode['action_buffer'] = action_buffer
                    episode['action_index'] = 0
            
            # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ‰€æœ‰ç¯å¢ƒæ„å»ºåŠ¨ä½œæ•°ç»„ï¼ˆç¡®ä¿é¡ºåºå’Œæ•°é‡åŒ¹é…ï¼‰
            actions_to_execute = []
            for i, obs in enumerate(obs_list):
                episode = episodes_data[i]
                
                if episode['completed'] or (episode['dones'] and len(episode['dones']) > 0 and episode['dones'][-1]):
                    # ç¯å¢ƒå·²å®Œæˆï¼Œä½¿ç”¨dummyåŠ¨ä½œ
                    actions_to_execute.append(dummy_action)
                else:
                    # è·å–å½“å‰åŠ¨ä½œï¼ˆåº”è¯¥å·²ç»æœ‰action_bufferäº†ï¼‰
                    if episode['action_buffer'] is not None:
                        current_action = episode['action_buffer'][episode['action_index'], :7]
                        actions_to_execute.append(current_action)
                        episode['action_index'] += 1
                    else:
                        # å¤‡ç”¨ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰action_bufferï¼Œä½¿ç”¨dummyåŠ¨ä½œ
                        actions_to_execute.append(dummy_action)
            
            # å¹¶è¡Œæ‰§è¡ŒåŠ¨ä½œ
            step_out = env.step(actions_to_execute)
            if isinstance(step_out, (list, tuple)) and len(step_out) >= 4:
                obs_any, rewards_any, dones_any, infos_any = step_out[:4]
            else:
                obs_any, rewards_any, dones_any, infos_any = step_out, 0.0, False, {}

            obs_list = self._ensure_list_of_dict_obs(obs_any, env_num)
            rewards = self._ensure_list_generic(rewards_any, env_num, 0.0, expect_dict=False)
            dones = self._ensure_list_generic(dones_any, env_num, False, expect_dict=False)
            infos = self._ensure_list_generic(infos_any, env_num, {}, expect_dict=True)
            
            # æ›´æ–°episodeæ•°æ®
            all_done = True
            for i in range(len(obs_list)):
                episode = episodes_data[i]
                
                # å·²å®Œæˆçš„ç¯å¢ƒä¸å†è¿½åŠ ä»»ä½•æ•°æ®ï¼Œä¿æŒä¸å•ç¯å¢ƒé€»è¾‘ä¸€è‡´ï¼ˆå®Œæˆå³åœæ­¢ï¼‰
                if episode['completed']:
                    continue
                
                episode['observations'].append(obs_list[i])
                episode['actions'].append(actions_to_execute[i])
                episode['rewards'].append(rewards[i])
                episode['dones'].append(dones[i])
                episode['infos'].append(infos[i])
                episode['total_reward'] += rewards[i]
                episode['step'] += 1
                
                # ğŸ¬ æ”¶é›†å›¾åƒç”¨äºè§†é¢‘
                if save_video and episode['rollout_images'] is not None:
                    try:
                        frame_img = obs_list[i]["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                        episode['rollout_images'].append(frame_img)
                    except Exception as e:
                        if self.rank == 0:
                            print(f"âš ï¸ æ”¶é›†å›¾åƒå¸§å¤±è´¥ (ç¯å¢ƒ{i}): {e}")
                
                if infos[i].get("success", False) or episode['total_reward'] > 0.5:
                    episode['success'] = True
                
                # å®Œæˆæ¡ä»¶ï¼šæœ¬æ­¥è¿”å› done æˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°
                if dones[i] or episode['step'] >= max_steps:
                    episode['completed'] = True
                
            # åªè¦å­˜åœ¨ä»»ä¸€æœªå®Œæˆçš„ç¯å¢ƒï¼Œåˆ™ç»§ç»­å¾ªç¯
            for i in range(len(obs_list)):
                if not episodes_data[i]['completed']:
                    all_done = False
                    break
        
        # ğŸ¬ ä¿å­˜è§†é¢‘å¹¶è¿”å›ç»“æœ
        results = []
        for i, episode in enumerate(episodes_data):
            # ä¿å­˜è§†é¢‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            video_path = None
            if save_video and episode['rollout_images'] and len(episode['rollout_images']) > 0:
                try:
                    from datetime import datetime
                    from pathlib import Path
                    import imageio
                    
                    # åˆ›å»ºè§†é¢‘ç›®å½•
                    video_dir = Path("pi0/ript/debug_images/videos")
                    video_dir.mkdir(parents=True, exist_ok=True)
                    
                    # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    task_str = env_name.replace(" ", "_")[:30] if isinstance(env_name, str) else f"parallel_env_{i}"
                    success_str = "success" if episode['success'] else "failure"
                    video_path = video_dir / f"{timestamp}_{task_str}_env{i}_{success_str}.mp4"
                    
                    # ä¿å­˜è§†é¢‘
                    writer = imageio.get_writer(str(video_path), fps=30)
                    for frame in episode['rollout_images']:
                        writer.append_data(frame)
                    writer.close()
                    
                    if self.rank == 0:
                        print(f"âœ… å·²ä¿å­˜è§†é¢‘ (ç¯å¢ƒ{i}): {video_path}")
                except Exception as e:
                    if self.rank == 0:
                        print(f"âš ï¸ ä¿å­˜è§†é¢‘å¤±è´¥ (ç¯å¢ƒ{i}): {e}")
            
            episode_data = {
                "observations": episode['observations'],
                "actions": episode['actions'],
                "rewards": episode['rewards'],
                "dones": episode['dones'],
                "infos": episode['infos'],
                "task": self._vector_env_prompts[i] if hasattr(self, '_vector_env_prompts') and i < len(self._vector_env_prompts) else env_name,
            }
            
            # æ·»åŠ è§†é¢‘è·¯å¾„ä¿¡æ¯
            if video_path:
                episode_data["video_path"] = str(video_path)
            
            results.append((episode['success'], episode['total_reward'], episode_data))
        
        return results
    
    def _batch_policy_inference(self, observations, env_name, prompts_for_obs=None):
        """æ‰¹é‡æ¨¡å‹æ¨ç† - é¿å…é€ä¸ªæ¨ç†æé«˜æ•ˆç‡"""
        if not observations:
            return []
        
        try:
            # æ„å»ºæ‰¹é‡è§‚æµ‹
            batch_obs = []
            for idx, obs in enumerate(observations):
                prompt_text = None
                if prompts_for_obs is not None and idx < len(prompts_for_obs):
                    prompt_text = prompts_for_obs[idx]
                pi0_obs = self.construct_pi0_observation(obs, prompt_text or env_name)
                batch_obs.append(pi0_obs)
            
            # åˆå¹¶æ‰¹é‡è§‚æµ‹ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            if len(batch_obs) == 1:
                # å•ä¸ªè§‚æµ‹ç›´æ¥æ¨ç†
                raw_action = self.policy.select_action(batch_obs[0])
                action = raw_action[0, :, :7]  # (50, 7)
                
                if isinstance(action, torch.Tensor):
                    action_after_cpu = action.cpu().numpy()
                else:
                    action_after_cpu = action
                
                action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                return [action_buffer]
            else:
                # å¤šä¸ªè§‚æµ‹åˆ†åˆ«æ¨ç† (ç›®å‰PI0ä¸æ”¯æŒçœŸæ­£çš„æ‰¹é‡æ¨ç†)
                batch_actions = []
                for pi0_obs in batch_obs:
                    raw_action = self.policy.select_action(pi0_obs)
                    action = raw_action[0, :, :7]
                    
                    if isinstance(action, torch.Tensor):
                        action_after_cpu = action.cpu().numpy()
                    else:
                        action_after_cpu = action
                    
                    action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                    batch_actions.append(action_buffer)
                
                return batch_actions
                
        except Exception as e:
            if self.rank == 0:
                print(f"æ‰¹é‡æ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°é€ä¸ªæ¨ç†
            return self._fallback_individual_inference(observations, env_name)
    
    def _fallback_individual_inference(self, observations, env_name, prompts_for_obs=None):
        """å›é€€åˆ°é€ä¸ªæ¨ç†"""
        batch_actions = []
        for idx, obs in enumerate(observations):
            prompt_text = None
            if prompts_for_obs is not None and idx < len(prompts_for_obs):
                prompt_text = prompts_for_obs[idx]
            pi0_obs = self.construct_pi0_observation(obs, prompt_text or env_name)
            raw_action = self.policy.select_action(pi0_obs)
            action = raw_action[0, :, :7]
            
            if isinstance(action, torch.Tensor):
                action_after_cpu = action.cpu().numpy()
            else:
                action_after_cpu = action
            
            action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
            batch_actions.append(action_buffer)
        
        return batch_actions
    
