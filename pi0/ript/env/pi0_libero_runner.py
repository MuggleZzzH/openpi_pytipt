import os
import sys
import torch
import numpy as np
from typing import List, Dict, Any, Tuple, Optional, Union, Callable
from datetime import datetime
import imageio
from pathlib import Path
import json

# å¯¼å…¥LIBEROç›¸å…³çš„å˜æ¢å‡½æ•° â€”â€” è‹¥å¤±è´¥ç›´æ¥æŠ›å¼‚å¸¸ï¼Œä¸å®˜æ–¹è„šæœ¬ä¿æŒä¸€è‡´
try:
    import robosuite.utils.transform_utils as T
except ImportError as e:
    raise ImportError("robosuite is required for LIBEROEnvRunner but not found") from e

# å¯¼å…¥å¹¶è¡Œç¯å¢ƒæ”¯æŒ (ä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨)
try:
    from libero.libero.envs import SubprocVectorEnv
    import multiprocessing
    import gc
    VECTOR_ENV_AVAILABLE = True
except ImportError:
    VECTOR_ENV_AVAILABLE = False

# å¯¼å…¥ç‹¬ç«‹ç¯å¢ƒå·¥å‚ (ç”¨äºçœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œ)
try:
    from .parallel_env_factory import create_env_factory
    TRUE_PARALLEL_AVAILABLE = True
except ImportError:
    TRUE_PARALLEL_AVAILABLE = False

# è°ƒè¯•è®¾ç½®
DEBUG_SAVE_IMAGES = False  # ç¦ç”¨å•ç‹¬å›¾åƒä¿å­˜ï¼Œåªä¿ç•™è§†é¢‘
DEBUG_SAVE_VIDEO = os.environ.get("PI0_DEBUG_SAVE_VIDEO", "false").lower() in ("true", "1", "yes")
DEBUG_IMAGE_DIR = os.environ.get("PI0_DEBUG_IMAGE_DIR", "ript/debug_images")

class LIBEROEnvRunner:
    """LIBEROç¯å¢ƒè¿è¡Œå™¨ï¼Œæä¾›PI0ç­–ç•¥åœ¨LIBEROç¯å¢ƒä¸­çš„è¿è¡Œæœºåˆ¶"""
    
    def __init__(self, policy=None, benchmark_name=None, rollouts_per_env=1, 
                 num_parallel_envs=1, max_episode_length=200, task_names_to_use=None,
                 config=None, rank=0, world_size=1, norm_stats_path=None):
        """åˆå§‹åŒ–LIBEROç¯å¢ƒè¿è¡Œå™¨"""
        # å­˜å‚¨å‚æ•°
        self.policy = policy
        self.benchmark_name = benchmark_name
        self.rollouts_per_env = rollouts_per_env
        self.num_parallel_envs = num_parallel_envs
        self.max_episode_length = max_episode_length
        self.task_names_to_use = task_names_to_use or []
        self.task_names = task_names_to_use or []  # å…¼å®¹æ€§åˆ«åï¼Œä¾›rollout_generatorä½¿ç”¨
        
        # âœ… å­˜å‚¨åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°ï¼ˆéœ€è¦å…ˆèµ‹å€¼rankï¼Œåç»­ä»£ç ä¼šç”¨åˆ°ï¼‰
        self.config = config
        self.rank = rank
        self.world_size = world_size
        
        # ğŸ”¥ ä½¿ç”¨RIPT-VLAå®˜æ–¹çš„ä»»åŠ¡æœ€å¤§æ­¥æ•°è®¾ç½®ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ç»Ÿè®¡ï¼‰
        TASK_MAX_STEPS = {
            'libero_spatial': 220,  # longest training demo has 193 steps
            'libero_object': 280,   # longest training demo has 254 steps
            'libero_goal': 300,     # longest training demo has 270 steps
            'libero_10': 520,       # longest training demo has 505 steps
            'libero_90': 400,       # longest training demo has 373 steps
        }
        
        if max_episode_length is not None:
            self.max_steps = max_episode_length
        elif self.benchmark_name and self.benchmark_name.lower() in TASK_MAX_STEPS:
            self.max_steps = TASK_MAX_STEPS[self.benchmark_name.lower()]
            if self.rank == 0:
                print(f"ğŸ¯ ä½¿ç”¨å®˜æ–¹ä»»åŠ¡é™åˆ¶: {self.benchmark_name} â†’ {self.max_steps}æ­¥")
        else:
            self.max_steps = 300  # å®‰å…¨é»˜è®¤å€¼ï¼ˆlibero_goalçš„é™åˆ¶ï¼‰
            if self.rank == 0:
                print(f"âš ï¸ æœªçŸ¥benchmark {self.benchmark_name}ï¼Œä½¿ç”¨é»˜è®¤é™åˆ¶: {self.max_steps}æ­¥")
        
        # ğŸ”¥ æ–°å¢ï¼šåŠŸèƒ½å¼€å…³æ§åˆ¶ (å®‰å…¨é›†æˆå¤æ‚åŠŸèƒ½)
        # ä»é…ç½®æ–‡ä»¶çš„featureséƒ¨åˆ†è¯»å–å¼€å…³è®¾ç½®
        if config and hasattr(config, 'features'):
            features = config.features
            self.enable_task_polling = getattr(features, 'enable_task_polling', False)
            self.enable_parallel_envs = getattr(features, 'enable_parallel_envs', False)
            self.enable_smart_sampling = getattr(features, 'enable_smart_sampling', False)
            # ğŸ†• æ–°å¢ï¼šçœŸæ­£å¤šè¿›ç¨‹å¹¶è¡Œå¼€å…³ï¼ˆé»˜è®¤å…³é—­ä»¥ä¿æŒå…¼å®¹æ€§ï¼‰
            self.enable_true_parallel_envs = getattr(features, 'enable_true_parallel_envs', False)
        else:
            # å›é€€åˆ°ç›´æ¥ä»configè¯»å–æˆ–ä½¿ç”¨é»˜è®¤å€¼
            self.enable_task_polling = getattr(config, 'enable_task_polling', False) if config else False
            self.enable_parallel_envs = getattr(config, 'enable_parallel_envs', False) if config else False
            self.enable_smart_sampling = getattr(config, 'enable_smart_sampling', False) if config else False
            # ğŸ†• æ–°å¢ï¼šçœŸæ­£å¤šè¿›ç¨‹å¹¶è¡Œå¼€å…³ï¼ˆé»˜è®¤å…³é—­ä»¥ä¿æŒå…¼å®¹æ€§ï¼‰
            self.enable_true_parallel_envs = getattr(config, 'enable_true_parallel_envs', False) if config else False
        
        if self.rank == 0:
            print(f"ğŸ”§ LIBEROEnvRunneråŠŸèƒ½å¼€å…³:")
            print(f"   ä»»åŠ¡è½®è¯¢: {'âœ…' if self.enable_task_polling else 'âŒ'}")
            print(f"   å¹¶è¡Œç¯å¢ƒ: {'âœ…' if self.enable_parallel_envs else 'âŒ'}")
            print(f"   çœŸæ­£å¤šè¿›ç¨‹å¹¶è¡Œ: {'âœ…' if self.enable_true_parallel_envs else 'âŒ'}")
            print(f"   æ™ºèƒ½é‡‡æ ·: {'âœ…' if self.enable_smart_sampling else 'âŒ'}")
        
        # ğŸ”¥ æ–°å¢ï¼šä»»åŠ¡è½®è¯¢æœºåˆ¶ (ä»…åœ¨å¼€å…³å¯ç”¨æ—¶åˆå§‹åŒ–)
        if self.enable_task_polling:
            self.assigned_tasks = task_names_to_use or []  
            self.task_cursor = 0                          
            self.current_task = None                      
            
            if self.assigned_tasks:
                self.current_task = self.assigned_tasks[0]
                if self.rank == 0:
                    print(f"ğŸ¯ ä»»åŠ¡è½®è¯¢åˆå§‹åŒ–: åˆ†é…ä»»åŠ¡ {self.assigned_tasks}, å½“å‰ä»»åŠ¡: {self.current_task}")
            else:
                if self.rank == 0:
                    print(f"âš ï¸ ä»»åŠ¡è½®è¯¢: Rank {self.rank} æ²¡æœ‰åˆ†é…ä»»åŠ¡")
        else:
            # ç®€å•æ¨¡å¼ï¼šä¸ä½¿ç”¨ä»»åŠ¡è½®è¯¢
            self.assigned_tasks = task_names_to_use or []
            self.current_task = task_names_to_use[0] if task_names_to_use else None
        
        # è°ƒè¯•é€‰é¡¹
        self.debug_save_images = DEBUG_SAVE_IMAGES
        # YAMLå¼€å…³ä¼˜å…ˆï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡
        yaml_save_video = False
        try:
            if config and hasattr(config, 'features'):
                yaml_save_video = bool(getattr(config.features, 'save_video', False))
        except Exception:
            yaml_save_video = False
        self.debug_save_video = bool(yaml_save_video) or DEBUG_SAVE_VIDEO
        self.debug_image_dir = DEBUG_IMAGE_DIR
        
        # ğŸ”¥ æ–°å¢ï¼šæ–‡ä»¶è®¡æ•°å™¨æ”¯æŒ
        self.file_counter = None
        
        # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
        self._load_norm_stats(norm_stats_path)

    def _ensure_list_of_dict_obs(self, obs_any, env_num: int):
        """Normalize vector-env observation to List[Dict] per environment.

        Supports formats:
        - dict of batched arrays -> split along first dim
        - numpy array of objects (each a dict) -> list(obs)
        - single dict (non-batched) -> broadcast to env_num
        - fallback: wrap unknown type by broadcasting
        """
        try:
            import numpy as _np
        except Exception:
            _np = None

        # dict of batched arrays
        if isinstance(obs_any, dict):
            result = []
            for i in range(env_num):
                per_env = {}
                for k, v in obs_any.items():
                    try:
                        if hasattr(v, '__len__') and len(v) >= env_num:
                            per_env[k] = v[i]
                        else:
                            per_env[k] = v
                    except Exception:
                        per_env[k] = v
                result.append(per_env)
            return result

        # numpy array of objects (each is a dict)
        if _np is not None and isinstance(obs_any, _np.ndarray):
            if obs_any.dtype == object:
                return [obs_any[i] for i in range(min(len(obs_any), env_num))]
            # unknown numeric array; broadcast
            return [obs_any for _ in range(env_num)]

        # single dict -> broadcast
        if isinstance(obs_any, dict):
            return [obs_any for _ in range(env_num)]

        # list already -> assume per-env
        if isinstance(obs_any, list):
            return obs_any[:env_num] if len(obs_any) >= env_num else (obs_any + [obs_any[-1]] * (env_num - len(obs_any)))

        # fallback: broadcast
        return [obs_any for _ in range(env_num)]

    def _ensure_list_generic(self, value, env_num: int, fill_default, expect_dict: bool = False):
        """Normalize rewards/dones/infos to python list of length env_num.

        - If numpy array: convert to list; if too short, pad by last; if too long, slice.
        - If object array and expect_dict, ensure dict per item; otherwise cast as-is.
        - If single scalar/value, broadcast.
        """
        try:
            import numpy as _np
        except Exception:
            _np = None

        result = None
        if isinstance(value, list):
            result = value
        elif _np is not None and isinstance(value, _np.ndarray):
            # object array (e.g., dicts)
            if value.dtype == object:
                result = list(value)
            else:
                # numeric/bool array
                try:
                    result = value.tolist()
                except Exception:
                    result = [value]
        else:
            # single scalar/value -> broadcast
            result = [value for _ in range(env_num)]

        # adjust length
        if len(result) < env_num:
            pad_val = result[-1] if result else fill_default
            result = (result + [pad_val] * env_num)[:env_num]
        else:
            result = result[:env_num]

        if expect_dict:
            # ensure each item is a dict
            result = [item if isinstance(item, dict) else {} for item in result]

        return result
        
    # ğŸ”¥ æ–°å¢ï¼šä»»åŠ¡è½®è¯¢æ ¸å¿ƒæ–¹æ³• (ä»…åœ¨åŠŸèƒ½å¼€å…³å¯ç”¨æ—¶å¯ç”¨)
    def set_current_task_by_cursor(self):
        """æ ¹æ®cursorè®¾ç½®å½“å‰ä»»åŠ¡"""
        if not self.enable_task_polling:
            return self.current_task  # ç®€å•æ¨¡å¼ï¼šç›´æ¥è¿”å›å½“å‰ä»»åŠ¡
            
        if not self.assigned_tasks:
            self.current_task = None
            return None
        
        # ä½¿ç”¨æ¨¡è¿ç®—å®ç°å¾ªç¯è½®è¯¢
        self.current_task = self.assigned_tasks[self.task_cursor % len(self.assigned_tasks)]
        return self.current_task
    
    def advance_task_cursor(self):
        """æ¨è¿›ä»»åŠ¡cursoråˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡"""
        if not self.enable_task_polling:
            return self.current_task  # ç®€å•æ¨¡å¼ï¼šä¸æ¨è¿›ï¼Œè¿”å›å½“å‰ä»»åŠ¡
            
        if not self.assigned_tasks:
            return None
        
        self.task_cursor += 1
        new_task = self.set_current_task_by_cursor()
        
        # è°ƒè¯•è¾“å‡º
        if self.rank == 0 or len(self.assigned_tasks) > 1:  # ä¸»è¿›ç¨‹æˆ–å¤šä»»åŠ¡æ—¶è¾“å‡º
            print(f"ğŸ”„ Rank {self.rank}: ä»»åŠ¡cursoræ¨è¿›åˆ° {self.task_cursor}, å½“å‰ä»»åŠ¡: {new_task}")
        
        return new_task
    
    def get_current_task(self):
        """è·å–å½“å‰ä»»åŠ¡"""
        return self.current_task
    
    def has_tasks(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é…çš„ä»»åŠ¡"""
        return len(self.assigned_tasks) > 0
    
    def to_hwc_hmirror(self, arr: np.ndarray) -> np.ndarray:
        """
        ç»Ÿä¸€å›¾åƒå¤„ç†å‡½æ•°ï¼Œä¸"2_pi0_on_libero.py"å®Œå…¨å¯¹é½
        å…ˆè§„èŒƒåˆ°HWCæ ¼å¼ï¼Œå†åšæ°´å¹³é•œåƒï¼ˆä¸åšé€šé“äº¤æ¢ï¼‰
        """
        if isinstance(arr, np.ndarray) and arr.ndim == 3:
            # CHW -> HWCï¼ˆå¦‚æœéœ€è¦ï¼‰
            if arr.shape[0] == 3 and arr.shape[-1] != 3:
                arr = arr.transpose(1, 2, 0)
            # æ°´å¹³é•œåƒï¼ˆç¿»è½¬å®½åº¦ç»´ï¼‰
            return arr[:, ::-1, :].copy()
        return arr
    
    # ğŸ”¥ æ–°å¢ï¼šä»»åŠ¡ç»Ÿè®¡å’Œé«˜çº§åŠŸèƒ½ (æ¨¡ä»¿åŸç‰ˆRIPT-VLA)
    def update_task_stats(self, task_name: str, success: bool, reward: float):
        """æ›´æ–°ä»»åŠ¡å®Œæˆç»Ÿè®¡"""
        if not hasattr(self, 'task_completion_stats'):
            self.task_completion_stats = {}
        
        if task_name not in self.task_completion_stats:
            self.task_completion_stats[task_name] = {
                'attempts': 0,
                'successes': 0,
                'total_reward': 0.0,
                'success_rate': 0.0
            }
        
        stats = self.task_completion_stats[task_name]
        stats['attempts'] += 1
        stats['total_reward'] += reward
        if success:
            stats['successes'] += 1
        stats['success_rate'] = stats['successes'] / stats['attempts'] if stats['attempts'] > 0 else 0.0
    
    def get_task_stats(self) -> dict:
        """è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        if not hasattr(self, 'task_completion_stats'):
            return {}
        return self.task_completion_stats.copy()
    
    def get_best_performing_task(self) -> Optional[str]:
        """è·å–è¡¨ç°æœ€å¥½çš„ä»»åŠ¡ï¼ˆç”¨äºæ™ºèƒ½é‡‡æ ·ï¼‰"""
        if not hasattr(self, 'task_completion_stats') or not self.task_completion_stats:
            return None
        
        best_task = None
        best_score = -1.0
        
        for task, stats in self.task_completion_stats.items():
            # ç»¼åˆè€ƒè™‘æˆåŠŸç‡å’Œå¹³å‡å¥–åŠ±
            if stats['attempts'] > 0:
                avg_reward = stats['total_reward'] / stats['attempts']
                score = stats['success_rate'] * 0.7 + min(avg_reward, 1.0) * 0.3
                if score > best_score:
                    best_score = score
                    best_task = task
        
        return best_task
    
    def setup_file_counter(self, counter_name: str = "rollout", work_dir: str = "./") -> Optional[object]:
        """è®¾ç½®æ–‡ä»¶è®¡æ•°å™¨ç”¨äºåˆ†å¸ƒå¼åè°ƒ"""
        try:
            from pi0.ript.algos.rl_optimizers.file_counter import setup_global_counter
            counter = setup_global_counter(counter_name, work_dir=work_dir)
            if counter:
                self.file_counter = counter  # ğŸ”¥ ä¿å­˜åˆ°å®ä¾‹å˜é‡
                if self.rank == 0:
                    print(f"âœ… æ–‡ä»¶è®¡æ•°å™¨è®¾ç½®æˆåŠŸ: {counter_name}")
            return counter
        except ImportError as e:
            if self.rank == 0:
                print(f"âš ï¸ æ–‡ä»¶è®¡æ•°å™¨ä¸å¯ç”¨: {e}")
            return None
        except Exception as e:
            if self.rank == 0:
                print(f"âš ï¸ æ–‡ä»¶è®¡æ•°å™¨åˆ›å»ºå¤±è´¥: {e}")
            return None
        
    def _load_norm_stats(self, norm_stats_path: Optional[str] = None):
        """Load normalization statistics from norm_stats.json"""
        if norm_stats_path is None:
            # å°è¯•åœ¨å¸¸è§ä½ç½®æ‰¾åˆ°norm_stats.json
            possible_paths = [
                "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch/norm_stats.json",
                "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json", 
                "./checkpoints/pi0_libero_pytorch/norm_stats.json",
                "./norm_stats.json"
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    norm_stats_path = path
                    break
        
        if norm_stats_path and Path(norm_stats_path).exists():
            print(f"LIBEROEnvRunner: Loading norm_stats from: {norm_stats_path}")
            with open(norm_stats_path) as f:
                norm_stats = json.load(f)
                
            # æå–çŠ¶æ€å’ŒåŠ¨ä½œçš„å½’ä¸€åŒ–å‚æ•°
            self.state_mean = np.array(norm_stats["norm_stats"]["state"]["mean"][:8], dtype=np.float32)
            self.state_std = np.array(norm_stats["norm_stats"]["state"]["std"][:8], dtype=np.float32)
            self.action_mean = np.array(norm_stats["norm_stats"]["actions"]["mean"][:7], dtype=np.float32)
            self.action_std = np.array(norm_stats["norm_stats"]["actions"]["std"][:7], dtype=np.float32)
            
            print(f"âœ… LIBEROEnvRunner: Loaded normalization stats for action post-processing")
        else:
            raise FileNotFoundError("norm_stats.json not found in expected locations; cannot proceed")

    def get_task_init_states(self, task_id=0):
        """
        è·å–ä»»åŠ¡çš„å¯ç”¨åˆå§‹çŠ¶æ€æ± 
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            list: å¯ç”¨çš„åˆå§‹çŠ¶æ€åˆ—è¡¨ï¼Œæ¯ä¸ªçŠ¶æ€æ˜¯ä¸€ä¸ª8ç»´numpyæ•°ç»„
        """
        # æ–¹æ¡ˆ1: ä»benchmarkè·å–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(self, 'benchmark') and self.benchmark is not None:
            try:
                return self.benchmark.get_task_init_states(task_id)
            except:
                pass
        
        # æ–¹æ¡ˆ2: ç”Ÿæˆå›ºå®šçš„å¯å¤ç°åˆå§‹çŠ¶æ€æ± 
        import numpy as np
        np.random.seed(42 + task_id)  # ç¡®ä¿å¯å¤ç°
        num_states = 100  # ç”Ÿæˆ100ä¸ªå€™é€‰åˆå§‹çŠ¶æ€
        init_states = []
        
        for i in range(num_states):
            # ç”Ÿæˆä¸€ä¸ª8ç»´çš„åˆå§‹çŠ¶æ€ï¼ˆ3 pos + 3 rot + 2 gripperï¼‰
            state = np.random.randn(8).astype(np.float32)
            state[:3] *= 0.1  # ä½ç½®èŒƒå›´é€‚ä¸­
            state[3:6] *= 0.5  # æ—‹è½¬èŒƒå›´é€‚ä¸­
            state[6:8] = np.array([0.0, 0.0])  # å¤¹çˆªåˆå§‹é—­åˆ
            init_states.append(state)
        
        return init_states
    
    def _compute_state_hash(self, state):
        """è®¡ç®—çŠ¶æ€çš„å“ˆå¸Œå€¼"""
        import hashlib
        state_bytes = np.array(state).astype(np.float32).tobytes()
        return hashlib.sha256(state_bytes).hexdigest()
    
    def _extract_state_from_obs(self, obs):
        """ä»è§‚æµ‹ä¸­æå–8ç»´çŠ¶æ€ç”¨äºå“ˆå¸Œè®¡ç®—"""
        try:
            if isinstance(obs, dict):
                # æå–ä½ç½®ã€å§¿æ€å’Œå¤¹çˆªçŠ¶æ€
                pos = np.array(obs.get("robot0_eef_pos", [0, 0, 0]), dtype=np.float32)[:3]
                quat = np.array(obs.get("robot0_eef_quat", [0, 0, 0, 1]), dtype=np.float32)[:4]
                gripper = np.array(obs.get("robot0_gripper_qpos", [0, 0]), dtype=np.float32)[:2]
                
                # è½¬æ¢å››å…ƒæ•°åˆ°è½´è§’
                try:
                    import robosuite.utils.transform_utils as T
                    axis_angle = T.quat2axisangle(quat).astype(np.float32)
                except Exception:
                    axis_angle = np.zeros(3, np.float32)
                
                # ç»„åˆæˆ8ç»´çŠ¶æ€
                state = np.concatenate([pos, axis_angle, gripper])
                return state[:8]  # ç¡®ä¿æ˜¯8ç»´
            else:
                # å¦‚æœobsä¸æ˜¯å­—å…¸ï¼Œè¿”å›é›¶çŠ¶æ€
                return np.zeros(8, np.float32)
        except Exception as e:
            print(f"âš ï¸ æå–çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return np.zeros(8, np.float32)


    
    def construct_pi0_observation(self, obs, task_description):
        """æ„é€ PI0éœ€è¦çš„è§‚æµ‹æ ¼å¼ï¼Œå®Œå…¨æ¨¡ä»¿2_test_pi0_on_libero.pyçš„åšæ³•"""
        # è·å–è®¾å¤‡
        device = self.policy.config.device if hasattr(self.policy, 'config') else 'cuda:0'
        
        # çŠ¶æ€å¤„ç†
        import robosuite.utils.transform_utils as T
        
        axis_angle = T.quat2axisangle(obs["robot0_eef_quat"])
            
        unnorm_state = np.concatenate([
            obs["robot0_eef_pos"],                    # 3D: end-effector position
            axis_angle, # 3D: rotation as axis-angle  
            obs["robot0_gripper_qpos"],               # 2D: gripper joint positions
        ], dtype=np.float32)
        # çŠ¶æ€å½’ä¸€åŒ–
        state = (unnorm_state - self.state_mean) / (self.state_std + 1e-6)
        
        # å›¾åƒå¤„ç† - ä½¿ç”¨ç»Ÿä¸€çš„to_hwc_hmirrorå‡½æ•°ï¼Œä¸"2_pi0_on_libero.py"å®Œå…¨å¯¹é½
        base_0_rgb = self.to_hwc_hmirror(obs["agentview_image"])
        left_wrist_0_rgb = self.to_hwc_hmirror(obs["robot0_eye_in_hand_image"])
        
        # æ„é€ è§‚æµ‹æ ¼å¼
        observation = {
            "image": {
                "base_0_rgb": torch.from_numpy(base_0_rgb).to(device)[None],
                "left_wrist_0_rgb": torch.from_numpy(left_wrist_0_rgb).to(device)[None],
            },
            "state": torch.from_numpy(state).to(device)[None],
            # å§‹ç»ˆä½¿ç”¨è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ä½œä¸ºpromptï¼Œé¿å…ä½¿ç”¨env_nameç­‰çŸ­æ ‡è¯†
            "prompt": [task_description],
        }
        
        return observation
    
    def denormalize_action(self, action: np.ndarray) -> np.ndarray:
        """Denormalize action using loaded statistics"""
        return action * (self.action_std + 1e-6) + self.action_mean
    
    def get_unnormalized_state(self, obs) -> np.ndarray:
        """ä»è§‚æµ‹ä¸­æå–æœªå½’ä¸€åŒ–çš„çŠ¶æ€ï¼ˆç”¨äºåŠ¨ä½œåç§»ï¼‰"""
        try:
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            
            if not isinstance(obs, dict) or not obs:
                raise RuntimeError("Required observation dict missing or empty")
            
            # ä½¿ç”¨ end-effector ä¿¡æ¯
            if "robot0_eef_pos" in obs and "robot0_eef_quat" in obs:
                eef_pos = np.array(obs["robot0_eef_pos"], dtype=np.float32)
                eef_quat = np.array(obs["robot0_eef_quat"], dtype=np.float32)
                
                if eef_pos.size != 3 or eef_quat.size != 4:
                    raise RuntimeError("Observation fields robot0_eef_pos or robot0_eef_quat have incorrect dimensions")
                
                # è½¬æ¢å››å…ƒæ•°ä¸ºè½´è§’
                try:
                    axis_angle = T.quat2axisangle(eef_quat).astype(np.float32)
                except Exception as e:
                    raise RuntimeError(f"Failed to convert quaternion to axis-angle: {e}")
                
                # è·å– gripper çŠ¶æ€ï¼ˆğŸ”¥ ä¿®å¤ï¼šä¿æŒ8ç»´ä¸€è‡´æ€§ï¼‰
                if "robot0_gripper_qpos" not in obs:
                    raise RuntimeError("Observation missing required field robot0_gripper_qpos")
                try:
                    gripper_qpos_raw = obs["robot0_gripper_qpos"]
                    gripper_qpos = np.array(gripper_qpos_raw, dtype=np.float32)
                    if gripper_qpos.size < 2:
                        # å¦‚æœgripperçŠ¶æ€ä¸è¶³2ç»´ï¼Œå¡«å……ä¸º2ç»´
                        full_gripper = np.zeros(2, dtype=np.float32)
                        full_gripper[:gripper_qpos.size] = gripper_qpos.flatten()[:gripper_qpos.size]
                        gripper_qpos = full_gripper
                    else:
                        gripper_qpos = gripper_qpos[:2]  # å–å‰2ç»´ï¼Œä¿æŒå’Œconstruct_pi0_observationä¸€è‡´
                except (IndexError, TypeError, ValueError) as e:
                    raise RuntimeError(f"Invalid robot0_gripper_qpos value: {e}")
                
                # ğŸ”¥ ä¿®å¤ï¼šè¿”å›8ç»´çŠ¶æ€ï¼Œä¸construct_pi0_observationå®Œå…¨ä¸€è‡´
                unnorm_state = np.concatenate([eef_pos[:3], axis_angle[:3], gripper_qpos]).astype(np.float32)
                return unnorm_state  # 8ç»´ï¼š3+3+2
                
            # å®˜æ–¹è„šæœ¬è¦æ±‚ä¸Šè¿°å…³é”®å­—æ®µå‡å­˜åœ¨ï¼Œè‹¥ç¼ºå¤±åˆ™ç«‹å³æŠ¥é”™
            else:
                raise RuntimeError("Observation missing required end-effector fields")
                
        except Exception as e:
            raise
    
    def make_env(self, env_name: str):
        """åˆ›å»ºLIBEROç¯å¢ƒ"""
        try:
            import gym
            from cleandiffuser.env import libero  # ç¡®ä¿ç¯å¢ƒæ³¨å†Œ
            
            # ä½¿ç”¨ä¸å‚è€ƒå®ç°2_test_pi0_on_libero.pyå®Œå…¨ç›¸åŒçš„ç¯å¢ƒé…ç½®
            benchmark_to_env_id = {
                'libero_spatial': 'libero-spatial-v0',
                'libero_object': 'libero-object-v0',
                'libero_goal': 'libero-goal-v0',
                'libero_10': 'libero-10-v0',
                'libero_90': 'libero-90-v0'
            }

            if self.benchmark_name not in benchmark_to_env_id:
                raise ValueError(f"Unknown benchmark_name: {self.benchmark_name}")

            env_id = benchmark_to_env_id[self.benchmark_name]
            # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®ä»»åŠ¡åç§°åŠ¨æ€ç¡®å®štask_idï¼Œä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´
            # å¯¹äºlibero_goalï¼Œä½¿ç”¨task_id=1 (ä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´)
            if self.benchmark_name == "libero_goal":
                task_id = 1  # ä¸2_test_pi0_on_libero.pyä¿æŒä¸€è‡´
            elif self.benchmark_name == "libero_spatial":
                task_id = 0  # ç¬¬ä¸€ä¸ªspatialä»»åŠ¡
            else:
                task_id = 0  # å…¶ä»–benchmarkçš„é»˜è®¤ä»»åŠ¡
            
            # åˆ›å»ºç¯å¢ƒ
            env = gym.make(
                env_id,
                task_id=task_id,
                image_size=224,  # åŒ¹é…PI0çš„è¾“å…¥å°ºå¯¸
                camera_names=["agentview", "robot0_eye_in_hand"],  # åŒ¹é…åŸå§‹demo
                seed=0,  # ä½¿ç”¨ä¸å‚è€ƒå®ç°ç›¸åŒçš„éšæœºç§å­
            )
            
            # è·å–ä»»åŠ¡æè¿°
            if hasattr(env, 'task_description'):
                task_description = env.task_description
            else:
                task_description = env_name
                
            return env, task_description
            
        except Exception as e:
            raise RuntimeError(f"åˆ›å»ºç¯å¢ƒå¤±è´¥: {e}") from e
        
    def run_policy_in_env(self, env_name, all_init_states=None, debug_save_video=None, created_env=None):
        """åœ¨ç¯å¢ƒä¸­è¿è¡Œç­–ç•¥ï¼Œç”Ÿæˆè½¨è¿¹ - æ”¯æŒå¹¶è¡Œå’Œä¸²è¡Œç¯å¢ƒ"""
        save_video = debug_save_video if debug_save_video is not None else self.debug_save_video
        
        # ğŸ”¥ æ ¹æ®åŠŸèƒ½å¼€å…³é€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œæ¨¡å¼
        if self.enable_parallel_envs and created_env is None:
            if self.rank == 0:
                print(f"ğŸš€ ä½¿ç”¨å¹¶è¡Œç¯å¢ƒæ¨¡å¼ (num_parallel_envs={self.num_parallel_envs})")
            # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
            env, env_id, env_num = self.create_parallel_envs(env_name, all_init_states)
            created_env = (env, env_id, env_num)
        elif created_env is None:
            if self.rank == 0:
                print(f"ğŸ”„ ä½¿ç”¨ä¸²è¡Œç¯å¢ƒæ¨¡å¼")
            # ä½¿ç”¨ä¸²è¡Œç¯å¢ƒ
            env, task_description = self.make_env(env_name)
            created_env = (env, env_name, 1)
        
        env, env_id, env_num = created_env
        
        if all_init_states is None:
            # å¦‚æœæ²¡æœ‰æä¾›åˆå§‹çŠ¶æ€ï¼Œç”Ÿæˆé»˜è®¤çŠ¶æ€
            all_init_states = np.zeros((self.rollouts_per_env, 8), dtype=np.float32)
        
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„å¹¶è¡Œç¯å¢ƒ
            is_vector_env = hasattr(env, 'num_envs') or 'VectorEnv' in str(type(env))
            
            if self.enable_parallel_envs and env_num > 1 and is_vector_env:
                # ä½¿ç”¨çœŸæ­£çš„å¹¶è¡Œç¯å¢ƒ
                if self.rank == 0:
                    print(f"ğŸš€ å¹¶è¡Œç¯å¢ƒæ¨¡å¼: {env_num} ä¸ªç¯å¢ƒ")
                yield from self._run_parallel_episodes(env, env_name, all_init_states, env_num, save_video)
            else:
                # ä½¿ç”¨å•ç¯å¢ƒæ¨¡å¼
                if self.rank == 0:
                    print(f"ğŸ”„ å•ç¯å¢ƒæ¨¡å¼")
                yield from self._run_serial_episodes(env, env_name, all_init_states, save_video)
        
        finally:
            # æ¸…ç†ç¯å¢ƒ
            try:
                if hasattr(env, 'close'):
                    env.close()
                if VECTOR_ENV_AVAILABLE:
                    import gc
                    gc.collect()
            except Exception as e:
                if self.rank == 0:
                    print(f"ç¯å¢ƒæ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def _run_serial_episodes(self, env, env_name, all_init_states, save_video):
        """è¿è¡Œä¸²è¡Œ episodesï¼ˆå¤„ç†å•ä¸ªç¯å¢ƒå’Œå•ä¸ªVectorEnvï¼‰"""
        if self.rank == 0:
            print(f"ä½¿ç”¨ä¸²è¡Œæ¨¡å¼æ‰§è¡Œ {len(all_init_states)} ä¸ªepisodes")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºVectorEnvç±»å‹
        is_vector_env = hasattr(env, 'num_envs') or 'VectorEnv' in str(type(env))
        if is_vector_env:
            if self.rank == 0:
                print(f"æ£€æµ‹åˆ°VectorEnvï¼Œä½¿ç”¨å•ä¸ªç¯å¢ƒæ¨¡å¼")
        
        episodes = []
        for i, target_init_state in enumerate(all_init_states):
            rollout_images = []
            
            # é‡ç”¨ä¼ å…¥çš„ç¯å¢ƒæˆ–åˆ›å»ºæ–°ç¯å¢ƒ
            if hasattr(env, 'task_description'):
                task_description = env.task_description
            else:
                task_description = env_name
            
            # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šå°è¯•è®¾ç½®åˆå§‹çŠ¶æ€
            if target_init_state is not None:
                try:
                    # ğŸ”¥ ç®€åŒ–ï¼šä½¿ç”¨åŸç‰ˆRIPTçš„è°ƒç”¨æ–¹å¼
                    # ç¡®ä¿çŠ¶æ€æ˜¯è¿ç»­çš„numpyæ•°ç»„
                    if isinstance(target_init_state, np.ndarray):
                        target_init_state = np.ascontiguousarray(target_init_state.astype(np.float64))
                    
                    # å…ˆresetï¼Œå†set_init_stateï¼ˆä¸åŸç‰ˆRIPTå®Œå…¨ä¸€è‡´ï¼‰
                    obs = env.reset()
                    obs = env.set_init_state(target_init_state)
                    print(f"âœ… ä¸²è¡Œæ¨¡å¼ï¼šçŠ¶æ€è®¾ç½®æˆåŠŸ {i}ï¼ŒçŠ¶æ€ç»´åº¦: {target_init_state.shape if hasattr(target_init_state, 'shape') else 'N/A'}")
                except Exception as e:
                    print(f"âš ï¸ è®¾ç½®åˆå§‹çŠ¶æ€å¤±è´¥: {e}ï¼Œå›é€€åˆ°éšæœºreset")
                    obs = env.reset()
                    if is_vector_env and isinstance(obs, list):
                        actual_obs = obs[0]
                    else:
                        actual_obs = obs
                    actual_init_state = self._extract_state_from_obs(actual_obs)
                    target_init_state = actual_init_state
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆå§‹çŠ¶æ€ï¼Œä½¿ç”¨é»˜è®¤reset
                obs = env.reset()
                if is_vector_env and isinstance(obs, list):
                    actual_obs = obs[0]
                else:
                    actual_obs = obs
                target_init_state = self._extract_state_from_obs(actual_obs)
            
            # å¦‚æœæ˜¯VectorEnvï¼Œå–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            if is_vector_env and isinstance(obs, list):
                obs = obs[0]
            
            # è®¡ç®—åˆå§‹çŠ¶æ€å“ˆå¸Œ
            init_hash = self._compute_state_hash(target_init_state)
            
            # çƒ­æœºæ­¥éª¤
            dummy_action = np.array([0, 0, 0, 0, 0, 0, -1])
            for _ in range(10):
                if is_vector_env:
                    # VectorEnvæœŸæœ›åŠ¨ä½œåˆ—è¡¨
                    step_results = env.step([dummy_action])
                    obs, _, _, _ = step_results
                    if isinstance(obs, list):
                        obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„ç»“æœ
                else:
                    obs, _, _, _ = env.step(dummy_action)
            
            step = 0
            done = False
            total_reward = 0
            success = False
            
            # æ”¶é›†åˆå§‹è§‚æµ‹å›¾åƒç”¨äºè§†é¢‘
            if save_video:
                initial_img = self.to_hwc_hmirror(obs["agentview_image"])
                rollout_images.append(initial_img)
            
            # æ”¶é›†è½¨è¿¹
            observations = [obs]
            actions = []
            rewards = []
            dones = []
            infos = []
            
            try:
                action_buffer = None
                action_index = 0
                
                # æ‰§è¡Œç­–ç•¥ç›´åˆ°å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°
                while not done and step < self.max_steps:
                    # åªåœ¨éœ€è¦æ—¶æ¨ç†ï¼ˆåŠ¨ä½œé˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼‰
                    if action_buffer is None or action_index >= action_buffer.shape[0]:
                        # ä½¿ç”¨æ­£ç¡®çš„PI0è§‚æµ‹æ ¼å¼
                        pi0_observation = self.construct_pi0_observation(obs, task_description)
                        
                        # é€‰æ‹©åŠ¨ä½œ - å¼ºåˆ¶è¦æ±‚YAMLä¸­é…ç½®CFGå‚æ•°
                        cfg_scale = None
                        
                        # ğŸ”¥ è¯»å–YAMLé…ç½®ï¼ˆæ”¯æŒå¯¹è±¡æ¨¡å¼å’Œdictæ¨¡å¼ï¼‰
                        if self.config:
                            # å¯¹è±¡æ¨¡å¼
                            if hasattr(self.config, 'algo') and hasattr(self.config.algo, 'collection_cfg_scale'):
                                cfg_scale = self.config.algo.collection_cfg_scale
                            # dictæ¨¡å¼
                            elif isinstance(self.config, dict):
                                cfg_scale = self.config.get('algo', {}).get('collection_cfg_scale', None)
                        
                        # ğŸ”¥ å¦‚æœYAMLæ²¡å†™ï¼Œç›´æ¥æŠ›é”™ï¼Œä¸å†å›é€€é»˜è®¤å€¼
                        if cfg_scale is None:
                            raise ValueError(
                                "âŒ æœªåœ¨YAMLçš„algo.collection_cfg_scaleä¸­æ‰¾åˆ°CFG scaleé…ç½®ï¼\n"
                                "è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ ï¼š\n"
                                "algo:\n"
                                "  collection_cfg_scale: 1.1  # æˆ–å…¶ä»–å€¼"
                            )
                        
                        # ğŸ”§ ï¼ˆå¯é€‰ï¼‰è­¦å‘Špolicyå†…ç½®å€¼è¢«å¿½ç•¥
                        if hasattr(self, 'policy') and hasattr(self.policy, 'default_cfg_scale'):
                            policy_cfg = getattr(self.policy, 'default_cfg_scale', None)
                            if policy_cfg is not None:
                                print(f"âš ï¸ policy.default_cfg_scale={policy_cfg} è¢«å¿½ç•¥ï¼Œä½¿ç”¨YAMLä¸­çš„ {cfg_scale}")
                        
                        # ç¡®è®¤ä½¿ç”¨çš„CFG scale
                        print(f"âœ… ä½¿ç”¨CFG Scale: {cfg_scale}")
                        
                        raw_action = self.policy.select_action(pi0_observation, cfg_scale=cfg_scale)
                        action = raw_action[0, :, :7]  # shape: (50, 7)
                        
                        # è½¬æ¢ä¸ºnumpyå¹¶å¤„ç†ç»´åº¦
                        if isinstance(action, torch.Tensor):
                            action_after_cpu = action.cpu().numpy()
                        else:
                            action_after_cpu = action
                        
                        # åå½’ä¸€åŒ–åŠ¨ä½œ
                        action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šçª—å£ä¸€è‡´çš„æ®‹å·®åŸºå‡†
                        # è·å–å½“å‰æœªå½’ä¸€åŒ–çŠ¶æ€ç”¨äºåç§»ï¼ˆæ•´ä¸ª50æ­¥åºåˆ—ç”¨åŒä¸€ä¸ªåŸºå‡†ï¼‰
                        import robosuite.utils.transform_utils as T
                        unnorm_state = np.concatenate([
                            obs["robot0_eef_pos"],
                            T.quat2axisangle(obs["robot0_eef_quat"]),
                            obs["robot0_gripper_qpos"],
                        ], dtype=np.float32)
                        
                        # å¯¹æ•´ä¸ªåŠ¨ä½œåºåˆ—åº”ç”¨åŒä¸€ä¸ªçŠ¶æ€åç§»ï¼ˆå‰6ç»´ï¼šä½ç½®+æ—‹è½¬ï¼‰
                        action_buffer[:, :6] += unnorm_state[None, :6]
                        
                        # é‡ç½®åŠ¨ä½œç´¢å¼•ï¼ˆæ³¨æ„ï¼šåœ¨æ•´ä¸ªåºåˆ—æ‰§è¡ŒæœŸé—´ï¼Œä¸å†æ›´æ–°åŸºå‡†ï¼‰
                        action_index = 0
                    
                    # ä»åŠ¨ä½œé˜Ÿåˆ—ä¸­å–å‡ºå½“å‰åŠ¨ä½œæ‰§è¡Œ
                    current_action = action_buffer[action_index, :7]
                    
                    # ğŸ”§ åŠ¨ä½œè£å‰ªï¼šç¡®ä¿åŠ¨ä½œåœ¨åˆæ³•èŒƒå›´å†…
                    if hasattr(env, 'action_space'):
                        action_space = env.action_space
                        if is_vector_env and hasattr(action_space, 'spaces') and len(action_space.spaces) > 0:
                            # VectorEnvä½¿ç”¨ç¬¬ä¸€ä¸ªç¯å¢ƒçš„åŠ¨ä½œç©ºé—´
                            sub_space = action_space.spaces[0]
                            current_action = np.clip(current_action, sub_space.low, sub_space.high)
                        elif hasattr(action_space, 'low') and hasattr(action_space, 'high'):
                            # å•ç¯å¢ƒåŠ¨ä½œç©ºé—´
                            current_action = np.clip(current_action, action_space.low, action_space.high)
                        else:
                            # ä½¿ç”¨é»˜è®¤èŒƒå›´[-1, 1]
                            current_action = np.clip(current_action, -1, 1)
                    else:
                        # æ²¡æœ‰action_spaceï¼Œä½¿ç”¨é»˜è®¤èŒƒå›´[-1, 1]
                        current_action = np.clip(current_action, -1, 1)
                    
                    # æ‰§è¡Œå•æ­¥åŠ¨ä½œ
                    if is_vector_env:
                        # VectorEnvæœŸæœ›åŠ¨ä½œåˆ—è¡¨
                        step_results = env.step([current_action])
                        next_obs, reward, done, info = step_results
                        # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„ç»“æœ
                        if isinstance(next_obs, list):
                            next_obs = next_obs[0]
                        if isinstance(reward, list):
                            reward = reward[0]
                        if isinstance(done, list):
                            done = done[0]
                        if isinstance(info, list):
                            info = info[0]
                    else:
                        next_obs, reward, done, info = env.step(current_action)
                    
                    # è®°å½•è½¨è¿¹æ•°æ®
                    observations.append(next_obs)
                    actions.append(current_action)
                    rewards.append(reward)
                    dones.append(done)
                    infos.append(info)
                    
                    # æ›´æ–°ç´¯è®¡å¥–åŠ±
                    total_reward += reward
                    
                    # æ”¶é›†å›¾åƒç”¨äºè§†é¢‘
                    if save_video:
                        frame_img = self.to_hwc_hmirror(next_obs["agentview_image"])
                        rollout_images.append(frame_img)
                    
                    # æ›´æ–°çŠ¶æ€å’Œè®¡æ•°å™¨
                    obs = next_obs
                    step += 1
                    action_index += 1
                    
                    # æ£€æŸ¥æˆåŠŸçŠ¶æ€ - ä¼˜å…ˆä½¿ç”¨infoä¸­çš„successï¼Œå¦åˆ™åŸºäºå¥–åŠ±åˆ¤æ–­ï¼ˆä¸4è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
                    if info.get("success", False):
                        success = True
                    # å¦‚æœinfoä¸­æ²¡æœ‰successå­—æ®µï¼Œä½¿ç”¨ä¸4_simple_train_ript.pyç›¸åŒçš„åˆ¤æ–­é€»è¾‘
                    elif total_reward > 0.5:
                        success = True
                    
                    # å¦‚æœä»»åŠ¡å®Œæˆï¼Œæå‰é€€å‡º
                    if done:
                        break
            
            except Exception as e:
                print(f"æ‰§è¡Œç¯å¢ƒæ­¥éª¤æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                # æ³¨æ„ï¼šç¯å¢ƒå…³é—­ç”±run_policy_in_envçš„finallyå—ç»Ÿä¸€å¤„ç†ï¼Œé¿å…é‡å¤å…³é—­
                pass
            
            # ä¿å­˜æ•´ä¸ªè½¨è¿¹çš„è§†é¢‘
            if save_video and rollout_images:
                try:
                    # åˆ›å»ºè§†é¢‘ç›®å½•
                    video_dir = Path("pi0/ript/debug_images/videos")
                    video_dir.mkdir(parents=True, exist_ok=True)
                    
                    # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    task_str = task_description.replace(" ", "_")[:30] if isinstance(task_description, str) else f"episode_{i}"
                    success_str = "success" if success else "failure"
                    video_path = video_dir / f"{timestamp}_{task_str}_{success_str}.mp4"
                    
                    # ä¿å­˜è§†é¢‘
                    writer = imageio.get_writer(str(video_path), fps=30)
                    for frame in rollout_images:
                        writer.append_data(frame)
                    writer.close()
                    
                    print(f"âœ… å·²ä¿å­˜è§†é¢‘: {video_path}")
                except Exception as e:
                    print(f"ä¿å­˜è§†é¢‘å‡ºé”™: {e}")
            
            # æ”¶é›†episodeæ•°æ®
            episode_data = {
                "observations": observations,
                "actions": actions,
                "rewards": rewards,
                "dones": dones,
                "infos": infos,
                "task": task_description,
                "init_state": target_init_state,  # ğŸ”¥ æ·»åŠ åˆå§‹çŠ¶æ€
                "init_hash": init_hash,           # ğŸ”¥ æ·»åŠ åˆå§‹çŠ¶æ€å“ˆå¸Œ
                "temp_init_hash": init_hash,      # ğŸ”¥ æ·»åŠ ä¸´æ—¶å“ˆå¸Œç”¨äºåç»­è¿ç§»
            }
            
            # è¿”å›è½¨è¿¹ç»“æœ
            episodes.append(episode_data)
        
        # è¿”å›æ‰€æœ‰episodes
        for i, episode_data in enumerate(episodes):
            # è®¡ç®—æœ€ç»ˆæˆåŠŸç‡å’Œå¥–åŠ±
            success = episode_data.get('rewards', [0])[-1] if episode_data.get('rewards') else False
            success = bool(success > 0) if isinstance(success, (int, float)) else bool(success)
            total_reward = sum(episode_data.get('rewards', []))
            yield (success, total_reward, episode_data)
    
    # ğŸ”¥ æ–°å¢ï¼šå¹¶è¡Œç¯å¢ƒæ”¯æŒ (ä»…åœ¨åŠŸèƒ½å¼€å…³å¯ç”¨æ—¶å¯ç”¨)
    def create_env(self, env_name: str):
        """åˆ›å»ºå•ä¸ªç¯å¢ƒï¼ˆç”¨äºå¹¶è¡Œç¯å¢ƒï¼‰"""
        if not self.enable_parallel_envs:
            return self.make_env(env_name)  # ç®€å•æ¨¡å¼ï¼šç›´æ¥è¿”å›å•ä¸ªç¯å¢ƒ
            
        env, task_description = self.make_env(env_name)
        return env, env_name, 1
    
    def create_parallel_envs(self, env_name: str, all_init_states=None):
        """åˆ›å»ºå¹¶è¡Œç¯å¢ƒ"""
        if not self.enable_parallel_envs or not VECTOR_ENV_AVAILABLE or self.num_parallel_envs <= 1:
            # å¦‚æœä¸æ”¯æŒå¹¶è¡Œç¯å¢ƒæˆ–åªéœ€è¦1ä¸ªç¯å¢ƒï¼Œä½¿ç”¨å•ä¸ªç¯å¢ƒ
            if self.rank == 0:
                print(f"ä½¿ç”¨å•ä¸ªç¯å¢ƒ (num_parallel_envs={self.num_parallel_envs})")
            env, task_description = self.make_env(env_name)
            return env, env_name, 1
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨çœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œ
        if self.enable_true_parallel_envs:
            return self._create_true_parallel_envs(env_name, all_init_states)
        else:
            # å›é€€åˆ°å•ç¯å¢ƒ
            return self._create_single_env(env_name)
    
    def _create_single_env(self, env_name: str):
        """åˆ›å»ºå•ä¸ªç¯å¢ƒ - ç®€å•ç›´æ¥"""
        if self.rank == 0:
            print(f"ğŸ”„ ä½¿ç”¨å•ç¯å¢ƒæ¨¡å¼")
        
        env, task_description = self.make_env(env_name) 
        return env, env_name, 1
    
    def _create_true_parallel_envs(self, env_name: str, all_init_states=None):
        """ğŸ†• åˆ›å»ºçœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œç¯å¢ƒ - ä½¿ç”¨ç‹¬ç«‹ç¯å¢ƒå·¥å‚è§£å†³åºåˆ—åŒ–é—®é¢˜"""
        if not TRUE_PARALLEL_AVAILABLE:
            if self.rank == 0:
                print(f"âš ï¸ ç‹¬ç«‹ç¯å¢ƒå·¥å‚ä¸å¯ç”¨ï¼Œå›é€€åˆ°å•ç¯å¢ƒæ¨¡å¼")
            return self._create_single_env(env_name)
        
        # è®¡ç®—GPUå†…å­˜éœ€æ±‚
        model_size_gb = 3.5  # PI0æ¨¡å‹å¤§å°
        required_memory_gb = self.num_parallel_envs * model_size_gb
        
        # æ£€æŸ¥GPUå†…å­˜
        available_memory_gb = 0
        current_usage = 0
        if torch.cuda.is_available():
            available_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            current_usage = torch.cuda.memory_allocated(0) / (1024**3)
            available_memory = available_memory_gb - current_usage
        
        if self.rank == 0:
            print(f"ğŸš€ å°è¯•åˆ›å»ºçœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œç¯å¢ƒ:")
            print(f"ğŸ§  å†…å­˜åˆ†æ:")
            print(f"   {self.num_parallel_envs}ä¸ªå¹¶è¡Œç¯å¢ƒç†è®ºéœ€è¦: {required_memory_gb:.1f}GB")
            print(f"   å½“å‰GPUæ€»å†…å­˜: {available_memory_gb:.1f}GB") 
            print(f"   å½“å‰GPUå·²ä½¿ç”¨: {current_usage:.1f}GB")
            print(f"   å¯ç”¨å†…å­˜: {available_memory:.1f}GB")
        
        # å†…å­˜å®‰å…¨æ£€æŸ¥
        if available_memory < required_memory_gb * 1.2:
            if self.rank == 0:
                print(f"âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°å•ç¯å¢ƒæ¨¡å¼")
            return self._create_single_env(env_name)
        
        try:
            # ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ç‹¬ç«‹ç¯å¢ƒå·¥å‚ï¼Œé¿å…åºåˆ—åŒ–selfå¯¹è±¡
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å›ºå®šåˆå§‹çŠ¶æ€IDç¡®ä¿å¹¶è¡Œç¯å¢ƒåŒæ­¥

            # ä»é…ç½®ä¸­è¯»å–åŒæ­¥è®¾ç½®
            sync_config = getattr(self.config, 'features', {}).get('parallel_env_sync', {})
            sync_enabled = sync_config.get('enabled', True)
            fixed_init_state_id = sync_config.get('fixed_init_state_id', 0) if sync_enabled else None

            env_factory = create_env_factory(
                benchmark_name=self.benchmark_name,
                env_name=env_name,
                task_id=None,  # è‡ªåŠ¨æ¨æ–­
                fixed_init_state_id=fixed_init_state_id,  # ğŸ”¥ æ–°å¢ï¼šå›ºå®šåˆå§‹çŠ¶æ€ID
                init_states_array=all_init_states  # ğŸ”¥ ä¼ é€’åˆå§‹çŠ¶æ€æ•°ç»„
            )

            # åˆ›å»ºå¤šä¸ªç¯å¢ƒå·¥å‚å®ä¾‹
            env_factories = [env_factory for _ in range(self.num_parallel_envs)]
            
            if self.rank == 0:
                print(f"ğŸ”§ åˆ›å»º {self.num_parallel_envs} ä¸ªç‹¬ç«‹å¹¶è¡Œç¯å¢ƒ...")
                if sync_enabled and fixed_init_state_id is not None:
                    if fixed_init_state_id == -1:
                        print("ğŸ² å¯ç”¨æ™ºèƒ½éšæœºæ¨¡å¼ï¼Œæ¯æ¬¡é‡ç½®éšæœºé€‰æ‹©åˆå§‹çŠ¶æ€")
                    else:
                        print(f"ğŸ”’ å¯ç”¨åŒæ­¥æ¨¡å¼ï¼Œå›ºå®šåˆå§‹çŠ¶æ€ID: {fixed_init_state_id}")
                else:
                    print("ğŸ² ä½¿ç”¨å®Œå…¨éšæœºåˆå§‹çŠ¶æ€æ¨¡å¼")

            # è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•
            if multiprocessing.get_start_method(allow_none=True) != 'spawn':
                multiprocessing.set_start_method('spawn', force=True)

            # åˆ›å»ºSubprocVectorEnv
            parallel_env = SubprocVectorEnv(env_factories)
            
            # ğŸ” éªŒè¯å¹¶è¡Œç¯å¢ƒåˆå§‹çŠ¶æ€åŒæ­¥æ€§
            try:
                test_obs = parallel_env.reset()
                if self.rank == 0:
                    if isinstance(test_obs, list):
                        print(f"âœ… SubprocVectorEnvå·²åˆ›å»ºï¼Œresetè¿”å›listï¼Œé•¿åº¦: {len(test_obs)}")

                        # ğŸ”¥ æ–°å¢ï¼šéªŒè¯åˆå§‹çŠ¶æ€åŒæ­¥æ€§
                        verify_sync = sync_config.get('verify_sync', True)
                        if verify_sync:
                            sync_verified = self._verify_parallel_env_sync(test_obs)
                            if sync_verified:
                                print("âœ… å¹¶è¡Œç¯å¢ƒåˆå§‹çŠ¶æ€åŒæ­¥éªŒè¯é€šè¿‡")
                            else:
                                print("âš ï¸ å¹¶è¡Œç¯å¢ƒåˆå§‹çŠ¶æ€å¯èƒ½ä¸åŒæ­¥")
                                if sync_enabled:
                                    print("   å»ºè®®æ£€æŸ¥SyncedInitStateWrapperæ˜¯å¦æ­£å¸¸å·¥ä½œ")
                        else:
                            print("â„¹ï¸ è·³è¿‡åŒæ­¥éªŒè¯ï¼ˆverify_sync=falseï¼‰")
                    else:
                        print(f"âœ… SubprocVectorEnvå·²åˆ›å»ºï¼Œresetè¿”å›ç±»å‹: {type(test_obs)}")
            except Exception as e:
                if self.rank == 0:
                    print(f"âš ï¸ SubprocVectorEnv.reset() è°ƒç”¨å¼‚å¸¸: {e}")
            
            if self.rank == 0:
                print(f"   ğŸ”„ {self.num_parallel_envs} ä¸ªç‹¬ç«‹å­è¿›ç¨‹")
                print(f"   ğŸ§  æ¯ä¸ªå­è¿›ç¨‹æ— æ¨¡å‹ï¼Œæ€»å†…å­˜èŠ‚çœ ~{(self.num_parallel_envs-1)*3.5:.1f}GB")
                print(f"   âš¡ çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œï¼Œæ€§èƒ½æå‡ ~{self.num_parallel_envs}x")
            
            # ğŸ§  è·å–æ¯ä¸ªå­è¿›ç¨‹çš„ä»»åŠ¡æè¿°ï¼ˆè‡ªç„¶è¯­è¨€promptï¼‰ï¼Œç”¨äºæ„é€ æ¨¡å‹è¾“å…¥
            try:
                vector_prompts = []
                for w in getattr(parallel_env, 'workers', []):
                    desc = None
                    try:
                        desc = w.get_env_attr('task_description')
                    except Exception:
                        desc = None
                    vector_prompts.append(desc if isinstance(desc, str) and len(desc) > 0 else env_name)
                self._vector_env_prompts = vector_prompts
            except Exception:
                self._vector_env_prompts = [env_name for _ in range(self.num_parallel_envs)]

            return parallel_env, env_name, self.num_parallel_envs
            
        except Exception as e:
            if self.rank == 0:
                print(f"âŒ çœŸæ­£å¹¶è¡Œç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
                print(f"ğŸ”„ å›é€€åˆ°å•ç¯å¢ƒæ¨¡å¼")
                import traceback
                traceback.print_exc()
            
            # æ¸…ç†å¯èƒ½çš„èµ„æº
            try:
                if 'parallel_env' in locals():
                    parallel_env.close()
            except:
                pass
            
            # å›é€€åˆ°å•ç¯å¢ƒ
            return self._create_single_env(env_name)
    
    # âŒ è½»é‡çº§å¹¶è¡Œç¯å¢ƒæ–¹æ¡ˆå·²è¢«ç§»é™¤
    # åŸå› ï¼šCloudPickleåºåˆ—åŒ–æ—¶ä¼šåŒ…å«åŒ…å«Policyçš„selfå¯¹è±¡å¼•ç”¨
    # æ— è®ºå¦‚ä½•æ„é€ ç¯å¢ƒå‡½æ•°ï¼Œéƒ½ä¼šå¯¼è‡´æ¨¡å‹åœ¨æ¯ä¸ªå­è¿›ç¨‹ä¸­é‡å¤åŠ è½½
    # è¿™æ˜¯SubprocVectorEnv + PyTorchæ¨¡å‹çš„å›ºæœ‰é™åˆ¶
    
    def _deprecated_lightweight_parallel_note(self):
        """è®°å½•è½»é‡çº§å¹¶è¡Œæ–¹æ¡ˆå¤±è´¥çš„åŸå› """
        return """
        è½»é‡çº§å¹¶è¡Œæ–¹æ¡ˆå¤±è´¥åŸå› åˆ†æï¼š
        
        1. CloudPickleåºåˆ—åŒ–é—®é¢˜ï¼š
           - SubprocVectorEnvä½¿ç”¨CloudPickleåºåˆ—åŒ–ç¯å¢ƒæ„é€ å‡½æ•°
           - å³ä½¿create_env_without_modelä¸ç›´æ¥å¼•ç”¨self.policy
           - ä½†å‡½æ•°é—­åŒ…ä¸­åŒ…å«selfå¯¹è±¡çš„å¼•ç”¨
           - CloudPickleä¼šé€’å½’åºåˆ—åŒ–selfåŠå…¶åŒ…å«çš„policyå¯¹è±¡
        
        2. GPUå†…å­˜é—®é¢˜ï¼š
           - æ¯ä¸ªå­è¿›ç¨‹ååºåˆ—åŒ–æ—¶éƒ½ä¼šé‡å»ºå®Œæ•´çš„PI0æ¨¡å‹
           - 3ä¸ªè¿›ç¨‹ Ã— 3.5GB = 10.5GB GPUå†…å­˜éœ€æ±‚
           - è¶…è¿‡å•å¡å†…å­˜é™åˆ¶å¯¼è‡´CUDA OOM
        
        3. è§£å†³æ–¹æ¡ˆï¼š
           - ä½¿ç”¨æ‰¹å¤„ç†æ¨¡æ‹Ÿå¹¶è¡Œç­–ç•¥
           - å•ç¯å¢ƒ + æ™ºèƒ½æ‰¹é‡å¤„ç†
           - å†…å­˜å®‰å…¨ä¸”æ€§èƒ½æ¥è¿‘çœŸå®å¹¶è¡Œ
        """
    
    def _run_parallel_episodes(self, env, env_name, all_init_states, env_num, save_video):
        """è¿è¡ŒçœŸæ­£çš„å¹¶è¡Œ episodes"""
        if self.rank == 0:
            print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {env_num} ä¸ªç¯å¢ƒ")
        
        # è®¡ç®—éœ€è¦çš„è½®æ¬¡
        eval_loop_num = (self.rollouts_per_env + env_num - 1) // env_num
        count = 0
        
        while count < eval_loop_num:
            # é€‰æ‹©å½“å‰è½®æ¬¡çš„åˆå§‹çŠ¶æ€
            start_idx = count * env_num
            end_idx = min(start_idx + env_num, len(all_init_states))
            indices = np.arange(start_idx, end_idx) % len(all_init_states)
            
            # ğŸ”¥ ä¿®å¤ï¼šå…¼å®¹åˆ—è¡¨å’Œnumpyæ•°ç»„ç´¢å¼•
            if isinstance(all_init_states, list):
                current_init_states = [all_init_states[i] for i in indices]
            else:
                current_init_states = all_init_states[indices]
            
            if self.rank == 0:
                print(f"å¹¶è¡Œè½®æ¬¡ {count+1}/{eval_loop_num}, çŠ¶æ€ç´¢å¼•: {indices}")
            
            # å¹¶è¡Œæ‰§è¡Œ episodes
            try:
                results = self._run_single_parallel_batch(env, env_name, current_init_states, env_num, save_video)
                
                # è¿”å›ç»“æœ
                for result in results:
                    yield result
                    
            except Exception as e:
                if self.rank == 0:
                    print(f"å¹¶è¡Œæ‰¹æ¬¡æ‰§è¡Œå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            
            count += 1
    
    def _run_single_parallel_batch(self, env, env_name, init_states, env_num, save_video):
        """æ‰§è¡Œå•ä¸ªå¹¶è¡Œæ‰¹æ¬¡ - æ”¯æŒè½»é‡çº§å¹¶è¡Œç¯å¢ƒ"""
        # æ£€æŸ¥ç¯å¢ƒç±»å‹
        is_vector_env = hasattr(env, 'num_envs') or 'VectorEnv' in str(type(env))
        
        if not is_vector_env or env_num == 1:
            # ä½¿ç”¨æ‰¹å¤„ç†æ¨¡æ‹Ÿå¹¶è¡Œï¼ˆæ¨èç­–ç•¥ï¼‰
            return self._run_batch_simulated_parallel(env, env_name, init_states, save_video)
        
        # çœŸå®å¹¶è¡Œç¯å¢ƒå¤„ç†
        if self.rank == 0:
            print(f"ğŸš€ æ‰§è¡ŒçœŸå®å¹¶è¡Œå¤„ç† ({env_num} ä¸ªç¯å¢ƒ)")
        
        # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
        obs_any = env.reset()
        # é»˜è®¤ä¸åœ¨å¹¶è¡Œæ¨¡å¼ä½¿ç”¨ set_init_stateï¼ˆé¿å… MuJoCo qpos ç»´åº¦ä¸åŒ¹é…å¯¼è‡´å­è¿›ç¨‹å´©æºƒï¼‰
        use_parallel_init = False
        try:
            if getattr(self, 'config', None) and hasattr(self.config, 'features'):
                use_parallel_init = bool(getattr(self.config.features, 'use_parallel_init_state', False))
                if self.rank == 0:
                    print(f"ğŸ”§ å¹¶è¡ŒçŠ¶æ€è®¾ç½®é…ç½®: use_parallel_init_state = {use_parallel_init}")
        except Exception as e:
            use_parallel_init = False
            if self.rank == 0:
                print(f"âš ï¸ è¯»å–å¹¶è¡ŒçŠ¶æ€é…ç½®å¤±è´¥: {e}")
                print(f"   configå­˜åœ¨: {getattr(self, 'config', None) is not None}")
                if getattr(self, 'config', None):
                    print(f"   featureså­˜åœ¨: {hasattr(self.config, 'features')}")
                    if hasattr(self.config, 'features'):
                        print(f"   featureså†…å®¹: {self.config.features}")

        if self.rank == 0:
            print(f"ğŸ”§ æœ€ç»ˆå†³å®š: use_parallel_init = {use_parallel_init}")
        if use_parallel_init and init_states is not None:
            try:
                # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿çŠ¶æ€æ ¼å¼æ­£ç¡®
                processed_states = self._process_init_states_for_parallel(init_states, env_num)

                # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿çŠ¶æ€æ•°æ®èƒ½æ­£ç¡®åºåˆ—åŒ–åˆ°å­è¿›ç¨‹
                if processed_states is not None and len(processed_states) > 0:
                    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨åŸå§‹RIPTçš„æ­£ç¡®æ–¹å¼ - é€šè¿‡resetæ–¹æ³•ä¼ é€’åˆå§‹çŠ¶æ€
                    # ç¡®ä¿çŠ¶æ€æ•°æ®æ ¼å¼æ­£ç¡®ï¼ˆnumpyæ•°ç»„æ ¼å¼ï¼Œä¸åŸå§‹RIPTä¿æŒä¸€è‡´ï¼‰
                    if isinstance(processed_states, list):
                        # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œä¿æŒ[env_num, state_dim]æ ¼å¼
                        init_states_array = np.array(processed_states, dtype=np.float64)
                    else:
                        init_states_array = processed_states
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨resetæ–¹æ³•è€Œä¸æ˜¯set_init_stateæ–¹æ³•
                    obs_any = env.reset(init_states=init_states_array)
                    if self.rank == 0:
                        print(f"âœ… å¹¶è¡ŒçŠ¶æ€è®¾ç½®æˆåŠŸï¼ŒçŠ¶æ€æ•°é‡: {len(init_states_array)}")
                else:
                    # æ²¡æœ‰åˆå§‹çŠ¶æ€æ—¶ï¼Œä½¿ç”¨æ™®é€šreset
                    obs_any = env.reset()
            except Exception as e:
                if self.rank == 0:
                    print(f"âš ï¸ å¹¶è¡ŒçŠ¶æ€è®¾ç½®å¤±è´¥: {e}")
                    print(f"   çŠ¶æ€ç±»å‹: {type(init_states)}")
                    if hasattr(init_states, 'shape'):
                        print(f"   çŠ¶æ€å½¢çŠ¶: {init_states.shape}")
                    print(f"   çŠ¶æ€æ•°æ®ç±»å‹: {init_states.dtype if hasattr(init_states, 'dtype') else 'N/A'}")
                # é‡æ–°resetç¯å¢ƒ
                obs_any = env.reset()
        else:
            if self.rank == 0:
                if init_states is not None:
                    print("â„¹ï¸ å¹¶è¡Œæ¨¡å¼ä¸‹è·³è¿‡çŠ¶æ€è®¾ç½®ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆé¿å…MuJoCoçŠ¶æ€æ ¼å¼ä¸å…¼å®¹ï¼‰")
                else:
                    print("â„¹ï¸ å¹¶è¡Œæ¨¡å¼ä¸‹ä½¿ç”¨éšæœºåˆå§‹åŒ–")
        obs_list = self._ensure_list_of_dict_obs(obs_any, env_num)
        
        if self.rank == 0:
            print(f"ğŸ”§ åˆå§‹åŒ– {len(obs_list)} ä¸ªå¹¶è¡Œç¯å¢ƒ")
        
        # å¯¹æ¯ä¸ªç¯å¢ƒè¿›è¡Œçƒ­èº«
        dummy_action = np.array([0, 0, 0, 0, 0, 0, -1])
        for warmup_step in range(10):
            # ğŸ”‘ ç¡®ä¿actionsæ•°ç»„é•¿åº¦ä¸ç¯å¢ƒæ•°é‡å®Œå…¨åŒ¹é…
            actions = [dummy_action.copy() for _ in range(env_num)]
            if self.rank == 0 and warmup_step == 0:
                print(f"ğŸ”§ çƒ­èº«åŠ¨ä½œ: {len(actions)} ä¸ªåŠ¨ä½œ for {env_num} ä¸ªç¯å¢ƒ")
            step_out = env.step(actions)
            obs_any = step_out[0] if isinstance(step_out, (list, tuple)) and len(step_out) >= 1 else step_out
            obs_list = self._ensure_list_of_dict_obs(obs_any, env_num)
        
        # åˆå§‹åŒ–episodeæ•°æ®
        episodes_data = []
        for i in range(len(obs_list)):
            episodes_data.append({
                'observations': [obs_list[i]],
                'actions': [],
                'rewards': [],
                'dones': [],
                'infos': [],
                'success': False,
                'total_reward': 0.0,
                'step': 0,
                'action_buffer': None,
                'action_index': 0,
                'rollout_images': [] if save_video else None,  # ğŸ¬ è§†é¢‘å¸§å­˜å‚¨
                'completed': False
            })
            
            # ğŸ¬ æ”¶é›†åˆå§‹è§‚æµ‹å›¾åƒç”¨äºè§†é¢‘
            if save_video:
                try:
                    initial_img = self.to_hwc_hmirror(obs_list[i]["agentview_image"])
                    episodes_data[i]['rollout_images'].append(initial_img)
                except Exception as e:
                    if self.rank == 0:
                        print(f"âš ï¸ æ”¶é›†åˆå§‹å›¾åƒå¤±è´¥ (ç¯å¢ƒ{i}): {e}")
                    episodes_data[i]['rollout_images'] = None
        
        max_steps = self.max_steps
        all_done = False
        
        # å¹¶è¡Œæ‰§è¡Œsteps - å…³é”®ä¼˜åŒ–ï¼šæ¨¡å‹æ¨ç†åœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œ
        while not all_done:
            # ğŸ”¥ æ‰¹é‡å¤„ç†æ‰€æœ‰ç¯å¢ƒçš„è§‚æµ‹ (é¿å…é€ä¸ªæ¨ç†)
            need_inference_indices = []
            observations_to_infer = []
            
            # ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«éœ€è¦æ¨ç†çš„ç¯å¢ƒ
            for i, obs in enumerate(obs_list):
                episode = episodes_data[i]
                
                if episode['dones'] and len(episode['dones']) > 0 and episode['dones'][-1]:
                    # ç¯å¢ƒå·²å®Œæˆï¼Œè·³è¿‡
                    continue
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¨ç†
                if (episode['action_buffer'] is None or 
                    episode['action_index'] >= episode['action_buffer'].shape[0]):
                    need_inference_indices.append(i)
                    observations_to_infer.append(obs)
            
            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡æ¨ç† (å…³é”®ä¼˜åŒ–: ä¸€æ¬¡æ¨ç†å¤šä¸ªè§‚æµ‹)
            if observations_to_infer:
                prompts_for_obs = None
                if hasattr(self, '_vector_env_prompts'):
                    prompts_for_obs = [self._vector_env_prompts[i] if i < len(self._vector_env_prompts) else env_name for i in need_inference_indices]
                batch_actions = self._batch_policy_inference(observations_to_infer, env_name, prompts_for_obs)
                
                # åˆ†é…æ¨ç†ç»“æœåˆ°å¯¹åº”çš„episode
                for batch_idx, env_idx in enumerate(need_inference_indices):
                    episode = episodes_data[env_idx]
                    obs = obs_list[env_idx]
                    
                    # å¤„ç†åŠ¨ä½œä¸çŠ¶æ€åç§»
                    action_buffer = batch_actions[batch_idx]
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šçª—å£ä¸€è‡´çš„æ®‹å·®åŸºå‡†
                    # è·å–æ¨ç†æ—¶åˆ»çš„çŠ¶æ€åç§»ï¼ˆæ•´ä¸ª50æ­¥åŠ¨ä½œåºåˆ—ä½¿ç”¨åŒä¸€ä¸ªåŸºå‡†ï¼‰
                    import robosuite.utils.transform_utils as T
                    unnorm_state = np.concatenate([
                        obs["robot0_eef_pos"],
                        T.quat2axisangle(obs["robot0_eef_quat"]),
                        obs["robot0_gripper_qpos"],
                    ], dtype=np.float32)
                    
                    # å¯¹æ•´ä¸ªåŠ¨ä½œåºåˆ—åº”ç”¨åŒä¸€ä¸ªçŠ¶æ€åç§»ï¼ˆå‰6ç»´ï¼šä½ç½®+æ—‹è½¬ï¼‰
                    action_buffer[:, :6] += unnorm_state[None, :6]
                    
                    episode['action_buffer'] = action_buffer
                    episode['action_index'] = 0
            
            # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ‰€æœ‰ç¯å¢ƒæ„å»ºåŠ¨ä½œæ•°ç»„ï¼ˆç¡®ä¿é¡ºåºå’Œæ•°é‡åŒ¹é…ï¼‰
            actions_to_execute = []
            for i, obs in enumerate(obs_list):
                episode = episodes_data[i]
                
                if episode['completed'] or (episode['dones'] and len(episode['dones']) > 0 and episode['dones'][-1]):
                    # ç¯å¢ƒå·²å®Œæˆï¼Œä½¿ç”¨dummyåŠ¨ä½œ
                    actions_to_execute.append(dummy_action)
                else:
                    # è·å–å½“å‰åŠ¨ä½œï¼ˆåº”è¯¥å·²ç»æœ‰action_bufferäº†ï¼‰
                    if episode['action_buffer'] is not None:
                        current_action = episode['action_buffer'][episode['action_index'], :7]
                        actions_to_execute.append(current_action)
                        episode['action_index'] += 1
                    else:
                        # å¤‡ç”¨ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰action_bufferï¼Œä½¿ç”¨dummyåŠ¨ä½œ
                        actions_to_execute.append(dummy_action)
            
            # ğŸ”§ åŠ¨ä½œè£å‰ªï¼šç¡®ä¿åŠ¨ä½œåœ¨ç¯å¢ƒçš„åˆæ³•èŒƒå›´å†…
            if hasattr(env, 'action_space'):
                action_space = env.action_space
                if hasattr(action_space, 'low') and hasattr(action_space, 'high'):
                    # å•ç¯å¢ƒåŠ¨ä½œç©ºé—´
                    action_low = action_space.low
                    action_high = action_space.high
                    actions_to_execute = [np.clip(action, action_low, action_high) for action in actions_to_execute]
                elif hasattr(action_space, 'spaces'):
                    # å‘é‡ç¯å¢ƒï¼Œæ¯ä¸ªå­ç¯å¢ƒæœ‰ç‹¬ç«‹çš„åŠ¨ä½œç©ºé—´
                    clipped_actions = []
                    for i, action in enumerate(actions_to_execute):
                        if i < len(action_space.spaces):
                            sub_space = action_space.spaces[i]
                            clipped_action = np.clip(action, sub_space.low, sub_space.high)
                        else:
                            # ä½¿ç”¨é»˜è®¤èŒƒå›´[-1, 1]
                            clipped_action = np.clip(action, -1, 1)
                        clipped_actions.append(clipped_action)
                    actions_to_execute = clipped_actions
                else:
                    # ä½¿ç”¨é»˜è®¤èŒƒå›´[-1, 1]è¿›è¡Œè£å‰ª
                    actions_to_execute = [np.clip(action, -1, 1) for action in actions_to_execute]
            else:
                # æ²¡æœ‰action_spaceä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤èŒƒå›´[-1, 1]
                actions_to_execute = [np.clip(action, -1, 1) for action in actions_to_execute]
            
            # å¹¶è¡Œæ‰§è¡ŒåŠ¨ä½œ
            step_out = env.step(actions_to_execute)
            if isinstance(step_out, (list, tuple)) and len(step_out) >= 4:
                obs_any, rewards_any, dones_any, infos_any = step_out[:4]
            else:
                obs_any, rewards_any, dones_any, infos_any = step_out, 0.0, False, {}

            obs_list = self._ensure_list_of_dict_obs(obs_any, env_num)
            rewards = self._ensure_list_generic(rewards_any, env_num, 0.0, expect_dict=False)
            dones = self._ensure_list_generic(dones_any, env_num, False, expect_dict=False)
            infos = self._ensure_list_generic(infos_any, env_num, {}, expect_dict=True)
            
            # æ›´æ–°episodeæ•°æ®
            all_done = True
            for i in range(len(obs_list)):
                episode = episodes_data[i]
                
                # å·²å®Œæˆçš„ç¯å¢ƒä¸å†è¿½åŠ ä»»ä½•æ•°æ®ï¼Œä¿æŒä¸å•ç¯å¢ƒé€»è¾‘ä¸€è‡´ï¼ˆå®Œæˆå³åœæ­¢ï¼‰
                if episode['completed']:
                    continue
                
                episode['observations'].append(obs_list[i])
                episode['actions'].append(actions_to_execute[i])
                episode['rewards'].append(rewards[i])
                episode['dones'].append(dones[i])
                episode['infos'].append(infos[i])
                episode['total_reward'] += rewards[i]
                episode['step'] += 1
                
                # ğŸ¬ æ”¶é›†å›¾åƒç”¨äºè§†é¢‘
                if save_video and episode['rollout_images'] is not None:
                    try:
                        frame_img = self.to_hwc_hmirror(obs_list[i]["agentview_image"])
                        episode['rollout_images'].append(frame_img)
                    except Exception as e:
                        if self.rank == 0:
                            print(f"âš ï¸ æ”¶é›†å›¾åƒå¸§å¤±è´¥ (ç¯å¢ƒ{i}): {e}")
                
                if infos[i].get("success", False) or episode['total_reward'] > 0.5:
                    episode['success'] = True
                
                # å®Œæˆæ¡ä»¶ï¼šæœ¬æ­¥è¿”å› done æˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°
                if dones[i] or episode['step'] >= max_steps:
                    episode['completed'] = True
                
            # åªè¦å­˜åœ¨ä»»ä¸€æœªå®Œæˆçš„ç¯å¢ƒï¼Œåˆ™ç»§ç»­å¾ªç¯
            for i in range(len(obs_list)):
                if not episodes_data[i]['completed']:
                    all_done = False
                    break
        
        # ğŸ¬ ä¿å­˜è§†é¢‘å¹¶è¿”å›ç»“æœ
        results = []
        for i, episode in enumerate(episodes_data):
            # ä¿å­˜è§†é¢‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            video_path = None
            if save_video and episode['rollout_images'] and len(episode['rollout_images']) > 0:
                try:
                    from datetime import datetime
                    from pathlib import Path
                    import imageio
                    
                    # åˆ›å»ºè§†é¢‘ç›®å½•
                    video_dir = Path("pi0/ript/debug_images/videos")
                    video_dir.mkdir(parents=True, exist_ok=True)
                    
                    # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    task_str = env_name.replace(" ", "_")[:30] if isinstance(env_name, str) else f"parallel_env_{i}"
                    success_str = "success" if episode['success'] else "failure"
                    video_path = video_dir / f"{timestamp}_{task_str}_env{i}_{success_str}.mp4"
                    
                    # ä¿å­˜è§†é¢‘
                    writer = imageio.get_writer(str(video_path), fps=30)
                    for frame in episode['rollout_images']:
                        writer.append_data(frame)
                    writer.close()
                    
                    if self.rank == 0:
                        print(f"âœ… å·²ä¿å­˜è§†é¢‘ (ç¯å¢ƒ{i}): {video_path}")
                except Exception as e:
                    if self.rank == 0:
                        print(f"âš ï¸ ä¿å­˜è§†é¢‘å¤±è´¥ (ç¯å¢ƒ{i}): {e}")
            
            episode_data = {
                "observations": episode['observations'],
                "actions": episode['actions'],
                "rewards": episode['rewards'],
                "dones": episode['dones'],
                "infos": episode['infos'],
                "task": self._vector_env_prompts[i] if hasattr(self, '_vector_env_prompts') and i < len(self._vector_env_prompts) else env_name,
            }
            
            # æ·»åŠ è§†é¢‘è·¯å¾„ä¿¡æ¯
            if video_path:
                episode_data["video_path"] = str(video_path)
            
            results.append((episode['success'], episode['total_reward'], episode_data))
        
        return results
    
    def _batch_policy_inference(self, observations, env_name, prompts_for_obs=None):
        """æ‰¹é‡æ¨¡å‹æ¨ç† - é¿å…é€ä¸ªæ¨ç†æé«˜æ•ˆç‡"""
        if not observations:
            return []
        
        
        # æ„å»ºæ‰¹é‡è§‚æµ‹
        batch_obs = []
        for idx, obs in enumerate(observations):
            prompt_text = None
            if prompts_for_obs is not None and idx < len(prompts_for_obs):
                prompt_text = prompts_for_obs[idx]
            pi0_obs = self.construct_pi0_observation(obs, prompt_text or env_name)
            batch_obs.append(pi0_obs)
        
        # åˆå¹¶æ‰¹é‡è§‚æµ‹ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        if len(batch_obs) == 1:
            # å•ä¸ªè§‚æµ‹ç›´æ¥æ¨ç† - å¼ºåˆ¶è¦æ±‚YAMLä¸­é…ç½®CFGå‚æ•°
            cfg_scale = None
            if self.config:
                # å¯¹è±¡æ¨¡å¼
                if hasattr(self.config, 'algo') and hasattr(self.config.algo, 'collection_cfg_scale'):
                    cfg_scale = self.config.algo.collection_cfg_scale
                # dictæ¨¡å¼
                elif isinstance(self.config, dict):
                    cfg_scale = self.config.get('algo', {}).get('collection_cfg_scale', None)
            
            if cfg_scale is None:
                raise ValueError("âŒ æœªåœ¨YAMLçš„algo.collection_cfg_scaleä¸­æ‰¾åˆ°CFG scaleé…ç½®ï¼")
            
            print(f"âœ… ä½¿ç”¨CFG Scale (å•è§‚æµ‹): {cfg_scale}")
            raw_action = self.policy.select_action(batch_obs[0], cfg_scale=cfg_scale)
            action = raw_action[0, :, :7]  # (50, 7)
            
            if isinstance(action, torch.Tensor):
                action_after_cpu = action.cpu().numpy()
            else:
                action_after_cpu = action
            
            action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
            return [action_buffer]
        else:
            # ğŸš€ ä¼˜åŒ–ï¼šå°è¯•çœŸæ­£çš„æ‰¹æ¨ç†ï¼Œå¤±è´¥æ—¶å›é€€åˆ°å¾ªç¯æ¨ç†
            try:
                batch_observation = self._stack_pi0_observations(batch_obs)
                
                # ğŸ”¥ å¼ºåˆ¶è¦æ±‚YAMLä¸­é…ç½®CFGå‚æ•°ï¼ˆæ‰¹æ¨ç†ï¼‰
                cfg_scale = None
                if self.config:
                    # å¯¹è±¡æ¨¡å¼
                    if hasattr(self.config, 'algo') and hasattr(self.config.algo, 'collection_cfg_scale'):
                        cfg_scale = self.config.algo.collection_cfg_scale
                    # dictæ¨¡å¼
                    elif isinstance(self.config, dict):
                        cfg_scale = self.config.get('algo', {}).get('collection_cfg_scale', None)
                
                if cfg_scale is None:
                    raise ValueError("âŒ æœªåœ¨YAMLçš„algo.collection_cfg_scaleä¸­æ‰¾åˆ°CFG scaleé…ç½®ï¼")
                
                print(f"âœ… ä½¿ç”¨CFG Scale (æ‰¹æ¨ç†): {cfg_scale}")
                
                # ä¸€æ¬¡æ€§æ‰¹æ¨ç† - è¿™æ˜¯æ ¸å¿ƒä¼˜åŒ–ç‚¹
                raw_actions = self.policy.select_action(batch_observation, cfg_scale=cfg_scale)
                # raw_actions shape: (B, T, 7)
                
                # åˆ†è§£æ‰¹é‡ç»“æœ
                batch_actions = []
                for i in range(len(batch_obs)):
                    action = raw_actions[i, :, :7]  # (50, 7)
                    
                    if isinstance(action, torch.Tensor):
                        action_after_cpu = action.cpu().numpy()
                    else:
                        action_after_cpu = action
                    
                    action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                    batch_actions.append(action_buffer)
                
                if self.rank == 0:
                    print(f"ğŸš€ æ‰¹æ¨ç†æˆåŠŸï¼š{len(batch_obs)} envs -> 1æ¬¡GPUè°ƒç”¨")
                return batch_actions
                
            except Exception as e:
                if self.rank == 0:
                    print(f"âš ï¸ æ‰¹æ¨ç†å¤±è´¥ï¼Œå›é€€åˆ°å¾ªç¯æ¨ç†: {e}")
                # å›é€€åˆ°åŸæœ‰çš„å¾ªç¯æ¨ç†
            
            # ğŸ”„ å›é€€è·¯å¾„ï¼šå¾ªç¯æ¨ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            batch_actions = []
            for pi0_obs in batch_obs:
                cfg_scale = getattr(self.config, 'collection_cfg_scale', None)
            if cfg_scale is None and self.config and hasattr(self.config, 'algo'):
                cfg_scale = getattr(self.config.algo, 'collection_cfg_scale', None)
            if cfg_scale is None:
                print(f"âš ï¸ æœªæ‰¾åˆ°collection_cfg_scaleé…ç½®ï¼Œè¯·åœ¨YAMLä¸­è®¾ç½®")
                cfg_scale = 1.5  # ä¸´æ—¶å›é€€
                raw_action = self.policy.select_action(pi0_obs, cfg_scale=cfg_scale)
                action = raw_action[0, :, :7]
                
                if isinstance(action, torch.Tensor):
                    action_after_cpu = action.cpu().numpy()
                else:
                    action_after_cpu = action
                
                action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                batch_actions.append(action_buffer)
            
            return batch_actions
    
    def _stack_pi0_observations(self, batch_obs):
        """å°†å¤šä¸ªPI0è§‚æµ‹å †å æˆæ‰¹é‡è§‚æµ‹
        
        è¾“å…¥: [obs1, obs2, ...] æ¯ä¸ªobsæ˜¯å•ç‹¬çš„PI0è§‚æµ‹å­—å…¸
        è¾“å‡º: æ‰¹é‡è§‚æµ‹å­—å…¸ï¼Œæ‰€æœ‰å¼ é‡çš„batchç»´åº¦å †å 
        """
        if not batch_obs:
            raise ValueError("batch_obsä¸èƒ½ä¸ºç©º")
        
        # è·å–ç¬¬ä¸€ä¸ªè§‚æµ‹ä½œä¸ºæ¨¡æ¿
        template_obs = batch_obs[0]
        batch_size = len(batch_obs)
        
        # æ„å»ºæ‰¹é‡è§‚æµ‹å­—å…¸
        batched_observation = {}
        
        # å¤„ç†å›¾åƒ
        if "image" in template_obs:
            batched_observation["image"] = {}
            for img_key in template_obs["image"]:
                # ğŸ”¥ ä¿®å¤ï¼šæ¯ä¸ªobs["image"][img_key]å·²ç»æ˜¯[1, C, H, W]ï¼Œéœ€è¦å…ˆsqueezeå†stack
                img_tensors = [obs["image"][img_key].squeeze(0) for obs in batch_obs]  # å»æ‰batchç»´åº¦1
                batched_observation["image"][img_key] = torch.stack(img_tensors, dim=0)  # -> [B, C, H, W]
        
        # å¤„ç†çŠ¶æ€  
        if "state" in template_obs:
            # ğŸ”¥ ä¿®å¤ï¼šæ¯ä¸ªobs["state"]å·²ç»æ˜¯[1, state_dim]ï¼Œéœ€è¦å…ˆsqueezeå†stack
            state_tensors = [obs["state"].squeeze(0) for obs in batch_obs]  # å»æ‰batchç»´åº¦1
            batched_observation["state"] = torch.stack(state_tensors, dim=0)  # -> [B, state_dim]
        
        # å¤„ç†æç¤ºæ–‡æœ¬
        if "prompt" in template_obs:
            # åˆå¹¶æ‰€æœ‰æç¤ºæ–‡æœ¬åˆ°åˆ—è¡¨
            batched_observation["prompt"] = [obs["prompt"][0] if isinstance(obs["prompt"], list) else obs["prompt"] 
                                           for obs in batch_obs]
        
        # å¤„ç†è¯­è¨€tokensï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "lang_tokens" in template_obs:
            # ğŸ”¥ ä¿®å¤ï¼šæ¯ä¸ªobs["lang_tokens"]å¯èƒ½å·²ç»æœ‰batchç»´åº¦ï¼Œéœ€è¦squeeze
            lang_tokens_list = [obs["lang_tokens"].squeeze(0) if obs["lang_tokens"].dim() > 1 else obs["lang_tokens"] for obs in batch_obs]
            batched_observation["lang_tokens"] = torch.stack(lang_tokens_list, dim=0)
        
        if "lang_masks" in template_obs:
            # ğŸ”¥ ä¿®å¤ï¼šæ¯ä¸ªobs["lang_masks"]å¯èƒ½å·²ç»æœ‰batchç»´åº¦ï¼Œéœ€è¦squeeze
            lang_masks_list = [obs["lang_masks"].squeeze(0) if obs["lang_masks"].dim() > 1 else obs["lang_masks"] for obs in batch_obs]
            batched_observation["lang_masks"] = torch.stack(lang_masks_list, dim=0)
        
        return batched_observation
    
    def _fallback_individual_inference(self, observations, env_name, prompts_for_obs=None):
        """å›é€€åˆ°é€ä¸ªæ¨ç†"""
        batch_actions = []
        for idx, obs in enumerate(observations):
            prompt_text = None
            if prompts_for_obs is not None and idx < len(prompts_for_obs):
                prompt_text = prompts_for_obs[idx]
            pi0_obs = self.construct_pi0_observation(obs, prompt_text or env_name)
            cfg_scale = None
            if hasattr(self, 'policy') and hasattr(self.policy, 'default_cfg_scale'):
                cfg_scale = getattr(self.policy, 'default_cfg_scale', None)
            if cfg_scale is None:
                cfg_scale = getattr(self.config, 'collection_cfg_scale', None) if self.config else None
            if cfg_scale is None:
                cfg_scale = 1.5
            raw_action = self.policy.select_action(pi0_obs, cfg_scale=cfg_scale)
            action = raw_action[0, :, :7]
            
            if isinstance(action, torch.Tensor):
                action_after_cpu = action.cpu().numpy()
            else:
                action_after_cpu = action
            
            action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
            batch_actions.append(action_buffer)
        
        return batch_actions
    
    def run_policy_in_env_batch(self, env_name, init_states, batch_size=None):
        """
        æ‰¹é‡æ‰§è¡Œå¤šä¸ªepisodeï¼Œç¡®ä¿æ¯ä¸ªä½¿ç”¨æŒ‡å®šçš„åˆå§‹çŠ¶æ€
        
        Args:
            env_name: ç¯å¢ƒåç§°
            init_states: åˆå§‹çŠ¶æ€åˆ—è¡¨ï¼Œé•¿åº¦åº”ä¸ºbatch_size
            batch_size: æ‰¹é‡å¤§å°ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨init_statesçš„é•¿åº¦
        
        Returns:
            list: episodeåˆ—è¡¨ï¼Œé•¿åº¦ä¸ºbatch_size
        """
        if batch_size is None:
            batch_size = len(init_states) if init_states else 8
        
        # ç¡®ä¿init_statesæ•°é‡æ­£ç¡®
        if init_states is not None and len(init_states) != batch_size:
            # å¤åˆ¶æˆ–æˆªæ–­åˆ°æ­£ç¡®å¤§å°
            if len(init_states) < batch_size:
                init_states = init_states * (batch_size // len(init_states) + 1)
            init_states = init_states[:batch_size]
        
        # ä¼˜å…ˆä½¿ç”¨å¹¶è¡Œç¯å¢ƒï¼ˆå¦‚æœå¯ç”¨ä¸”é…ç½®å…è®¸ï¼‰
        if self._can_use_parallel_envs(batch_size):
            return self._run_parallel_episodes_true(
                env=None,  # å°†åœ¨å†…éƒ¨åˆ›å»º
                policy=self.policy_wrapper,
                max_steps=self.max_steps,
                init_states=init_states,
                num_episodes=batch_size
            )
        else:
            # å›é€€åˆ°ä¸²è¡Œæ‰§è¡Œ
            if self.rank == 0:
                print(f"ğŸ“‹ æ‰¹é‡æ‰§è¡Œå›é€€åˆ°ä¸²è¡Œæ¨¡å¼ï¼ˆbatch_size={batch_size}ï¼‰")
            
            # åˆ›å»ºç¯å¢ƒ
            env = self._create_single_env(env_name)
            
            # ä½¿ç”¨ä¸²è¡Œæ¨¡å¼æ‰§è¡Œ
            episodes = []
            for success, total_reward, episode_data in self._run_serial_episodes(
                env, env_name, init_states, save_video=False
            ):
                # æ·»åŠ æˆåŠŸç‡å’Œæ€»å¥–åŠ±ä¿¡æ¯
                episode_data['success'] = success
                episode_data['total_reward'] = total_reward
                episodes.append(episode_data)
            
            # æ¸…ç†ç¯å¢ƒ
            try:
                env.close()
            except:
                pass
            
            return episodes

    def _verify_parallel_env_sync(self, obs_list):
        """
        éªŒè¯å¹¶è¡Œç¯å¢ƒçš„åˆå§‹çŠ¶æ€åŒæ­¥æ€§

        Args:
            obs_list: å¹¶è¡Œç¯å¢ƒresetè¿”å›çš„è§‚æµ‹åˆ—è¡¨

        Returns:
            bool: Trueè¡¨ç¤ºåŒæ­¥ï¼ŒFalseè¡¨ç¤ºä¸åŒæ­¥
        """
        if not isinstance(obs_list, list) or len(obs_list) < 2:
            return True  # å•ç¯å¢ƒæˆ–æ— æ•ˆè¾“å…¥ï¼Œè®¤ä¸ºåŒæ­¥

        try:
            # æå–æ¯ä¸ªç¯å¢ƒçš„çŠ¶æ€
            states = []
            hashes = []

            for i, obs in enumerate(obs_list):
                state = self._extract_state_from_obs(obs)
                state_hash = self._compute_state_hash(state)
                states.append(state)
                hashes.append(state_hash)
                print(f"   ç¯å¢ƒ {i}: çŠ¶æ€å“ˆå¸Œ = {state_hash}")

            # æ£€æŸ¥åŒæ­¥æ€§
            unique_hashes = set(hashes)
            is_synced = len(unique_hashes) == 1

            if is_synced:
                print(f"   ğŸ”’ æ‰€æœ‰ {len(obs_list)} ä¸ªç¯å¢ƒçŠ¶æ€å®Œå…¨åŒæ­¥")
            else:
                print(f"   âš ï¸ å‘ç° {len(unique_hashes)} ä¸ªä¸åŒçŠ¶æ€ï¼Œç¯å¢ƒæœªå®Œå…¨åŒæ­¥")
                # è¯¦ç»†æŠ¥å‘Šä¸åŒçŠ¶æ€çš„åˆ†å¸ƒ
                for j, unique_hash in enumerate(unique_hashes):
                    indices = [i for i, h in enumerate(hashes) if h == unique_hash]
                    print(f"     çŠ¶æ€ {j+1}: {unique_hash} (ç¯å¢ƒ {indices})")

            return is_synced

        except Exception as e:
            print(f"   âŒ åŒæ­¥éªŒè¯å¤±è´¥: {e}")
            return False

    def _can_use_parallel_envs(self, batch_size):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨å¹¶è¡Œç¯å¢ƒ"""
        # ç®€åŒ–åˆ¤æ–­é€»è¾‘
        features_config = getattr(self.config, 'features', {})
        enable_parallel = features_config.get('enable_parallel_envs', False)
        enable_true_parallel = features_config.get('enable_true_parallel_envs', False)

        return enable_parallel and enable_true_parallel and batch_size > 1

    def _is_mujoco_state(self, state_data):
        """æ£€æŸ¥æ˜¯å¦ä¸ºMuJoCoçŠ¶æ€å‘é‡ï¼ˆä¸åŸç‰ˆRIPTå¯¹é½ï¼‰"""
        try:
            if isinstance(state_data, dict):
                return False  # è§‚æµ‹å­—å…¸ä¸æ˜¯MuJoCoçŠ¶æ€

            if hasattr(state_data, 'shape'):
                # æ£€æŸ¥æ˜¯å¦ä¸ºé«˜ç»´çŠ¶æ€å‘é‡ï¼ˆMuJoCoçŠ¶æ€é€šå¸¸>50ç»´ï¼ŒåŒ…æ‹¬92ç»´ï¼‰
                if len(state_data.shape) == 1 and state_data.shape[0] >= 50:
                    return True
                elif len(state_data.shape) == 2 and state_data.shape[-1] >= 50:
                    return True

            return False
        except:
            return False
    


    def _process_init_states_for_parallel(self, init_states, env_num):
        """å¤„ç†åˆå§‹çŠ¶æ€ä»¥é€‚é…å¹¶è¡Œç¯å¢ƒï¼ˆä¿®å¤MuJoCoæ ¼å¼é—®é¢˜ï¼‰"""
        try:
            if init_states is None:
                return None

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(init_states, torch.Tensor):
                init_states = init_states.cpu().numpy()

            # ç¡®ä¿æ˜¯numpyæ•°ç»„
            if not isinstance(init_states, np.ndarray):
                init_states = np.array(init_states)

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat64ï¼ˆMuJoCoæœŸæœ›ï¼‰
            if init_states.dtype != np.float64:
                init_states = init_states.astype(np.float64)

            # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
            if init_states.ndim == 1:
                # å•ä¸ªçŠ¶æ€ï¼Œå¤åˆ¶ç»™æ‰€æœ‰ç¯å¢ƒ
                init_states = np.tile(init_states, (env_num, 1))
            elif init_states.ndim == 2:
                if init_states.shape[0] == 1:
                    # å•ä¸ªçŠ¶æ€ï¼Œå¤åˆ¶ç»™æ‰€æœ‰ç¯å¢ƒ
                    init_states = np.tile(init_states[0], (env_num, 1))
                elif init_states.shape[0] != env_num:
                    # çŠ¶æ€æ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªçŠ¶æ€
                    init_states = np.tile(init_states[0], (env_num, 1))

            print(f"ğŸ”§ å¤„ç†åçš„å¹¶è¡ŒçŠ¶æ€: å½¢çŠ¶={init_states.shape}, ç±»å‹={init_states.dtype}")
            return init_states

        except Exception as e:
            print(f"âš ï¸ çŠ¶æ€å¤„ç†å¤±è´¥: {e}")
            return None
    
