import os
import sys
import torch
import numpy as np
from typing import List, Dict, Any, Tuple, Optional, Union, Callable
from datetime import datetime
import imageio
from pathlib import Path
import json
import multiprocessing
import gc

# å¯¼å…¥LIBEROç›¸å…³çš„å˜æ¢å‡½æ•° â€”â€” è‹¥å¤±è´¥ç›´æ¥æŠ›å¼‚å¸¸ï¼Œä¸å®˜æ–¹è„šæœ¬ä¿æŒä¸€è‡´
try:
    import robosuite.utils.transform_utils as T
except ImportError as e:
    raise ImportError("robosuite is required for LIBEROEnvRunner but not found") from e

# å¯¼å…¥å¹¶è¡Œç¯å¢ƒæ”¯æŒ
try:
    from libero.libero.envs import SubprocVectorEnv
    VECTOR_ENV_AVAILABLE = True
except ImportError:
    print("âš ï¸ SubprocVectorEnvä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¸²è¡Œç¯å¢ƒ")
    VECTOR_ENV_AVAILABLE = False

# è°ƒè¯•è®¾ç½®
DEBUG_SAVE_IMAGES = False  # ç¦ç”¨å•ç‹¬å›¾åƒä¿å­˜ï¼Œåªä¿ç•™è§†é¢‘
DEBUG_SAVE_VIDEO = os.environ.get("PI0_DEBUG_SAVE_VIDEO", "true").lower() in ("true", "1", "yes")
DEBUG_IMAGE_DIR = os.environ.get("PI0_DEBUG_IMAGE_DIR", "ript/debug_images")

class LIBEROEnvRunner:
    """LIBEROç¯å¢ƒè¿è¡Œå™¨ï¼Œæä¾›PI0ç­–ç•¥åœ¨LIBEROç¯å¢ƒä¸­çš„è¿è¡Œæœºåˆ¶"""
    
    def __init__(self, policy=None, benchmark_name=None, rollouts_per_env=1, 
                 num_parallel_envs=1, max_episode_length=500, task_names_to_use=None,
                 config=None, rank=0, world_size=1, norm_stats_path=None):
        """åˆå§‹åŒ–LIBEROç¯å¢ƒè¿è¡Œå™¨"""
        # å­˜å‚¨å‚æ•°
        self.policy = policy
        self.benchmark_name = benchmark_name
        self.rollouts_per_env = rollouts_per_env
        self.num_parallel_envs = num_parallel_envs
        self.max_episode_length = max_episode_length
        self.task_names_to_use = task_names_to_use or []
        self.task_names = task_names_to_use or []  # å…¼å®¹æ€§åˆ«åï¼Œä¾›rollout_generatorä½¿ç”¨
        self.max_steps = max_episode_length if max_episode_length is not None else 500
        
        # âœ… å­˜å‚¨åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
        self.config = config
        self.rank = rank
        self.world_size = world_size
        
        # ğŸ”¥ æ–°å¢ï¼šä»»åŠ¡è½®è¯¢æœºåˆ¶
        self.assigned_tasks = task_names_to_use or []  # åˆ†é…ç»™å½“å‰è¿›ç¨‹çš„ä»»åŠ¡åˆ—è¡¨
        self.task_cursor = 0                          # ä»»åŠ¡è½®è¯¢ç´¢å¼•
        self.current_task = None                      # å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡
        
        # åˆå§‹åŒ–å½“å‰ä»»åŠ¡
        if self.assigned_tasks:
            self.current_task = self.assigned_tasks[0]
            if self.rank == 0:
                print(f"ğŸ¯ LIBEROEnvRunneråˆå§‹åŒ–: åˆ†é…ä»»åŠ¡ {self.assigned_tasks}, å½“å‰ä»»åŠ¡: {self.current_task}")
        else:
            if self.rank == 0:
                print(f"âš ï¸ LIBEROEnvRunneråˆå§‹åŒ–: Rank {self.rank} æ²¡æœ‰åˆ†é…ä»»åŠ¡")
        
        # è°ƒè¯•é€‰é¡¹
        self.debug_save_images = DEBUG_SAVE_IMAGES
        self.debug_save_video = DEBUG_SAVE_VIDEO
        self.debug_image_dir = DEBUG_IMAGE_DIR
        
        # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
        self._load_norm_stats(norm_stats_path)
        
    def _load_norm_stats(self, norm_stats_path: Optional[str] = None):
        """Load normalization statistics from norm_stats.json"""
        if norm_stats_path is None:
            # å°è¯•åœ¨å¸¸è§ä½ç½®æ‰¾åˆ°norm_stats.json
            possible_paths = [
                "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch/norm_stats.json",
                "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json", 
                "./checkpoints/pi0_libero_pytorch/norm_stats.json",
                "./norm_stats.json"
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    norm_stats_path = path
                    break
        
        if norm_stats_path and Path(norm_stats_path).exists():
            print(f"LIBEROEnvRunner: Loading norm_stats from: {norm_stats_path}")
            with open(norm_stats_path) as f:
                norm_stats = json.load(f)
                
            # æå–çŠ¶æ€å’ŒåŠ¨ä½œçš„å½’ä¸€åŒ–å‚æ•°
            self.state_mean = np.array(norm_stats["norm_stats"]["state"]["mean"][:8], dtype=np.float32)
            self.state_std = np.array(norm_stats["norm_stats"]["state"]["std"][:8], dtype=np.float32)
            self.action_mean = np.array(norm_stats["norm_stats"]["actions"]["mean"][:7], dtype=np.float32)
            self.action_std = np.array(norm_stats["norm_stats"]["actions"]["std"][:7], dtype=np.float32)
            
            print(f"âœ… LIBEROEnvRunner: Loaded normalization stats for action post-processing")
        else:
            raise FileNotFoundError("norm_stats.json not found in expected locations; cannot proceed")

    # ğŸ”¥ æ–°å¢ï¼šä»»åŠ¡è½®è¯¢æ ¸å¿ƒæ–¹æ³•
    def set_current_task_by_cursor(self):
        """æ ¹æ®cursorè®¾ç½®å½“å‰ä»»åŠ¡"""
        if not self.assigned_tasks:
            self.current_task = None
            return None
        
        # ä½¿ç”¨æ¨¡è¿ç®—å®ç°å¾ªç¯è½®è¯¢
        self.current_task = self.assigned_tasks[self.task_cursor % len(self.assigned_tasks)]
        return self.current_task
    
    def advance_task_cursor(self):
        """æ¨è¿›ä»»åŠ¡cursoråˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡"""
        if not self.assigned_tasks:
            return None
        
        self.task_cursor += 1
        new_task = self.set_current_task_by_cursor()
        
        # è°ƒè¯•è¾“å‡º
        if self.rank == 0 or len(self.assigned_tasks) > 1:  # ä¸»è¿›ç¨‹æˆ–å¤šä»»åŠ¡æ—¶è¾“å‡º
            print(f"ğŸ”„ Rank {self.rank}: ä»»åŠ¡cursoræ¨è¿›åˆ° {self.task_cursor}, å½“å‰ä»»åŠ¡: {new_task}")
        
        return new_task
    
    def get_current_task(self):
        """è·å–å½“å‰ä»»åŠ¡"""
        return self.current_task
    
    def has_tasks(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é…çš„ä»»åŠ¡"""
        return len(self.assigned_tasks) > 0
    
    def construct_pi0_observation(self, obs, task_description):
        """æ„é€ PI0éœ€è¦çš„è§‚æµ‹æ ¼å¼ï¼Œå®Œå…¨æ¨¡ä»¿2_test_pi0_on_libero.pyçš„åšæ³•"""
        # è·å–è®¾å¤‡
        device = self.policy.config.device if hasattr(self.policy, 'config') else 'cuda:0'
        
        # çŠ¶æ€å¤„ç†
        import robosuite.utils.transform_utils as T
        
        axis_angle = T.quat2axisangle(obs["robot0_eef_quat"])
            
        unnorm_state = np.concatenate([
            obs["robot0_eef_pos"],                    # 3D: end-effector position
            axis_angle, # 3D: rotation as axis-angle  
            obs["robot0_gripper_qpos"],               # 2D: gripper joint positions
        ], dtype=np.float32)
        print(">> gripper_qpos shape:", np.asarray(obs["robot0_gripper_qpos"]).shape)
        # çŠ¶æ€å½’ä¸€åŒ–
        state = (unnorm_state - self.state_mean) / (self.state_std + 1e-6)
        
        # å›¾åƒå¤„ç†
        base_0_rgb = obs["agentview_image"][:, :, ::-1].copy()
        left_wrist_0_rgb = obs["robot0_eye_in_hand_image"][:, :, ::-1].copy()
        
        # æ„é€ è§‚æµ‹æ ¼å¼
        observation = {
            "image": {
                "base_0_rgb": torch.from_numpy(base_0_rgb).to(device)[None],
                "left_wrist_0_rgb": torch.from_numpy(left_wrist_0_rgb).to(device)[None],
            },
            "state": torch.from_numpy(state).to(device)[None],
            "prompt": [task_description],
        }
        
        return observation
    
    def denormalize_action(self, action: np.ndarray) -> np.ndarray:
        """Denormalize action using loaded statistics"""
        return action * (self.action_std + 1e-6) + self.action_mean
    
    def get_unnormalized_state(self, obs) -> np.ndarray:
        """ä»è§‚æµ‹ä¸­æå–æœªå½’ä¸€åŒ–çš„çŠ¶æ€ï¼ˆç”¨äºåŠ¨ä½œåç§»ï¼‰"""
        try:
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            
            if not isinstance(obs, dict) or not obs:
                raise RuntimeError("Required observation dict missing or empty")
            
            # ä½¿ç”¨ end-effector ä¿¡æ¯
            if "robot0_eef_pos" in obs and "robot0_eef_quat" in obs:
                eef_pos = np.array(obs["robot0_eef_pos"], dtype=np.float32)
                eef_quat = np.array(obs["robot0_eef_quat"], dtype=np.float32)
                
                if eef_pos.size != 3 or eef_quat.size != 4:
                    raise RuntimeError("Observation fields robot0_eef_pos or robot0_eef_quat have incorrect dimensions")
                
                # è½¬æ¢å››å…ƒæ•°ä¸ºè½´è§’
                try:
                    axis_angle = T.quat2axisangle(eef_quat).astype(np.float32)
                except Exception as e:
                    raise RuntimeError(f"Failed to convert quaternion to axis-angle: {e}")
                
                # è·å– gripper çŠ¶æ€
                if "robot0_gripper_qpos" not in obs:
                    raise RuntimeError("Observation missing required field robot0_gripper_qpos")
                try:
                    gripper_qpos = float(obs["robot0_gripper_qpos"][0])
                except (IndexError, TypeError, ValueError) as e:
                    raise RuntimeError(f"Invalid robot0_gripper_qpos value: {e}")
                
                # æ„é€ æœªå½’ä¸€åŒ–çš„çŠ¶æ€
                unnorm_state = np.concatenate([eef_pos[:3], axis_angle[:3], [gripper_qpos]]).astype(np.float32)
                return unnorm_state
                
            # å®˜æ–¹è„šæœ¬è¦æ±‚ä¸Šè¿°å…³é”®å­—æ®µå‡å­˜åœ¨ï¼Œè‹¥ç¼ºå¤±åˆ™ç«‹å³æŠ¥é”™
            else:
                raise RuntimeError("Observation missing required end-effector fields")
                
        except Exception as e:
            raise
    
    def make_env(self, env_name: str):
        """åˆ›å»ºLIBEROç¯å¢ƒ"""
        try:
            import gym
            from cleandiffuser.env import libero  # ç¡®ä¿ç¯å¢ƒæ³¨å†Œ
            
            # ä½¿ç”¨ä¸å‚è€ƒå®ç°2_test_pi0_on_libero.pyå®Œå…¨ç›¸åŒçš„ç¯å¢ƒé…ç½®
            benchmark_to_env_id = {
                'libero_spatial': 'libero-spatial-v0',
                'libero_object': 'libero-object-v0',
                'libero_goal': 'libero-goal-v0',
                'libero_10': 'libero-10-v0',
                'libero_90': 'libero-90-v0'
            }

            if self.benchmark_name not in benchmark_to_env_id:
                raise ValueError(f"Unknown benchmark_name: {self.benchmark_name}")

            env_id = benchmark_to_env_id[self.benchmark_name]
            task_id = 9  # ä¸å‚è€ƒå®ç°ä¿æŒä¸€è‡´
            
            # åˆ›å»ºç¯å¢ƒ
            env = gym.make(
                env_id,
                task_id=task_id,
                image_size=224,  # åŒ¹é…PI0çš„è¾“å…¥å°ºå¯¸
                camera_names=["agentview", "robot0_eye_in_hand"],  # åŒ¹é…åŸå§‹demo
                seed=0,  # ä½¿ç”¨ä¸å‚è€ƒå®ç°ç›¸åŒçš„éšæœºç§å­
            )
            
            # è·å–ä»»åŠ¡æè¿°
            if hasattr(env, 'task_description'):
                task_description = env.task_description
            else:
                task_description = env_name
                
            return env, task_description
            
        except Exception as e:
            raise RuntimeError(f"åˆ›å»ºç¯å¢ƒå¤±è´¥: {e}") from e
    
    def create_env(self, env_name: str):
        """åˆ›å»ºå•ä¸ªç¯å¢ƒï¼ˆç”¨äºå¹¶è¡Œç¯å¢ƒï¼‰"""
        env, task_description = self.make_env(env_name)
        return env, env_name, 1
    
    def create_parallel_envs(self, env_name: str):
        """åˆ›å»ºå¹¶è¡Œç¯å¢ƒï¼Œæ¨¡ä»¿åŸç‰ˆRIPTçš„å®ç°"""
        if not VECTOR_ENV_AVAILABLE or self.num_parallel_envs <= 1:
            # å¦‚æœä¸æ”¯æŒå¹¶è¡Œç¯å¢ƒæˆ–åªéœ€è¦1ä¸ªç¯å¢ƒï¼Œä½¿ç”¨å•ä¸ªç¯å¢ƒ
            if self.rank == 0:
                print(f"ä½¿ç”¨å•ä¸ªç¯å¢ƒ (num_parallel_envs={self.num_parallel_envs})")
            env, task_description = self.make_env(env_name)
            return env, env_name, 1
        
        # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
        if multiprocessing.get_start_method(allow_none=True) != "spawn":
            multiprocessing.set_start_method("spawn", force=True)
            if self.rank == 0:
                print("è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ä¸º'spawn'")
        
        # è®¡ç®—å®é™…å¹¶è¡Œç¯å¢ƒæ•°é‡
        env_num = min(self.num_parallel_envs, self.rollouts_per_env)
        
        if self.rank == 0:
            print(f"åˆ›å»º {env_num} ä¸ªå¹¶è¡Œç¯å¢ƒç”¨äºä»»åŠ¡ {env_name}")
        
        # åˆ›å»ºç¯å¢ƒå·¥å‚å‡½æ•°
        def env_factory():
            try:
                env, _ = self.make_env(env_name)
                return env
            except Exception as e:
                print(f"ç¯å¢ƒå·¥å‚å‡½æ•°åˆ›å»ºç¯å¢ƒå¤±è´¥: {e}")
                raise
        
        # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
        try:
            parallel_env = SubprocVectorEnv([env_factory for _ in range(env_num)])
            if self.rank == 0:
                print(f"âœ… æˆåŠŸåˆ›å»º {env_num} ä¸ªå¹¶è¡Œç¯å¢ƒ")
            return parallel_env, env_name, env_num
        except Exception as e:
            if self.rank == 0:
                print(f"âŒ å¹¶è¡Œç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
                print("å›é€€åˆ°å•ä¸ªç¯å¢ƒ")
            env, task_description = self.make_env(env_name)
            return env, env_name, 1
        
    def run_policy_in_env(self, env_name, all_init_states=None, debug_save_video=None, created_env=None):
        """åœ¨ç¯å¢ƒä¸­è¿è¡Œç­–ç•¥ï¼Œç”Ÿæˆè½¨è¿¹ - æ”¯æŒå¹¶è¡Œå’Œä¸²è¡Œç¯å¢ƒ"""
        save_video = debug_save_video if debug_save_video is not None else self.debug_save_video
        
        if created_env is None:
            # è‡ªåŠ¨åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
            env, env_id, env_num = self.create_parallel_envs(env_name)
        else:
            env, env_id, env_num = created_env
        
        if all_init_states is None:
            # å¦‚æœæ²¡æœ‰æä¾›åˆå§‹çŠ¶æ€ï¼Œç”Ÿæˆé»˜è®¤çŠ¶æ€
            all_init_states = np.zeros((self.rollouts_per_env, 8), dtype=np.float32)
        
        try:
            if env_num > 1 and hasattr(env, 'reset') and hasattr(env, 'step_async'):
                # ä½¿ç”¨çœŸæ­£çš„å¹¶è¡Œç¯å¢ƒ
                yield from self._run_parallel_episodes(env, env_name, all_init_states, env_num, save_video)
            else:
                # ä½¿ç”¨ä¸²è¡Œç¯å¢ƒï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
                yield from self._run_serial_episodes(env, env_name, all_init_states, save_video)
        
        finally:
            # æ¸…ç†ç¯å¢ƒ
            try:
                if hasattr(env, 'close'):
                    env.close()
                gc.collect()
            except Exception as e:
                if self.rank == 0:
                    print(f"ç¯å¢ƒæ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def _run_parallel_episodes(self, env, env_name, all_init_states, env_num, save_video):
        """è¿è¡Œå¹¶è¡Œepisodes"""
        if self.rank == 0:
            print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {env_num} ä¸ªç¯å¢ƒ")
        
        # è®¡ç®—éœ€è¦çš„è½®æ¬¡
        eval_loop_num = (self.rollouts_per_env + env_num - 1) // env_num
        count = 0
        
        while count < eval_loop_num:
            # é€‰æ‹©å½“å‰è½®æ¬¡çš„åˆå§‹çŠ¶æ€
            start_idx = count * env_num
            end_idx = min(start_idx + env_num, len(all_init_states))
            indices = np.arange(start_idx, end_idx) % len(all_init_states)
            current_init_states = all_init_states[indices]
            
            if self.rank == 0:
                print(f"å¹¶è¡Œè½®æ¬¡ {count+1}/{eval_loop_num}, çŠ¶æ€ç´¢å¼•: {indices}")
            
            # å¹¶è¡Œæ‰§è¡Œepisodes
            try:
                results = self._run_single_parallel_batch(env, env_name, current_init_states, env_num, save_video)
                
                # è¿”å›ç»“æœ
                for result in results:
                    yield result
                    
            except Exception as e:
                if self.rank == 0:
                    print(f"å¹¶è¡Œæ‰¹æ¬¡æ‰§è¡Œå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            
            count += 1
    
    def _run_single_parallel_batch(self, env, env_name, init_states, env_num, save_video):
        """æ‰§è¡Œå•ä¸ªå¹¶è¡Œæ‰¹æ¬¡"""
        # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
        obs_list = env.reset()
        if not isinstance(obs_list, list):
            obs_list = [obs_list]
        
        # å¯¹æ¯ä¸ªç¯å¢ƒè¿›è¡Œçƒ­èº«
        dummy_action = np.array([0, 0, 0, 0, 0, 0, -1])
        for warmup_step in range(20):
            actions = [dummy_action] * len(obs_list)
            obs_list, _, _, _ = env.step(actions)
        
        # åˆå§‹åŒ–episodeæ•°æ®
        episodes_data = []
        for i in range(len(obs_list)):
            episodes_data.append({
                'observations': [obs_list[i]],
                'actions': [],
                'rewards': [],
                'dones': [],
                'infos': [],
                'success': False,
                'total_reward': 0.0,
                'step': 0,
                'action_buffer': None,
                'action_index': 0
            })
        
        max_steps = self.max_steps
        all_done = False
        
        # å¹¶è¡Œæ‰§è¡Œsteps
        while not all_done:
            actions_to_execute = []
            
            # ä¸ºæ¯ä¸ªç¯å¢ƒå‡†å¤‡åŠ¨ä½œ
            for i, obs in enumerate(obs_list):
                episode = episodes_data[i]
                
                if episode['dones'] and len(episode['dones']) > 0 and episode['dones'][-1]:
                    # å¦‚æœç¯å¢ƒå·²å®Œæˆï¼Œä½¿ç”¨dummyåŠ¨ä½œ
                    actions_to_execute.append(dummy_action)
                    continue
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¨ç†æ–°çš„åŠ¨ä½œåºåˆ—
                if (episode['action_buffer'] is None or 
                    episode['action_index'] >= episode['action_buffer'].shape[0]):
                    
                    # æ¨ç†æ–°åŠ¨ä½œ
                    pi0_obs = self.construct_pi0_observation(obs, env_name)
                    raw_action = self.policy.select_action(pi0_obs)
                    action = raw_action[0, :, :7]  # shape: (50, 7)
                    
                    # å¤„ç†åŠ¨ä½œ
                    if isinstance(action, torch.Tensor):
                        action_after_cpu = action.cpu().numpy()
                    else:
                        action_after_cpu = action
                    
                    # åå½’ä¸€åŒ–å’ŒçŠ¶æ€åç§»
                    action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                    
                    # è·å–çŠ¶æ€åç§»
                    unnorm_state = np.concatenate([
                        obs["robot0_eef_pos"],
                        T.quat2axisangle(obs["robot0_eef_quat"]),
                        obs["robot0_gripper_qpos"],
                    ], dtype=np.float32)
                    
                    action_buffer[:, :6] += unnorm_state[None, :6]
                    
                    episode['action_buffer'] = action_buffer
                    episode['action_index'] = 0
                
                # è·å–å½“å‰åŠ¨ä½œ
                current_action = episode['action_buffer'][episode['action_index'], :7]
                actions_to_execute.append(current_action)
                episode['action_index'] += 1
            
            # å¹¶è¡Œæ‰§è¡ŒåŠ¨ä½œ
            obs_list, rewards, dones, infos = env.step(actions_to_execute)
            
            # æ›´æ–°episodeæ•°æ®
            all_done = True
            for i in range(len(obs_list)):
                episode = episodes_data[i]
                
                # è®°å½•æ•°æ®
                episode['observations'].append(obs_list[i])
                episode['actions'].append(actions_to_execute[i])
                episode['rewards'].append(rewards[i])
                episode['dones'].append(dones[i])
                episode['infos'].append(infos[i])
                episode['total_reward'] += rewards[i]
                episode['step'] += 1
                
                # æ£€æŸ¥æˆåŠŸçŠ¶æ€
                if infos[i].get("success", False) or episode['total_reward'] > 0.5:
                    episode['success'] = True
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if not dones[i] and episode['step'] < max_steps:
                    all_done = False
        
        # è¿”å›ç»“æœ
        results = []
        for i, episode in enumerate(episodes_data):
            episode_data = {
                "observations": episode['observations'],
                "actions": episode['actions'],
                "rewards": episode['rewards'],
                "dones": episode['dones'],
                "infos": episode['infos'],
                "task": env_name,
            }
            results.append((episode['success'], episode['total_reward'], episode_data))
        
        return results
    
    def _run_serial_episodes(self, env, env_name, all_init_states, save_video):
        """è¿è¡Œä¸²è¡Œepisodesï¼ˆåŸæœ‰å®ç°çš„ç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        if self.rank == 0:
            print(f"ä½¿ç”¨ä¸²è¡Œæ¨¡å¼æ‰§è¡Œ {len(all_init_states)} ä¸ªepisodes")
        
        for i, init_state in enumerate(all_init_states):
            rollout_images = []
            
            # é‡ç½®ç¯å¢ƒ
            obs = env.reset()
            
            # çƒ­èº«æ­¥éª¤
            dummy_action = np.array([0, 0, 0, 0, 0, 0, -1])
            for warmup_step in range(20):
                obs, _, _, _ = env.step(dummy_action)
            
            # åˆå§‹åŒ–episodeæ•°æ®
            step = 0
            done = False
            total_reward = 0
            success = False
            observations = [obs]
            actions = []
            rewards = []
            dones = []
            infos = []
            
            # æ”¶é›†åˆå§‹å›¾åƒ
            if save_video:
                initial_img = obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                rollout_images.append(initial_img)
            
            try:
                action_buffer = None
                action_index = 0
                
                # æ‰§è¡Œepisode
                while not done and step < self.max_steps:
                    # æ¨ç†åŠ¨ä½œ
                    if action_buffer is None or action_index >= action_buffer.shape[0]:
                        pi0_observation = self.construct_pi0_observation(obs, env_name)
                        raw_action = self.policy.select_action(pi0_observation)
                        action = raw_action[0, :, :7]
                        
                        if isinstance(action, torch.Tensor):
                            action_after_cpu = action.cpu().numpy()
                        else:
                            action_after_cpu = action
                        
                        # åå½’ä¸€åŒ–å’ŒçŠ¶æ€åç§»
                        action_buffer = action_after_cpu * (self.action_std + 1e-6) + self.action_mean
                        
                        unnorm_state = np.concatenate([
                            obs["robot0_eef_pos"],
                            T.quat2axisangle(obs["robot0_eef_quat"]),
                            obs["robot0_gripper_qpos"],
                        ], dtype=np.float32)
                        
                        action_buffer[:, :6] += unnorm_state[None, :6]
                        action_index = 0
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    current_action = action_buffer[action_index, :7]
                    next_obs, reward, done, info = env.step(current_action)
                    
                    # è®°å½•æ•°æ®
                    observations.append(next_obs)
                    actions.append(current_action)
                    rewards.append(reward)
                    dones.append(done)
                    infos.append(info)
                    total_reward += reward
                    
                    # æ”¶é›†å›¾åƒ
                    if save_video:
                        frame_img = next_obs["agentview_image"][:, :, ::-1].transpose(1, 2, 0).copy()
                        rollout_images.append(frame_img)
                    
                    # æ›´æ–°çŠ¶æ€
                    obs = next_obs
                    step += 1
                    action_index += 1
                    
                    # æ£€æŸ¥æˆåŠŸ
                    if info.get("success", False) or total_reward > 0.5:
                        success = True
                    
                    if done:
                        break
            
            except Exception as e:
                if self.rank == 0:
                    print(f"ä¸²è¡Œepisodeæ‰§è¡Œå‡ºé”™: {e}")
            
            # ä¿å­˜è§†é¢‘
            if save_video and rollout_images:
                try:
                    video_dir = Path("pi0/ript/debug_images/videos")
                    video_dir.mkdir(parents=True, exist_ok=True)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    success_str = "success" if success else "failure"
                    video_path = video_dir / f"{timestamp}_episode_{i}_{success_str}.mp4"
                    
                    writer = imageio.get_writer(str(video_path), fps=30)
                    for frame in rollout_images:
                        writer.append_data(frame)
                    writer.close()
                    
                    if self.rank == 0:
                        print(f"âœ… å·²ä¿å­˜è§†é¢‘: {video_path}")
                except Exception as e:
                    if self.rank == 0:
                        print(f"ä¿å­˜è§†é¢‘å‡ºé”™: {e}")
            
            # è¿”å›ç»“æœ
            episode_data = {
                "observations": observations,
                "actions": actions,
                "rewards": rewards,
                "dones": dones,
                "infos": infos,
                "task": env_name,
            }
            
            yield (success, total_reward, episode_data)