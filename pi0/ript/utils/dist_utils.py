"""
åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·æ¨¡å—
åŸºäºåŸç‰ˆRIPTçš„åˆ†å¸ƒå¼æ¶æ„ï¼Œæä¾›å®Œæ•´çš„åˆ†å¸ƒå¼åè°ƒåŠŸèƒ½
"""

import os
import json
import pickle
import torch
import torch.distributed as dist
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import time
import fcntl
import uuid
from datetime import datetime

def is_distributed():
    """æ£€æŸ¥æ˜¯å¦åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿è¡Œ"""
    return dist.is_initialized()

def get_rank():
    """è·å–å½“å‰è¿›ç¨‹çš„rank"""
    return dist.get_rank() if dist.is_initialized() else 0

def get_world_size():
    """è·å–æ€»è¿›ç¨‹æ•°"""
    return dist.get_world_size() if dist.is_initialized() else 1

def barrier():
    """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
    if dist.is_initialized():
        dist.barrier()

def all_reduce_tensor(tensor, op=dist.ReduceOp.SUM):
    """åœ¨æ‰€æœ‰è¿›ç¨‹é—´èšåˆå¼ é‡"""
    if dist.is_initialized():
        dist.all_reduce(tensor, op=op)
    return tensor

def all_gather_tensors(tensor):
    """æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„å¼ é‡"""
    if not dist.is_initialized():
        return [tensor]
    
    world_size = dist.get_world_size()
    gathered_tensors = [torch.zeros_like(tensor) for _ in range(world_size)]
    dist.all_gather(gathered_tensors, tensor)
    return gathered_tensors

def all_gather_objects(obj):
    """æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„å¯¹è±¡"""
    if not dist.is_initialized():
        return [obj]
    
    world_size = dist.get_world_size()
    gathered_objects = [None] * world_size
    dist.all_gather_object(gathered_objects, obj)
    return gathered_objects

def broadcast_object(obj, src=0):
    """ä»æºè¿›ç¨‹å¹¿æ’­å¯¹è±¡åˆ°æ‰€æœ‰è¿›ç¨‹"""
    if not dist.is_initialized():
        return obj
    
    objects = [obj]
    dist.broadcast_object_list(objects, src=src)
    return objects[0]

class DistributedFileCounter:
    """
    åˆ†å¸ƒå¼æ–‡ä»¶è®¡æ•°å™¨ï¼Œç”¨äºè¿›ç¨‹é—´åè°ƒ
    åŸºäºRIPTçš„FileGlobalCounterå®ç°
    """
    
    def __init__(self, filename: str):
        self.filename = filename
        self.setup_counter()
    
    def setup_counter(self):
        """è®¾ç½®è®¡æ•°å™¨æ–‡ä»¶"""
        if get_rank() == 0:
            # ä¸»è¿›ç¨‹åˆ›å»ºæ–‡ä»¶
            if not os.path.exists(self.filename):
                with open(self.filename, "w") as f:
                    f.write("0")
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        barrier()
    
    def reset(self, value=0):
        """é‡ç½®è®¡æ•°å™¨"""
        if get_rank() == 0:
            with open(self.filename, "w") as f:
                f.write(str(value))
        barrier()
    
    def update(self, increment=1):
        """åŸå­æ€§æ›´æ–°è®¡æ•°å™¨"""
        with open(self.filename, "r+") as f:
            fcntl.flock(f, fcntl.LOCK_EX)
            try:
                content = f.read().strip()
                current = int(content) if content else 0
                current += increment
                f.seek(0)
                f.write(str(current))
                f.truncate()
            finally:
                fcntl.flock(f, fcntl.LOCK_UN)
            return current
    
    def get(self):
        """è·å–å½“å‰è®¡æ•°å€¼"""
        with open(self.filename, "r") as f:
            fcntl.flock(f, fcntl.LOCK_SH)
            try:
                content = f.read().strip()
                current = int(content) if content else 0
            finally:
                fcntl.flock(f, fcntl.LOCK_UN)
            return current
    
    def cleanup(self):
        """æ¸…ç†è®¡æ•°å™¨æ–‡ä»¶"""
        if get_rank() == 0 and os.path.exists(self.filename):
            try:
                os.remove(self.filename)
            except:
                pass

class DistributedStatsSynchronizer:
    """
    åˆ†å¸ƒå¼ç»Ÿè®¡æ•°æ®åŒæ­¥å™¨
    ç”¨äºåŒæ­¥é‡‡æ ·ç»Ÿè®¡ã€è®­ç»ƒæŒ‡æ ‡ç­‰æ•°æ®
    """
    
    def __init__(self, temp_dir: str = "/tmp/ript_sync"):
        self.temp_dir = Path(temp_dir)
        self.rank = get_rank()
        self.world_size = get_world_size()
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        if self.rank == 0:
            self.temp_dir.mkdir(parents=True, exist_ok=True)
        barrier()
    
    def sync_stats_dict(self, local_stats: Dict[str, Any]) -> Dict[str, Any]:
        """åŒæ­¥ç»Ÿè®¡å­—å…¸"""
        if not is_distributed():
            return local_stats
        
        # æ¯ä¸ªè¿›ç¨‹ä¿å­˜æœ¬åœ°ç»Ÿè®¡
        temp_file = self.temp_dir / f"stats_rank_{self.rank}.pkl"
        with open(temp_file, 'wb') as f:
            pickle.dump(local_stats, f)
        
        barrier()
        
        # ä¸»è¿›ç¨‹æ”¶é›†å’Œåˆå¹¶ç»Ÿè®¡
        if self.rank == 0:
            merged_stats = {}
            
            for r in range(self.world_size):
                rank_file = self.temp_dir / f"stats_rank_{r}.pkl"
                if rank_file.exists():
                    with open(rank_file, 'rb') as f:
                        rank_stats = pickle.load(f)
                    
                    # åˆå¹¶ç»Ÿè®¡æ•°æ®
                    for key, value in rank_stats.items():
                        if key not in merged_stats:
                            merged_stats[key] = []
                        
                        if isinstance(value, list):
                            merged_stats[key].extend(value)
                        else:
                            merged_stats[key].append(value)
            
            # ä¿å­˜åˆå¹¶åçš„ç»Ÿè®¡
            merged_file = self.temp_dir / "merged_stats.pkl"
            with open(merged_file, 'wb') as f:
                pickle.dump(merged_stats, f)
        
        barrier()
        
        # æ‰€æœ‰è¿›ç¨‹åŠ è½½åˆå¹¶åçš„ç»Ÿè®¡
        merged_file = self.temp_dir / "merged_stats.pkl"
        with open(merged_file, 'rb') as f:
            merged_stats = pickle.load(f)
        
        barrier()
        return merged_stats
    
    def sync_rollout_stats(self, local_rollout_stats: Dict[str, Dict]) -> Dict[str, Dict]:
        """åŒæ­¥rolloutç»Ÿè®¡æ•°æ®"""
        if not is_distributed():
            return local_rollout_stats
        
        # ä¿å­˜æœ¬åœ°rolloutç»Ÿè®¡
        temp_file = self.temp_dir / f"rollout_stats_rank_{self.rank}.pkl"
        with open(temp_file, 'wb') as f:
            pickle.dump(local_rollout_stats, f)
        
        barrier()
        
        # ä¸»è¿›ç¨‹åˆå¹¶rolloutç»Ÿè®¡
        if self.rank == 0:
            merged_rollout_stats = {}
            
            for r in range(self.world_size):
                rank_file = self.temp_dir / f"rollout_stats_rank_{r}.pkl"
                if rank_file.exists():
                    with open(rank_file, 'rb') as f:
                        rank_rollout_stats = pickle.load(f)
                    
                    # åˆå¹¶rolloutç»Ÿè®¡
                    for state_hash, state_info in rank_rollout_stats.items():
                        if state_hash not in merged_rollout_stats:
                            merged_rollout_stats[state_hash] = {
                                'attempts': 0,
                                'successes': [],
                                'last_success_rate': 0.0
                            }
                        
                        merged_rollout_stats[state_hash]['attempts'] += state_info.get('attempts', 0)
                        merged_rollout_stats[state_hash]['successes'].extend(state_info.get('successes', []))
                        
                        # é™åˆ¶å†å²è®°å½•é•¿åº¦
                        if len(merged_rollout_stats[state_hash]['successes']) > 50:
                            merged_rollout_stats[state_hash]['successes'] = merged_rollout_stats[state_hash]['successes'][-50:]
                        
                        # é‡æ–°è®¡ç®—æˆåŠŸç‡
                        if merged_rollout_stats[state_hash]['successes']:
                            merged_rollout_stats[state_hash]['last_success_rate'] = (
                                sum(merged_rollout_stats[state_hash]['successes']) / 
                                len(merged_rollout_stats[state_hash]['successes'])
                            )
            
            # ä¿å­˜åˆå¹¶åçš„rolloutç»Ÿè®¡
            merged_file = self.temp_dir / "merged_rollout_stats.pkl"
            with open(merged_file, 'wb') as f:
                pickle.dump(merged_rollout_stats, f)
        
        barrier()
        
        # æ‰€æœ‰è¿›ç¨‹åŠ è½½åˆå¹¶åçš„rolloutç»Ÿè®¡
        merged_file = self.temp_dir / "merged_rollout_stats.pkl"
        with open(merged_file, 'rb') as f:
            merged_rollout_stats = pickle.load(f)
        
        barrier()
        return merged_rollout_stats
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if self.rank == 0:
            import shutil
            try:
                shutil.rmtree(self.temp_dir)
            except:
                pass

def sync_rollout_results_via_file(rollout_results, logger, step, is_train=False):
    """
    é€šè¿‡æ–‡ä»¶åŒæ­¥rolloutç»“æœ
    åŸºäºRIPTçš„sync_rollout_results_via_fileå®ç°
    """
    if not is_distributed():
        # éåˆ†å¸ƒå¼ç¯å¢ƒç›´æ¥è®°å½•
        if logger and get_rank() == 0:
            prefix = "rl_train" if is_train else "rl_eval"
            for key, value in rollout_results.items():
                logger.log_metric(f"{prefix}/{key}", value, step)
        return rollout_results
    
    rank = get_rank()
    world_size = get_world_size()
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç›®å½•
    temp_dir = Path(f"/tmp/rollout_sync_{step}")
    if rank == 0:
        temp_dir.mkdir(parents=True, exist_ok=True)
    barrier()
    
    # æ¯ä¸ªè¿›ç¨‹ä¿å­˜ç»“æœ
    rank_file = temp_dir / f"rollout_results_rank_{rank}.json"
    with open(rank_file, 'w') as f:
        json.dump(rollout_results, f)
    
    barrier()
    
    # ä¸»è¿›ç¨‹æ”¶é›†å’Œèšåˆç»“æœ
    if rank == 0:
        all_results = {}
        task_results = {}
        
        for r in range(world_size):
            result_file = temp_dir / f"rollout_results_rank_{r}.json"
            if result_file.exists():
                with open(result_file, 'r') as f:
                    rank_results = json.load(f)
                
                # èšåˆç»“æœ
                for key, value in rank_results.items():
                    if key not in all_results:
                        all_results[key] = []
                    all_results[key].append(value)
        
        # å¹³å‡åŒ–ç»“æœ
        aggregated_results = {}
        for key, values in all_results.items():
            if all(isinstance(v, (int, float)) for v in values):
                aggregated_results[key] = np.mean(values)
            else:
                aggregated_results[key] = values
        
        # è®°å½•åˆ°logger
        if logger:
            prefix = "rl_train" if is_train else "rl_eval"
            for key, value in aggregated_results.items():
                if isinstance(value, (int, float)):
                    logger.log_metric(f"{prefix}/{key}", value, step)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        try:
            shutil.rmtree(temp_dir)
        except:
            pass
        
        return aggregated_results
    else:
        barrier()
        return rollout_results

def setup_distributed_counter(tmp_dir="/tmp", counter_name="global_counter"):
    """
    è®¾ç½®åˆ†å¸ƒå¼è®¡æ•°å™¨
    åŸºäºRIPTçš„setup_file_counterå®ç°
    """
    if is_distributed():
        rank = get_rank()
        if rank == 0:
            counter_filename = f"{tmp_dir}/{counter_name}_{uuid.uuid4().hex}.txt"
        else:
            counter_filename = ""
        
        filename_list = [counter_filename]
        dist.broadcast_object_list(filename_list, src=0)
        counter_filename = filename_list[0]
        
        file_counter = DistributedFileCounter(counter_filename)
        if rank == 0:
            file_counter.reset(0)
        barrier()
    else:
        counter_filename = f"{tmp_dir}/{counter_name}_{uuid.uuid4().hex}.txt"
        file_counter = DistributedFileCounter(counter_filename)
        file_counter.reset(0)
    
    return file_counter, counter_filename

def log_metrics_distributed(metrics: Dict[str, float], step: int, logger=None):
    """åˆ†å¸ƒå¼ç¯å¢ƒä¸‹è®°å½•æŒ‡æ ‡"""
    if not is_distributed():
        if logger:
            for key, value in metrics.items():
                logger.log_metric(key, value, step)
        return metrics
    
    rank = get_rank()
    
    # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æŒ‡æ ‡
    all_metrics = all_gather_objects(metrics)
    
    if rank == 0 and logger:
        # èšåˆæŒ‡æ ‡
        aggregated_metrics = {}
        
        for metric_dict in all_metrics:
            for key, value in metric_dict.items():
                if key not in aggregated_metrics:
                    aggregated_metrics[key] = []
                aggregated_metrics[key].append(value)
        
        # è®¡ç®—å¹³å‡å€¼å¹¶è®°å½•
        for key, values in aggregated_metrics.items():
            if all(isinstance(v, (int, float)) for v in values):
                avg_value = np.mean(values)
                logger.log_metric(f"distributed/{key}", avg_value, step)
                logger.log_metric(f"distributed/{key}_std", np.std(values), step)
    
    return metrics

def reduce_gradients(model, world_size):
    """æ‰‹åŠ¨èšåˆæ¢¯åº¦ï¼ˆç”¨äºéDDPæƒ…å†µï¼‰"""
    if not is_distributed():
        return
    
    for param in model.parameters():
        if param.grad is not None:
            dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)
            param.grad /= world_size

def save_checkpoint_distributed(model, optimizer, step, save_path, keep_latest=3):
    """åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ä¿å­˜æ£€æŸ¥ç‚¹"""
    if get_rank() != 0:
        return  # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜
    
    save_path = Path(save_path)
    save_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'step': step,
        'timestamp': datetime.now().isoformat(),
    }
    
    checkpoint_path = save_path / f"checkpoint_step_{step}.pth"
    torch.save(checkpoint, checkpoint_path)
    
    # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹é“¾æ¥
    latest_path = save_path / "checkpoint_latest.pth"
    if latest_path.exists():
        latest_path.unlink()
    latest_path.symlink_to(checkpoint_path.name)
    
    # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
    if keep_latest > 0:
        checkpoints = sorted(save_path.glob("checkpoint_step_*.pth"), key=os.path.getmtime)
        if len(checkpoints) > keep_latest:
            for old_checkpoint in checkpoints[:-keep_latest]:
                try:
                    old_checkpoint.unlink()
                except:
                    pass
    
    print(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

def load_checkpoint_distributed(model, optimizer, checkpoint_path, device):
    """åˆ†å¸ƒå¼ç¯å¢ƒä¸‹åŠ è½½æ£€æŸ¥ç‚¹"""
    checkpoint_path = Path(checkpoint_path)
    
    if not checkpoint_path.exists():
        if checkpoint_path.name == "checkpoint_latest.pth":
            # å°è¯•æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹
            parent_dir = checkpoint_path.parent
            checkpoints = sorted(parent_dir.glob("checkpoint_step_*.pth"), key=os.path.getmtime)
            if checkpoints:
                checkpoint_path = checkpoints[-1]
            else:
                raise FileNotFoundError(f"æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
        else:
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    print(f"ğŸ“¥ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    step = checkpoint.get('step', 0)
    print(f"âœ“ æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆï¼Œä»æ­¥éª¤ {step} ç»§ç»­")
    
    return step

class DistributedLogger:
    """åˆ†å¸ƒå¼æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir: str, rank: int = None):
        self.rank = rank if rank is not None else get_rank()
        self.world_size = get_world_size()
        self.log_dir = Path(log_dir)
        
        if self.rank == 0:
            self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¯ä¸ªè¿›ç¨‹æœ‰è‡ªå·±çš„æ—¥å¿—æ–‡ä»¶
        self.log_file = self.log_dir / f"train_rank_{self.rank}.log"
        
        barrier()
    
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [RANK {self.rank}] [{level}] {message}\n"
        
        with open(self.log_file, 'a') as f:
            f.write(log_entry)
        
        # ä¸»è¿›ç¨‹ä¹Ÿæ‰“å°åˆ°æ§åˆ¶å°
        if self.rank == 0:
            print(f"[{level}] {message}")
    
    def log_metrics(self, metrics: Dict[str, float], step: int):
        """è®°å½•æŒ‡æ ‡"""
        metrics_str = ", ".join([f"{k}={v:.4f}" for k, v in metrics.items()])
        self.log(f"Step {step}: {metrics_str}")