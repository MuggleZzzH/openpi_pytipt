import torch
from torch import nn
from typing import Any, Dict, List, Optional, Tuple, Union
import numpy as np
import torch.nn.functional as F
import json
from pathlib import Path
from pi0.modeling_pi0 import PI0Policy
# from lerobot.common.utils.utils import get_safe_dtype  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œä½¿ç”¨æœ¬åœ°å®ç°

# Assuming the base interface is in a shared location
from pi0.ript.algos.rl_optimizers.model_interface import RLModelInterface

# æœ¬åœ°å®ç°get_safe_dtypeå‡½æ•°
def get_safe_dtype(tensor):
    """Get safe dtype for tensor operations"""
    if tensor.dtype in [torch.float16, torch.bfloat16]:
        return torch.float32
    return tensor.dtype


class PI0_CFG_Adapter(RLModelInterface):
    """
    Adapter for PI0Policy to work with the RIPT framework using an
    advantage-weighted loss, inspired by Classifier-Free Guidance.

    This adapter computes a weighted L2 loss from the policy's forward pass,
    which serves as a proxy for policy gradient updates, bypassing the need
    for explicit log_prob calculations or a value network.
    """

    def __init__(self, policy: PI0Policy, norm_stats_path: Optional[str] = None, **kwargs):
        """
        Initialize the adapter with a PI0Policy instance.
        Args:
            policy: An instance of PI0Policy.
            norm_stats_path: Path to norm_stats.json file for normalization. 
                           If None, will try to find it automatically.
        """
        # The base model is the PI0Policy itself.
        super().__init__(model=policy, **kwargs)
        self.policy = policy
        
        # è§†é¢‘æ”¶é›†ç›¸å…³
        self.video_frames = {}  # {episode_idx: [frames]}
        self.video_save_enabled = True
        
        # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
        self._load_norm_stats(norm_stats_path)
        
    def _load_norm_stats(self, norm_stats_path: Optional[str] = None):
        """Load normalization statistics from norm_stats.json"""
        if norm_stats_path is None:
            # å°è¯•åœ¨å¸¸è§ä½ç½®æ‰¾åˆ°norm_stats.json
            possible_paths = [
                "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch/norm_stats.json",
                "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json", 
                "./norm_stats.json"
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    norm_stats_path = path
                    break
        
        if norm_stats_path and Path(norm_stats_path).exists():
            print(f"Loading norm_stats from: {norm_stats_path}")
            with open(norm_stats_path) as f:
                norm_stats = json.load(f)
                
            # æå–çŠ¶æ€å’ŒåŠ¨ä½œçš„å½’ä¸€åŒ–å‚æ•°
            self.state_mean = np.array(norm_stats["norm_stats"]["state"]["mean"][:8], dtype=np.float32)
            self.state_std = np.array(norm_stats["norm_stats"]["state"]["std"][:8], dtype=np.float32)
            self.action_mean = np.array(norm_stats["norm_stats"]["actions"]["mean"][:7], dtype=np.float32)
            self.action_std = np.array(norm_stats["norm_stats"]["actions"]["std"][:7], dtype=np.float32)
            
            print(f"âœ… Loaded normalization stats:")
            print(f"  State mean shape: {self.state_mean.shape}")
            print(f"  State std shape: {self.state_std.shape}")
            print(f"  Action mean shape: {self.action_mean.shape}")
            print(f"  Action std shape: {self.action_std.shape}")
        else:
            print("âš ï¸  Warning: No norm_stats.json found, using identity normalization")
            # ä½¿ç”¨å•ä½å½’ä¸€åŒ–ï¼ˆä¸è¿›è¡Œå½’ä¸€åŒ–ï¼‰
            self.state_mean = np.zeros(8, dtype=np.float32)
            self.state_std = np.ones(8, dtype=np.float32)
            self.action_mean = np.zeros(7, dtype=np.float32)
            self.action_std = np.ones(7, dtype=np.float32)
    
    def normalize_state(self, state: np.ndarray) -> np.ndarray:
        """Normalize state using loaded statistics"""
        return (state - self.state_mean[:len(state)]) / (self.state_std[:len(state)] + 1e-6)
    
    def denormalize_action(self, action: np.ndarray) -> np.ndarray:
        """Denormalize action using loaded statistics"""
        return action * (self.action_std + 1e-6) + self.action_mean

    def process_episodes(
        self,
        episodes: List[Dict[str, Any]],
        device: Optional[torch.device] = None,
    ) -> Dict[str, Any]:
        """
        å°† episodes æ‰“åŒ…ä¸º PI0Policy æœŸæœ›çš„ batchï¼š
        - action å½¢çŠ¶ä¿æŒ (B, T, 7)ï¼Œä¸åœ¨è¿™é‡Œå¡«åˆ° 32 ç»´ï¼›
        - æä¾› action_is_pad: (B, T) å¸ƒå°”ï¼ŒTrue è¡¨ç¤ºè¯¥æ—¶é—´æ­¥æ˜¯ paddingï¼›
        - çŠ¶æ€ç»Ÿä¸€ 8 ç»´ (3 pos + 3 axis-angle + 2 gripper)ï¼›
        """
        if device is None:
            device = self.device
        if not episodes:
            raise ValueError("Empty episodes list provided")

        all_states, all_images, all_actions = [], [], []
        all_action_is_pad, all_tasks = [], []

        target_seq_len = getattr(self.policy.config, 'n_action_steps', 50)
        diffusion_steps = getattr(self.policy.config, 'num_steps', 10)
        print(f"Using target_seq_len={target_seq_len} (n_action_steps), diffusion_steps={diffusion_steps}")

        for i, ep in enumerate(episodes):
            try:
                observations = ep['observations']
                actions = ep['actions']
                tasks = ep.get('task', "default task")
                if not observations:
                    raise ValueError(f"Episode {i} empty observations")

                # æ”¶é›†
                states_seq, images_seq, actions_seq = [], [], []
                max_steps = min(len(observations), len(actions))
                sample_idx = list(range(max_steps))  # å…¨é‡ä¿å­˜ï¼Œä¾¿äºè§†é¢‘

                for t in sample_idx:
                    obs_t = observations[t] if t < len(observations) else {}
                    act_t = actions[t] if t < len(actions) else np.zeros(7, np.float32)

                    # çŠ¶æ€(8ç»´)
                    states_seq.append(self._extract_state_from_obs(obs_t, i, t))
                    # å›¾åƒ(HWC)
                    images_seq.append(self._extract_image_from_obs(obs_t, i, t))

                    # åŠ¨ä½œ(7ç»´)
                    act_t = np.array(act_t[0] if (isinstance(act_t, list) and len(act_t) > 0) else act_t,
                                    dtype=np.float32)
                    if act_t.size != 7:
                        buf = np.zeros(7, dtype=np.float32)
                        buf[:min(7, act_t.size)] = act_t[:min(7, act_t.size)]
                        act_t = buf
                    actions_seq.append(act_t)

                # å›ºå®šé•¿åº¦ + æ©ç 
                valid_len = min(len(actions_seq), target_seq_len)
                final_action = np.zeros((target_seq_len, 7), dtype=np.float32)
                if valid_len > 0:
                    final_action[:valid_len] = np.asarray(actions_seq[-valid_len:], dtype=np.float32)

                action_is_pad = np.ones((target_seq_len,), dtype=bool)
                action_is_pad[:valid_len] = False

                final_state = np.asarray(states_seq[-1], dtype=np.float32) if states_seq else np.zeros(8, np.float32)
                final_image = images_seq[-1] if images_seq else (np.ones((224, 224, 3), np.uint8) * 128)

                all_states.append(final_state)
                all_images.append(final_image)
                all_actions.append(final_action)
                all_action_is_pad.append(action_is_pad)

                task_str = tasks[0] if isinstance(tasks, list) else str(tasks)
                all_tasks.append(task_str)

            except Exception as e:
                print(f"Error processing episode {i}: {e}")
                all_states.append(np.zeros(8, np.float32))
                all_images.append(np.ones((224, 224, 3), np.uint8) * 128)
                all_actions.append(np.zeros((target_seq_len, 7), np.float32))
                all_action_is_pad.append(np.ones((target_seq_len,), dtype=bool))
                all_tasks.append("default task")

        batch = {
            "state": torch.from_numpy(np.stack(all_states)).to(device, dtype=torch.float32),            # (B,8)
            "image": {"base_0_rgb": torch.from_numpy(np.stack(all_images)).to(device, dtype=torch.uint8)},  # (B,H,W,3)
            "action": torch.from_numpy(np.stack(all_actions)).to(device, dtype=torch.float32),          # (B,T,7)
            "action_is_pad": torch.from_numpy(np.stack(all_action_is_pad)).to(device),                  # (B,T) bool
            "prompt": all_tasks,
        }

        bs = len(episodes)
        assert batch["state"].shape[0] == bs
        assert batch["image"]["base_0_rgb"].shape[0] == bs
        assert batch["action"].shape[0] == bs
        assert batch["action_is_pad"].shape[0] == bs
        print(f"Processed batch: state {batch['state'].shape}, image {batch['image']['base_0_rgb'].shape}, "
            f"action {batch['action'].shape}, action_is_pad {batch['action_is_pad'].shape}")
        return batch
    def compute_weighted_loss(
        self,
        episodes: List[Dict[str, Any]],
        advantages: torch.Tensor,
        device: Optional[torch.device] = None,
    ) -> torch.Tensor:
        """
        CFGé£æ ¼è®­ç»ƒï¼šåŒæ—¶è®¡ç®—æ¡ä»¶å’Œæ— æ¡ä»¶æŸå¤±
        
        ä¸¥æ ¼å¥‘çº¦æ¨¡å¼ï¼šæ‰€æœ‰è¾“å…¥/è¾“å‡ºå¿…é¡»æ»¡è¶³é¢„å®šä¹‰å¥‘çº¦ï¼Œè¿ååˆ™ç«‹å³å¤±è´¥
        
        è¾“å…¥å¥‘çº¦:
        - episodes: éç©ºlistï¼Œæ¯ä¸ªepisodeåŒ…å«å¿…éœ€å­—æ®µ
        - advantages: (B,) tensorï¼ŒBä¸episodesæ•°é‡åŒ¹é…
        
        è¾“å‡ºå¥‘çº¦:
        - policy.forward()å¿…é¡»è¿”å›åŒ…å«"losses"å­—æ®µçš„dictï¼Œshapeä¸º(B,T,D)
        - action_is_padå¿…é¡»ä¸º(B,T) bool tensorï¼Œè‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆæ­¥
        """
        if device is None:
            device = self.device

        # === ä¸¥æ ¼è¾“å…¥å¥‘çº¦éªŒè¯ ===
        assert episodes and len(episodes) > 0, "episodesä¸èƒ½ä¸ºç©º"
        assert advantages is not None and len(advantages) > 0, "advantagesä¸èƒ½ä¸ºç©º"
        assert len(episodes) == len(advantages), f"episodesæ•°é‡({len(episodes)})å¿…é¡»ä¸advantagesæ•°é‡({len(advantages)})åŒ¹é…"
        assert advantages.dim() == 1, f"advantageså¿…é¡»æ˜¯1ç»´tensorï¼Œå½“å‰ç»´åº¦: {advantages.dim()}"
        assert isinstance(advantages, torch.Tensor), f"advantageså¿…é¡»æ˜¯torch.Tensorç±»å‹ï¼Œå½“å‰ç±»å‹: {type(advantages)}"

        batch = self.process_episodes(episodes, device)
        
        # === æ‰¹æ¬¡å¥‘çº¦éªŒè¯ ===
        assert "action_is_pad" in batch, "batchä¸­å¿…é¡»åŒ…å«action_is_padå­—æ®µ"
        action_is_pad = batch["action_is_pad"]
        assert action_is_pad.dtype == torch.bool, f"action_is_padå¿…é¡»æ˜¯boolç±»å‹ï¼Œå½“å‰ç±»å‹: {action_is_pad.dtype}"
        assert action_is_pad.dim() == 2, f"action_is_padå¿…é¡»æ˜¯2ç»´tensor (B,T)ï¼Œå½“å‰ç»´åº¦: {action_is_pad.dim()}"
        assert action_is_pad.shape[0] == len(episodes), f"action_is_padæ‰¹æ¬¡ç»´åº¦({action_is_pad.shape[0]})å¿…é¡»ä¸episodesæ•°é‡({len(episodes)})åŒ¹é…"
        
        # éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆæ­¥
        valid_steps = (~action_is_pad).sum()
        assert valid_steps > 0, "action_is_padæ˜¾ç¤ºæ‰€æœ‰æ­¥éª¤éƒ½æ˜¯paddingï¼Œå¿…é¡»è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆæ­¥éª¤"
        
        # === å…³é”®ä¿®å¤ï¼šç»Ÿä¸€é‡‡æ ·noiseå’Œtimeç”¨äºCFGåŒåˆ†æ”¯ ===
        B = batch["state"].shape[0]
        n, d = self.policy.config.n_action_steps, self.policy.config.max_action_dim
        dtype = batch["state"].dtype
        
        # é‡‡æ ·ä¸€æ¬¡noiseå’Œtimeï¼Œä¸¤ä¸ªåˆ†æ”¯å…±äº«ï¼ˆä¸æœ€åˆCFGRLå®ç°å¯¹é½ï¼‰
        noise = torch.randn(B, n, d, device=device, dtype=dtype)
        time = self.policy.model.sample_time(B, device).to(dtype)
        
        # === CFGé£æ ¼åŒåˆ†æ”¯æŸå¤±è®¡ç®— ===
        
        # 1. æ¡ä»¶åˆ†æ”¯ï¼ˆæ­£æ ·æœ¬æŒ‡ç¤ºï¼‰- ä½¿ç”¨å…±äº«çš„noiseå’Œtime
        batch_positive = batch.copy()
        batch_positive["is_positive"] = torch.ones(B, device=device, dtype=torch.long)
        batch_positive["noise"] = noise
        batch_positive["time"] = time
        
        out_positive = self.policy.forward(batch_positive)
        
        # === ä¸¥æ ¼è¾“å‡ºå¥‘çº¦éªŒè¯ - æ¡ä»¶åˆ†æ”¯ ===
        assert isinstance(out_positive, (dict, tuple)), f"policy.forward()å¿…é¡»è¿”å›dictæˆ–tupleï¼Œæ¡ä»¶åˆ†æ”¯è¿”å›ç±»å‹: {type(out_positive)}"
        
        if isinstance(out_positive, tuple):
            assert len(out_positive) >= 2, f"tupleè¿”å›å€¼å¿…é¡»è‡³å°‘åŒ…å«2ä¸ªå…ƒç´ ï¼Œå½“å‰é•¿åº¦: {len(out_positive)}"
            loss_scalar_pos, loss_dict_pos = out_positive[0], out_positive[1]
        else:
            loss_dict_pos = out_positive
            loss_scalar_pos = out_positive.get("loss")
        
        assert isinstance(loss_dict_pos, dict), f"loss_dictå¿…é¡»æ˜¯dictç±»å‹ï¼Œæ¡ä»¶åˆ†æ”¯ç±»å‹: {type(loss_dict_pos)}"
        assert "losses" in loss_dict_pos, "policy forwardè¾“å‡ºå¿…é¡»åŒ…å«'losses'å­—æ®µ"
        
        per_step_per_dim_pos = loss_dict_pos["losses"]
        assert isinstance(per_step_per_dim_pos, torch.Tensor), f"losseså¿…é¡»æ˜¯torch.Tensorï¼Œæ¡ä»¶åˆ†æ”¯ç±»å‹: {type(per_step_per_dim_pos)}"
        assert per_step_per_dim_pos.dim() == 3, f"losseså¿…é¡»æ˜¯3ç»´tensor (B,T,D)ï¼Œæ¡ä»¶åˆ†æ”¯ç»´åº¦: {per_step_per_dim_pos.dim()}"
        assert per_step_per_dim_pos.shape[0] == len(episodes), f"lossesæ‰¹æ¬¡ç»´åº¦({per_step_per_dim_pos.shape[0]})å¿…é¡»ä¸episodesæ•°é‡({len(episodes)})åŒ¹é…"
        assert per_step_per_dim_pos.shape[1] == action_is_pad.shape[1], f"lossesæ—¶é—´ç»´åº¦({per_step_per_dim_pos.shape[1]})å¿…é¡»ä¸action_is_padæ—¶é—´ç»´åº¦({action_is_pad.shape[1]})åŒ¹é…"

        # 2. æ— æ¡ä»¶åˆ†æ”¯ï¼ˆæ— æŒ‡ç¤ºï¼‰- ä½¿ç”¨ç›¸åŒçš„noiseå’Œtime
        batch_uncond = batch.copy()
        batch_uncond["is_positive"] = torch.zeros(B, device=device, dtype=torch.long)
        batch_uncond["noise"] = noise  # å…³é”®ï¼šä¸æ¡ä»¶åˆ†æ”¯å…±äº«ç›¸åŒçš„noise
        batch_uncond["time"] = time    # å…³é”®ï¼šä¸æ¡ä»¶åˆ†æ”¯å…±äº«ç›¸åŒçš„time
        
        out_uncond = self.policy.forward(batch_uncond)
        
        # === ä¸¥æ ¼è¾“å‡ºå¥‘çº¦éªŒè¯ - æ— æ¡ä»¶åˆ†æ”¯ ===
        assert isinstance(out_uncond, (dict, tuple)), f"policy.forward()å¿…é¡»è¿”å›dictæˆ–tupleï¼Œæ— æ¡ä»¶åˆ†æ”¯è¿”å›ç±»å‹: {type(out_uncond)}"
        
        if isinstance(out_uncond, tuple):
            assert len(out_uncond) >= 2, f"tupleè¿”å›å€¼å¿…é¡»è‡³å°‘åŒ…å«2ä¸ªå…ƒç´ ï¼Œå½“å‰é•¿åº¦: {len(out_uncond)}"
            loss_scalar_uncond, loss_dict_uncond = out_uncond[0], out_uncond[1]
        else:
            loss_dict_uncond = out_uncond
            loss_scalar_uncond = out_uncond.get("loss")
        
        assert isinstance(loss_dict_uncond, dict), f"loss_dictå¿…é¡»æ˜¯dictç±»å‹ï¼Œæ— æ¡ä»¶åˆ†æ”¯ç±»å‹: {type(loss_dict_uncond)}"
        assert "losses" in loss_dict_uncond, "policy forwardè¾“å‡ºå¿…é¡»åŒ…å«'losses'å­—æ®µ"
        
        per_step_per_dim_uncond = loss_dict_uncond["losses"]
        assert isinstance(per_step_per_dim_uncond, torch.Tensor), f"losseså¿…é¡»æ˜¯torch.Tensorï¼Œæ— æ¡ä»¶åˆ†æ”¯ç±»å‹: {type(per_step_per_dim_uncond)}"
        assert per_step_per_dim_uncond.dim() == 3, f"losseså¿…é¡»æ˜¯3ç»´tensor (B,T,D)ï¼Œæ— æ¡ä»¶åˆ†æ”¯ç»´åº¦: {per_step_per_dim_uncond.dim()}"
        assert per_step_per_dim_uncond.shape == per_step_per_dim_pos.shape, f"æ— æ¡ä»¶åˆ†æ”¯losseså½¢çŠ¶({per_step_per_dim_uncond.shape})å¿…é¡»ä¸æ¡ä»¶åˆ†æ”¯({per_step_per_dim_pos.shape})å®Œå…¨åŒ¹é…"

        # 3. è®¡ç®—CFGç»„åˆæŸå¤±
        per_step_pos = per_step_per_dim_pos.mean(dim=-1)  # (B,T)
        per_step_uncond = per_step_per_dim_uncond.mean(dim=-1)  # (B,T)
        
        # è·å–æœ‰æ•ˆæ­¥æ©ç ï¼Œæ’é™¤paddingæ­¥
        mask = (~action_is_pad).float()  # (B,T)
        
        # CFGæƒé‡è®¡ç®—ï¼šäºŒå€¼æƒé‡
        w = (advantages.to(device).float() > 0).float().unsqueeze(1).expand_as(mask)  # (B,T)
        
        # æ ·æœ¬çº§ç»„åˆåç»Ÿä¸€å¹³å‡ï¼ˆå…³é”®ä¿®å¤ï¼šç»Ÿä¸€åˆ†æ¯ï¼‰
        cfg_alpha = getattr(self.policy.config, 'cfg_uncond_weight', 0.1)
        combined = per_step_pos * w + cfg_alpha * per_step_uncond
        
        # === æœ€ç»ˆç»“æœéªŒè¯ ===
        mask_sum = mask.sum()
        assert mask_sum > 0, "æ‰€æœ‰æ­¥éª¤éƒ½è¢«maskæ‰ï¼Œæ— æ³•è®¡ç®—æœ‰æ•ˆæŸå¤±"
        
        final_loss = (combined * mask).sum() / mask_sum
        
        assert torch.isfinite(final_loss), f"CFGæŸå¤±è®¡ç®—ç»“æœå¿…é¡»æ˜¯æœ‰é™æ•°å€¼ï¼Œå½“å‰å€¼: {final_loss}"
        assert not torch.isnan(final_loss), "CFGæŸå¤±è®¡ç®—ç»“æœä¸èƒ½æ˜¯NaN"
        assert not torch.isinf(final_loss), "CFGæŸå¤±è®¡ç®—ç»“æœä¸èƒ½æ˜¯Inf"
        
        return final_loss
    
    def compute_act_logits(self, model, episodes: List[Dict[str, Any]], device: Optional[torch.device] = None):
        """
        This method is required by the RLModelInterface, but for our CFG-style
        optimizer, we use `compute_weighted_loss` instead.
        We can raise an error or return a placeholder.
        """
        raise NotImplementedError(
            "compute_act_logits is not used for PI0_CFG_Adapter. "
            "Use compute_weighted_loss instead."
        )

    @property
    def device(self):
        return next(self.policy.parameters()).device

    def get_policy_model(self):
        """Return the policy model, which is the PI0Policy instance itself."""
        return self.policy
        
    def _extract_state_from_obs(self, obs, episode_idx, step_idx):
        """æå– 8 ç»´çŠ¶æ€: 3 pos + 3 axis-angle + 2 gripperï¼Œå¹¶æŒ‰ norm_stats å½’ä¸€åŒ–"""
        try:
            if isinstance(obs, np.ndarray) and obs.dtype == object:
                obs = (obs.item() if obs.size == 1 else obs[0]) if obs.size > 0 else {}
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]
            if not isinstance(obs, dict) or not obs:
                return np.zeros(8, np.float32)

            if "robot0_eef_pos" in obs and "robot0_eef_quat" in obs:
                eef_pos = np.array(obs["robot0_eef_pos"], dtype=np.float32)
                eef_quat = np.array(obs["robot0_eef_quat"], dtype=np.float32)
                if eef_pos.size != 3: eef_pos = np.zeros(3, np.float32)
                if eef_quat.size != 4: eef_quat = np.array([0,0,0,1], np.float32)
                try:
                    from pi0.ript.utils.pi0_libero_utils import quat2axisangle
                    axis_angle = quat2axisangle(eef_quat).astype(np.float32)
                except Exception:
                    axis_angle = np.zeros(3, np.float32)

                gr = obs.get("robot0_gripper_qpos", [0.0, 0.0])
                gr = np.array(gr, dtype=np.float32)
                if gr.size < 2:
                    gr = np.pad(gr, (0, 2 - gr.size))
                else:
                    gr = gr[:2]

                unnorm = np.concatenate([eef_pos[:3], axis_angle[:3], gr[:2]], dtype=np.float32)  # (8,)
                state = self.normalize_state(unnorm)

            else:
                # æ— å…³é”®å­—æ®µåˆ™å›é€€ä¸º 0ï¼ˆä¸ç»Ÿè®¡ç›¸å®¹ï¼‰
                state = np.zeros(8, np.float32)

            return np.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0).astype(np.float32)

        except Exception as e:
            print(f"æå–çŠ¶æ€æ—¶å‡ºé”™ (episode {episode_idx}, step {step_idx}): {e}")
            return np.zeros(8, np.float32)
    
    def _extract_image_from_obs(self, obs, episode_idx, step_idx):
        """ä»è§‚æµ‹ä¸­æå–å›¾åƒä¿¡æ¯"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†SubprocVectorEnvè¿”å›çš„numpy.arrayåŒ…è£…çš„è§‚æµ‹
            if isinstance(obs, np.ndarray) and obs.dtype == object:
                if obs.size == 1:
                    obs = obs.item()  # æå–å•ä¸ªå…ƒç´ 
                elif obs.size > 0:
                    obs = obs[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            
            if isinstance(obs, dict) and "agentview_image" in obs:
                img = obs["agentview_image"]
                if isinstance(img, np.ndarray) and img.size > 0:
                    # ğŸ”§ ä¿®å¤å›¾åƒæ ¼å¼æ£€æŸ¥ï¼šå¤„ç†CHWå’ŒHWCä¸¤ç§æ ¼å¼
                    if img.ndim == 3:
                        # æ£€æŸ¥æ˜¯CHWæ ¼å¼ (3, H, W) è¿˜æ˜¯HWCæ ¼å¼ (H, W, 3)
                        if img.shape[0] == 3 and img.shape[-1] != 3:  # CHWæ ¼å¼
                            # è½¬æ¢CHW â†’ HWC
                            img = img.transpose(1, 2, 0)
                        elif img.shape[-1] != 3:  # æ—¢ä¸æ˜¯CHWä¹Ÿä¸æ˜¯HWC
                            print(f"âœ— æœªçŸ¥å›¾åƒæ ¼å¼: {img.shape}")
                            raise ValueError(f"Unexpected image format: {img.shape}")
                        
                        # ç°åœ¨imgåº”è¯¥æ˜¯HWCæ ¼å¼
                        # é™é»˜å¤„ç†å›¾åƒï¼ˆå‡å°‘å†—ä½™æ—¥å¿—ï¼‰
                        
                        # ä¸ºæ¨¡å‹æ¨ç†å‡†å¤‡å›¾åƒï¼ˆç¡®ä¿æ¥æ”¶æ­£ç¡®æ–¹å‘çš„å›¾åƒï¼‰
                        # æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œç¡®ä¿æ¨¡å‹æ¥æ”¶çš„æ˜¯æ­£ç€çš„å›¾åƒ
                        model_img = img.copy()  # ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼Œä¸è¿›è¡Œæ—‹è½¬
                        
                        # æ”¶é›†å›¾åƒç”¨äºè§†é¢‘ç”Ÿæˆï¼ˆéœ€è¦æ—‹è½¬ä»¥æ˜¾ç¤ºæ­£ç¡®æ–¹å‘ï¼‰
                        video_img = img[::-1, ::-1]  # 180åº¦æ—‹è½¬ç”¨äºè§†é¢‘æ˜¾ç¤º
                        self._collect_image_for_video(video_img, episode_idx, step_idx)
                        
                        # è¿”å›ç”¨äºæ¨¡å‹æ¨ç†çš„å›¾åƒï¼ˆå·²è¿›è¡Œ180åº¦æ—‹è½¬ï¼‰
                        # å¦‚æœå›¾åƒå°ºå¯¸ä¸æ˜¯ 224x224ï¼Œéœ€è¦è°ƒæ•´å¤§å°
                        if model_img.shape[:2] != (224, 224):
                            try:
                                from skimage.transform import resize
                                model_img = resize(model_img, (224, 224), preserve_range=True).astype(np.uint8)
                            except ImportError:
                                # å¦‚æœæ²¡æœ‰ skimageï¼Œä½¿ç”¨ç®€å•çš„æ’å€¼
                                import cv2
                                model_img = cv2.resize(model_img, (224, 224))
                        return model_img
                    else:
                        print(f"âœ— å›¾åƒç»´åº¦å¼‚å¸¸: {img.shape}")
                else:
                    print(f"âœ— å›¾åƒæ•°æ®æ— æ•ˆ: type={type(img)}, size={getattr(img, 'size', 'N/A')}")
            else:
                available_keys = list(obs.keys()) if isinstance(obs, dict) else "éå­—å…¸ç±»å‹"
                print(f"âœ— è§‚æµ‹ä¸­æ— agentview_imageé”®, å¯ç”¨é”®: {available_keys}")
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå›¾åƒï¼Œè¿”å›å ä½ç¬¦
            print(f"âœ— ä½¿ç”¨å ä½ç¬¦å›¾åƒ (episode {episode_idx}, step {step_idx})")
            return np.ones((224, 224, 3), dtype=np.uint8) * 128
            
        except Exception as e:
            print(f"æå–å›¾åƒæ—¶å‡ºé”™ (episode {episode_idx}, step {step_idx}): {e}")
            return np.ones((224, 224, 3), dtype=np.uint8) * 128

    def _collect_image_for_video(self, img, episode_idx, step_idx):
        """æ”¶é›†å›¾åƒç”¨äºè§†é¢‘ç”Ÿæˆ"""
        if not self.video_save_enabled:
            return
            
        try:
            if episode_idx not in self.video_frames:
                self.video_frames[episode_idx] = []
            
            # ä¿å­˜åŸå§‹æ–¹å‘çš„å›¾åƒç”¨äºè§†é¢‘
            self.video_frames[episode_idx].append(img.copy())
            
        except Exception as e:
            print(f"æ”¶é›†è§†é¢‘å¸§æ—¶å‡ºé”™: {e}")

    def finalize_episode_video(self, episode_idx, task_description="default_task"):
        """å®Œæˆepisodeæ—¶ç”Ÿæˆè§†é¢‘"""
        if not self.video_save_enabled or episode_idx not in self.video_frames:
            return None
            
        frames = self.video_frames[episode_idx]
        if len(frames) > 0:
            video_path = self._generate_episode_video(frames, episode_idx, task_description)
            # æ¸…ç†å·²ä½¿ç”¨çš„å¸§
            del self.video_frames[episode_idx]
            return video_path
        return None

    def _generate_episode_video(self, episode_images, episode_idx, task_description):
        """ä»episodeå›¾åƒç”Ÿæˆè§†é¢‘"""
        try:
            import os
            import imageio
            from datetime import datetime
            
            # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
            debug_dir = os.path.join(os.getcwd(), "ript", "debug_images")
            video_dir = os.path.join(debug_dir, "videos")
            os.makedirs(video_dir, exist_ok=True)
            
            # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            task_str = str(task_description).replace(" ", "_").replace("/", "_")[:30]
            video_path = os.path.join(video_dir, f"episode_{episode_idx}_{timestamp}_{task_str}.mp4")
            
            # ç¡®ä¿æ‰€æœ‰å›¾åƒå°ºå¯¸ä¸€è‡´
            processed_images = []
            for img in episode_images:
                if isinstance(img, np.ndarray) and img.size > 0:
                    if img.ndim == 3 and img.shape[-1] == 3:
                        # è°ƒæ•´åˆ°ç»Ÿä¸€å°ºå¯¸
                        if img.shape[:2] != (224, 224):
                            try:
                                from skimage.transform import resize
                                img = resize(img, (224, 224), preserve_range=True).astype(np.uint8)
                            except ImportError:
                                import cv2
                                img = cv2.resize(img, (224, 224))
                        
                        # å›¾åƒå·²ç»åœ¨pi0_libero_utils.pyä¸­å¤„ç†è¿‡180åº¦æ—‹è½¬
                        # ä½†æ˜¯ä¸ºäº†è§†é¢‘æ˜¾ç¤ºæ­£ç¡®ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨åŸå§‹æ–¹å‘çš„å›¾åƒ
                        processed_images.append(img)  # ä½¿ç”¨åŸå§‹æ–¹å‘çš„å›¾åƒ
            
            if len(processed_images) >= 5:  # è‡³å°‘5å¸§æ‰ç”Ÿæˆè§†é¢‘
                # ä¿å­˜è§†é¢‘
                imageio.mimsave(video_path, processed_images, fps=10)
                print(f"âœ“ Episodeè§†é¢‘å·²ç”Ÿæˆ: {video_path} ({len(processed_images)} å¸§)")
                return video_path
            else:
                print(f"âš ï¸ Episode {episode_idx} å›¾åƒæ•°é‡ä¸è¶³ï¼Œè·³è¿‡è§†é¢‘ç”Ÿæˆ ({len(processed_images)} å¸§)")
                return None
                
        except Exception as e:
            print(f"ç”Ÿæˆepisodeè§†é¢‘æ—¶å‡ºé”™: {e}")
            return None