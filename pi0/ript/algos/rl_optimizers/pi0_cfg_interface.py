import torch
from torch import nn
from typing import Any, Dict, List, Optional, Tuple, Union
import numpy as np
import torch.nn.functional as F
import json
from pathlib import Path
from pi0.modeling_pi0 import PI0Policy
# from lerobot.common.utils.utils import get_safe_dtype  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œä½¿ç”¨æœ¬åœ°å®ç°

# Assuming the base interface is in a shared location
from pi0.ript.algos.rl_optimizers.model_interface import RLModelInterface

# ğŸ”¥ Phase 2: Import new SO100-style data processing classes
from pi0.ript.data import SO100StyleProcessor, TrajectoryToSampleGenerator

# æœ¬åœ°å®ç°get_safe_dtypeå‡½æ•°
def get_safe_dtype(tensor):
    """Get safe dtype for tensor operations"""
    if tensor.dtype in [torch.float16, torch.bfloat16]:
        return torch.float32
    return tensor.dtype


class PI0_CFG_Adapter(RLModelInterface):
    """
    Adapter for PI0Policy to work with the RIPT framework using an
    advantage-weighted loss, inspired by Classifier-Free Guidance.

    This adapter computes a weighted L2 loss from the policy's forward pass,
    which serves as a proxy for policy gradient updates, bypassing the need
    for explicit log_prob calculations or a value network.
    """

    def __init__(self, policy: PI0Policy, norm_stats_path: Optional[str] = None,
                 windowing_mode: str = 'last', window_stride: int = 10,
                 max_windows_per_episode: int = 1,
                 use_so100_processing: bool = False, **kwargs):
        """
        Initialize the adapter with a PI0Policy instance.
        Args:
            policy: An instance of PI0Policy.
            norm_stats_path: Path to norm_stats.json file for normalization.
                           If None, will try to find it automatically.
            windowing_mode: Windowing strategy - 'last'|'random'|'slide' (default: 'last' for compatibility)
            window_stride: Stride for sliding window mode (default: 10)
            max_windows_per_episode: Maximum windows per episode (default: 1)
            use_so100_processing: Whether to use SO100-style sample processing (Phase 2 feature)
        """
        # The base model is the PI0Policy itself.
        super().__init__(model=policy, **kwargs)
        self.policy = policy

        # ğŸ”¥ Phase 2: Processing method configuration
        self.use_so100_processing = use_so100_processing

        # ğŸ”¥ Legacy: çª—å£åŒ–é‡‡æ ·é…ç½® (kept for backward compatibility)
        self.windowing_mode = windowing_mode
        self.window_stride = window_stride
        self.max_windows_per_episode = max_windows_per_episode

        if use_so100_processing:
            print(f"ğŸš€ Using SO100-style sample processing (Phase 2)")
        else:
            print(f"ğŸ”§ Using legacy windowing: mode={windowing_mode}, stride={window_stride}, max_windows={max_windows_per_episode}")

        # è§†é¢‘æ”¶é›†ç›¸å…³
        self.video_frames = {}  # {episode_idx: [frames]}
        self.video_save_enabled = True

        # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
        self._load_norm_stats(norm_stats_path)

        # ğŸ”¥ Phase 2: Initialize SO100-style processors if enabled
        if self.use_so100_processing:
            self._initialize_so100_processors()
        
    def _load_norm_stats(self, norm_stats_path: Optional[str] = None):
        """Load normalization statistics from norm_stats.json"""
        if norm_stats_path is None:
            # å°è¯•åœ¨å¸¸è§ä½ç½®æ‰¾åˆ°norm_stats.json
            possible_paths = [
                "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch/norm_stats.json",
                "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json", 
                "./norm_stats.json"
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    norm_stats_path = path
                    break
        
        if norm_stats_path and Path(norm_stats_path).exists():
            print(f"Loading norm_stats from: {norm_stats_path}")
            with open(norm_stats_path) as f:
                norm_stats = json.load(f)
                
            # æå–çŠ¶æ€å’ŒåŠ¨ä½œçš„å½’ä¸€åŒ–å‚æ•°
            self.state_mean = np.array(norm_stats["norm_stats"]["state"]["mean"][:8], dtype=np.float32)
            self.state_std = np.array(norm_stats["norm_stats"]["state"]["std"][:8], dtype=np.float32)
            self.action_mean = np.array(norm_stats["norm_stats"]["actions"]["mean"][:7], dtype=np.float32)
            self.action_std = np.array(norm_stats["norm_stats"]["actions"]["std"][:7], dtype=np.float32)
            
            print(f"âœ… Loaded normalization stats:")
            print(f"  State mean shape: {self.state_mean.shape}")
            print(f"  State std shape: {self.state_std.shape}")
            print(f"  Action mean shape: {self.action_mean.shape}")
            print(f"  Action std shape: {self.action_std.shape}")
        else:
            print("âš ï¸  Warning: No norm_stats.json found, using identity normalization")
            # ä½¿ç”¨å•ä½å½’ä¸€åŒ–ï¼ˆä¸è¿›è¡Œå½’ä¸€åŒ–ï¼‰
            self.state_mean = np.zeros(8, dtype=np.float32)
            self.state_std = np.ones(8, dtype=np.float32)
            self.action_mean = np.zeros(7, dtype=np.float32)
            self.action_std = np.ones(7, dtype=np.float32)

    def _initialize_so100_processors(self):
        """
        Initialize SO100-style data processors with current normalization configuration.

        This method sets up the SO100StyleProcessor and TrajectoryToSampleGenerator
        using the same normalization statistics as the legacy windowing approach.
        """
        print("ğŸ”„ Initializing SO100-style data processors...")

        # Create configuration for SO100 processors
        so100_config = {
            'action_chunk_size': getattr(self.policy.config, 'n_action_steps', 50),
            'norm_stats_path': "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch/norm_stats.json",  # ç»™çœŸå®è·¯å¾„
            'state_mean': self.state_mean,
            'state_std': self.state_std,
            'action_mean': self.action_mean,
            'action_std': self.action_std
        }

        # Initialize processors
        self.so100_processor = SO100StyleProcessor(so100_config)
        self.sample_generator = TrajectoryToSampleGenerator(self.so100_processor)

        print(f"âœ… SO100 processors initialized:")
        print(f"  - Action chunk size: {so100_config['action_chunk_size']}")
        print(f"  - State normalization: mean={self.state_mean.shape}, std={self.state_std.shape}")
        print(f"  - Action normalization: mean={self.action_mean.shape}, std={self.action_std.shape}")

    def process_episodes_to_samples_so100(
        self,
        episodes: List[Dict[str, Any]],
        device: Optional[torch.device] = None,
    ) -> Tuple[Dict[str, Any], Dict[int, List[int]]]:
        """
        ğŸ”¥ Phase 2: Process episodes using SO100-style sample generation.

        This method replaces the windowing approach with SO100-style processing:
        - Each trajectory of length L generates L-50+1 training samples
        - Each sample contains obs[t] â†’ action[t:t+50] (relative actions)
        - Maintains episode-to-sample mapping for advantage computation

        Args:
            episodes: List of episode dictionaries
            device: Target device for tensors

        Returns:
            Tuple of:
                - batch: Training batch in OpenPI format
                - episode_to_samples_map: Mapping from episode indices to sample indices
        """
        if not self.use_so100_processing:
            raise ValueError("SO100 processing not enabled. Set use_so100_processing=True in constructor.")

        if device is None:
            device = next(self.policy.parameters()).device

        print(f"ğŸš€ Processing {len(episodes)} episodes with SO100-style sampling...")

        # Convert episodes to the format expected by SO100 processors
        formatted_episodes = []
        for i, ep in enumerate(episodes):
            # Convert episode to SO100 format
            formatted_episode = self._convert_episode_to_so100_format(ep, i)
            formatted_episodes.append(formatted_episode)

        # Use SO100 processors to generate training batch
        training_batch, episode_to_samples_map = self.sample_generator.process_episodes_to_training_batch(
            formatted_episodes, device
        )

        # Add additional fields expected by CFG training
        training_batch['owner_indices'] = self._create_owner_indices(episode_to_samples_map)

        print(f"âœ… SO100 processing complete:")
        print(f"  - Episodes: {len(episodes)}")
        print(f"  - Training samples: {training_batch['batch_size']}")
        print(f"  - Data utilization: {training_batch['batch_size'] / len(episodes):.1f}x")

        return training_batch, episode_to_samples_map

    def _convert_episode_to_so100_format(self, episode: Dict[str, Any], episode_idx: int) -> Dict[str, Any]:
        """
        Convert an episode from the current format to SO100 processor format.

        Args:
            episode: Episode dictionary from current system
            episode_idx: Episode index for identification

        Returns:
            Episode in SO100 processor format
        """
        # Extract episode data
        observations = episode.get('observations', [])
        actions = episode.get('actions', [])

        # Process observations and extract states
        processed_observations = []
        states = []

        for obs in observations:
            # Extract state (8-dimensional)
            unnorm_state = self._extract_state_from_obs(obs, episode_idx, 0)
            states.append(unnorm_state)

            # Create processed observation in OpenPI format
            processed_obs = self._create_processed_observation(obs, unnorm_state)
            processed_observations.append(processed_obs)

        # Create SO100 format episode
        so100_episode = {
            'id': f"episode_{episode_idx}",
            'processed_observations': processed_observations,
            'actions': actions,
            'states': states,
            'total_reward': episode.get('total_reward', 0.0)
        }

        return so100_episode

    def _create_owner_indices(self, episode_to_samples_map: Dict[int, List[int]]) -> List[int]:
        """
        Create owner indices array for tracking which samples belong to which episodes.

        Args:
            episode_to_samples_map: Mapping from episode indices to sample indices

        Returns:
            List where owner_indices[sample_idx] = episode_idx
        """
        # Find total number of samples
        total_samples = 0
        for sample_indices in episode_to_samples_map.values():
            total_samples = max(total_samples, max(sample_indices) + 1 if sample_indices else 0)

        # Create owner indices array
        owner_indices = [-1] * total_samples  # Initialize with -1 (invalid)

        for episode_idx, sample_indices in episode_to_samples_map.items():
            for sample_idx in sample_indices:
                owner_indices[sample_idx] = episode_idx

        return owner_indices

    def _create_processed_observation(self, obs: Dict[str, Any], unnorm_state: np.ndarray) -> Dict[str, Any]:
        """
        Create processed observation in OpenPI format.

        Args:
            obs: Raw observation dictionary
            unnorm_state: Unnormalized state (8-dimensional)

        Returns:
            Processed observation in OpenPI format
        """
        # Extract and process images
        base_image, wrist_image = self._extract_dual_images_from_obs(obs, 0, 0)

        # Apply image transformations (following 2_pi0_on_libero.py)
        def to_hwc_hmirror(arr: np.ndarray) -> np.ndarray:
            if isinstance(arr, np.ndarray) and arr.ndim == 3:
                # CHW -> HWC if needed
                if arr.shape[0] == 3 and arr.shape[-1] != 3:
                    arr = arr.transpose(1, 2, 0)
                # Horizontal mirror
                return arr[:, ::-1, :].copy()
            return arr

        base_image = to_hwc_hmirror(base_image)
        wrist_image = to_hwc_hmirror(wrist_image)

        # Normalize state
        normalized_state = self.normalize_state(unnorm_state)

        # Create processed observation
        processed_obs = {
            'image': {
                'base_0_rgb': torch.from_numpy(base_image).float(),
                'left_wrist_0_rgb': torch.from_numpy(wrist_image).float()
            },
            'state': torch.from_numpy(normalized_state).float(),
            'prompt': [obs.get('task_description', '')]
        }

        return processed_obs

    def map_episode_advantages_to_samples_so100(
        self,
        episode_advantages: torch.Tensor,
        episode_to_samples_map: Dict[int, List[int]]
    ) -> torch.Tensor:
        """
        ğŸ”¥ Phase 2: Map episode-level RLOO advantages to sample-level advantages.

        This is critical for RIPT integration: episode-level RLOO advantages are computed
        correctly using RIPT mathematics, then propagated to all samples from that episode.

        Args:
            episode_advantages: Tensor of shape (num_episodes,) with RLOO advantages
            episode_to_samples_map: Mapping from episode indices to sample indices

        Returns:
            sample_advantages: Tensor of shape (num_samples,) with advantages for each sample
        """
        if not self.use_so100_processing:
            raise ValueError("SO100 processing not enabled.")

        print(f"ğŸ”„ Mapping {len(episode_advantages)} episode advantages to sample level...")

        # Use the sample generator's mapping method
        sample_advantages = self.sample_generator.map_episode_advantages_to_samples(
            episode_advantages, episode_to_samples_map
        )

        # ğŸ”§ ç¡®ä¿ä¼˜åŠ¿ tensor ä¸ batch ä½äºåŒä¸€è®¾å¤‡ï¼Œé¿å… "different devices" é”™è¯¯
        sample_advantages = sample_advantages.to(self.device)

        # Validate mapping consistency
        is_valid = self.sample_generator.validate_episode_to_sample_mapping(
            episode_advantages, episode_to_samples_map, len(sample_advantages)
        )

        if not is_valid:
            raise ValueError("Episode-to-sample advantage mapping validation failed")

        print(f"âœ… Advantage mapping complete:")
        print(f"  - Episode advantages: {len(episode_advantages)}")
        print(f"  - Sample advantages: {len(sample_advantages)}")
        print(f"  - Positive samples: {(sample_advantages > 0).sum().item()}")
        print(f"  - Negative samples: {(sample_advantages <= 0).sum().item()}")

        return sample_advantages

    def create_unified_sample_pool(
        self,
        episodes: List[Dict[str, Any]],
        advantages: torch.Tensor,
        device: Optional[torch.device] = None,
        shuffle_samples: bool = True
    ) -> Tuple[List[Dict[str, Any]], torch.Tensor]:
        """
        ğŸš€ ç»Ÿä¸€æ ·æœ¬æ± æ–¹æ³•ï¼šä½ æƒ³è¦çš„ç†æƒ³æ¶æ„

        å°†æ‰€æœ‰episodesä¸€æ¬¡æ€§è½¬æ¢ä¸ºç»Ÿä¸€çš„æ ·æœ¬æ± ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰å¯¹åº”çš„ä¼˜åŠ¿å€¼ã€‚
        è¿™æ˜¯æ ‡å‡†æ·±åº¦å­¦ä¹ è®­ç»ƒèŒƒå¼ï¼Œé¿å…äº†å¤æ‚çš„episode-to-sampleæ˜ å°„ã€‚

        Args:
            episodes: åŸå§‹episodeåˆ—è¡¨
            advantages: episodeçº§åˆ«çš„ä¼˜åŠ¿ (E,)
            device: ç›®æ ‡è®¾å¤‡
            shuffle_samples: æ˜¯å¦æ‰“æ•£æ ·æœ¬é¡ºåº

        Returns:
            Tuple of:
                - unified_samples: ç»Ÿä¸€çš„æ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹
                - sample_advantages: å¯¹åº”çš„æ ·æœ¬çº§ä¼˜åŠ¿ (N,)
        """
        if device is None:
            device = self.device

        print(f"ğŸ”„ Creating unified sample pool from {len(episodes)} episodes...")

        # 1. ç”Ÿæˆæ‰€æœ‰æ ·æœ¬å¹¶è®°å½•æ¥æºepisode
        all_samples = []
        sample_episode_mapping = []  # è®°å½•æ¯ä¸ªæ ·æœ¬æ¥è‡ªå“ªä¸ªepisode

        for episode_idx, episode in enumerate(episodes):
            # è½¬æ¢episodeæ ¼å¼
            formatted_episode = self._convert_episode_to_so100_format(episode, episode_idx)

            # ç”Ÿæˆè¯¥episodeçš„æ‰€æœ‰æ ·æœ¬
            episode_samples = self.so100_processor.process_trajectory_to_samples(formatted_episode)

            # ğŸ”§ è½¬æ¢ä¸ºOpenPIæ ¼å¼ï¼Œç¡®ä¿åŒ…å«'image'ç­‰å­—æ®µ
            for sample in episode_samples:
                openpi_sample = self.so100_processor.convert_to_openpi_format(sample)
                all_samples.append(openpi_sample)
                sample_episode_mapping.append(episode_idx)

        print(f"  Generated {len(all_samples)} samples from {len(episodes)} episodes")
        print(f"  Average samples per episode: {len(all_samples) / len(episodes):.1f}")

        # 2. åˆ›å»ºæ ·æœ¬çº§ä¼˜åŠ¿ï¼ˆç›´æ¥ä»episodeä¼˜åŠ¿å¤åˆ¶ï¼‰
        sample_advantages = torch.zeros(len(all_samples), device=device, dtype=advantages.dtype)
        for sample_idx, episode_idx in enumerate(sample_episode_mapping):
            sample_advantages[sample_idx] = advantages[episode_idx]

        # 3. å¯é€‰ï¼šæ‰“æ•£æ ·æœ¬é¡ºåºï¼Œç ´é™¤ç›¸å…³æ€§
        if shuffle_samples:
            import random
            # åˆ›å»ºç´¢å¼•åˆ—è¡¨å¹¶æ‰“æ•£
            indices = list(range(len(all_samples)))
            random.shuffle(indices)

            # é‡æ–°æ’åˆ—æ ·æœ¬å’Œä¼˜åŠ¿
            shuffled_samples = [all_samples[i] for i in indices]
            shuffled_advantages = sample_advantages[indices]

            all_samples = shuffled_samples
            sample_advantages = shuffled_advantages
            print(f"  âœ… Samples shuffled to break episode/temporal correlations")

        print(f"âœ… Unified sample pool created:")
        print(f"  - Total samples: {len(all_samples)}")
        print(f"  - Positive samples: {(sample_advantages > 0).sum().item()}")
        print(f"  - Negative samples: {(sample_advantages <= 0).sum().item()}")
        print(f"  - Data utilization: {len(all_samples) / len(episodes):.1f}x")

        return all_samples, sample_advantages

    def compute_loss_from_sample_pool(
        self,
        samples: List[Dict[str, Any]],
        sample_advantages: torch.Tensor,
        batch_size: int = 8,  # ğŸ”¥ å‡å°‘batchå¤§å°é¿å…OOM
        device: Optional[torch.device] = None,
        scaler: Optional[torch.cuda.amp.GradScaler] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        gradient_accumulation_steps: int = 1
    ) -> float:
        """
        ğŸš€ ä»ç»Ÿä¸€æ ·æœ¬æ± è®¡ç®—æŸå¤±ï¼šæ ‡å‡†æ·±åº¦å­¦ä¹ è®­ç»ƒèŒƒå¼

        å°†æ ·æœ¬æ± æŒ‰å›ºå®šbatch_sizeåˆ‡åˆ†ï¼Œé€batchè®¡ç®—æŸå¤±å¹¶ç´¯ç§¯ã€‚
        è¿™æ˜¯ä½ æƒ³è¦çš„ç†æƒ³æ¶æ„ï¼šå›ºå®šbatchå¤§å°ï¼Œæ ‡å‡†æ¢¯åº¦ç´¯ç§¯ã€‚

        Args:
            samples: ç»Ÿä¸€æ ·æœ¬æ± 
            sample_advantages: å¯¹åº”çš„æ ·æœ¬ä¼˜åŠ¿ (N,)
            batch_size: å›ºå®šçš„batchå¤§å°
            device: ç›®æ ‡è®¾å¤‡

        Returns:
            total_loss: ç´¯ç§¯çš„æ€»æŸå¤±
        """
        if device is None:
            device = self.device

        total_samples = len(samples)
        num_batches = (total_samples + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´

        print(f"ğŸ”„ Computing loss from sample pool:")
        print(f"  - Total samples: {total_samples}")
        print(f"  - Batch size: {batch_size}")
        print(f"  - Number of batches: {num_batches}")

        total_loss = 0.0
        processed_samples = 0
        gradient_step = 0  # æ¢¯åº¦ç´¯ç§¯è®¡æ•°å™¨

        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_samples)

            # æå–å½“å‰batchçš„æ ·æœ¬å’Œä¼˜åŠ¿
            batch_samples = samples[start_idx:end_idx]
            batch_advantages = sample_advantages[start_idx:end_idx]

            # ğŸ”¥ åŠ¨æ€batchå¤§å°è°ƒæ•´ï¼Œé¿å…OOM
            current_batch_size = len(batch_samples)

            try:
                # å°†æ ·æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
                batch_data = self._collate_samples_to_batch(batch_samples, device)

                # è®¡ç®—CFGæŸå¤±
                batch_loss = self._compute_cfg_loss_for_batch(batch_data, batch_advantages, device)

            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"âš ï¸ Batch {batch_idx + 1} OOMï¼Œå°è¯•å‡åŠbatchå¤§å°...")

                    # æ¸…ç†æ˜¾å­˜
                    torch.cuda.empty_cache()

                    # åˆ†å‰²batchä¸ºä¸¤ä¸ªæ›´å°çš„batch
                    mid_point = len(batch_samples) // 2
                    if mid_point == 0:
                        print(f"âŒ å•ä¸ªæ ·æœ¬éƒ½æ— æ³•å¤„ç†ï¼Œè·³è¿‡batch {batch_idx + 1}")
                        continue

                    # å¤„ç†å‰åŠéƒ¨åˆ†
                    sub_batch1 = batch_samples[:mid_point]
                    sub_advantages1 = batch_advantages[:mid_point]
                    sub_data1 = self._collate_samples_to_batch(sub_batch1, device)
                    sub_loss1 = self._compute_cfg_loss_for_batch(sub_data1, sub_advantages1, device)
                    
                    # ç«‹å³å¤„ç†ç¬¬ä¸€ä¸ªå­batchçš„æ¢¯åº¦
                    sub_weight1 = len(sub_batch1) / total_samples
                    sub_normalized_loss1 = sub_loss1 * sub_weight1 / gradient_accumulation_steps
                    sub_loss1_value = sub_loss1.detach().item()
                    
                    if scaler is not None:
                        scaler.scale(sub_normalized_loss1).backward()
                    else:
                        sub_normalized_loss1.backward()
                    
                    gradient_step += 1

                    # æ¸…ç†å¹¶å¤„ç†ååŠéƒ¨åˆ†
                    del sub_data1, sub_loss1, sub_normalized_loss1
                    torch.cuda.empty_cache()

                    sub_batch2 = batch_samples[mid_point:]
                    sub_advantages2 = batch_advantages[mid_point:]
                    sub_data2 = self._collate_samples_to_batch(sub_batch2, device)
                    sub_loss2 = self._compute_cfg_loss_for_batch(sub_data2, sub_advantages2, device)
                    
                    # ç«‹å³å¤„ç†ç¬¬äºŒä¸ªå­batchçš„æ¢¯åº¦
                    sub_weight2 = len(sub_batch2) / total_samples
                    sub_normalized_loss2 = sub_loss2 * sub_weight2 / gradient_accumulation_steps
                    sub_loss2_value = sub_loss2.detach().item()
                    
                    if scaler is not None:
                        scaler.scale(sub_normalized_loss2).backward()
                    else:
                        sub_normalized_loss2.backward()
                    
                    gradient_step += 1

                    # è®¡ç®—åŠ æƒå¹³å‡losså€¼ï¼ˆä»…ç”¨äºæ—¥å¿—ï¼‰
                    batch_loss_value = (sub_loss1_value * len(sub_batch1) + sub_loss2_value * len(sub_batch2)) / current_batch_size

                    del sub_data2, sub_loss2, sub_normalized_loss2
                    torch.cuda.empty_cache()
                    
                    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦éœ€è¦å‚æ•°æ›´æ–°ï¼ˆOOMåˆ†å‰²æƒ…å†µï¼‰
                    if gradient_step == gradient_accumulation_steps or batch_idx == num_batches - 1:
                        if optimizer is not None:
                            # æ¢¯åº¦è£å‰ª
                            if scaler is not None:
                                scaler.unscale_(optimizer)
                            grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
                            
                            # å‚æ•°æ›´æ–°
                            if scaler is not None:
                                scaler.step(optimizer)
                                scaler.update()
                            else:
                                optimizer.step()
                            
                            optimizer.zero_grad()
                            print(f"  âœ“ å‚æ•°æ›´æ–°å®Œæˆ (OOMåˆ†å‰², æ­¥éª¤ {gradient_step}/{gradient_accumulation_steps}, æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f})")
                            gradient_step = 0
                    
                    # è·³è¿‡æ­£å¸¸çš„æ¢¯åº¦ç´¯ç§¯ï¼Œå› ä¸ºå·²ç»å¤„ç†è¿‡äº†
                    total_loss += batch_loss_value * batch_weight
                    processed_samples += len(batch_samples)
                    
                    print(f"  Batch {batch_idx + 1}/{num_batches}: {len(batch_samples)} samples, loss={batch_loss_value:.6f}")
                    continue  # è·³è¿‡æ­£å¸¸å¤„ç†æµç¨‹
                else:
                    raise e

            # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯ç­–ç•¥ï¼šç±»ä¼¼ript-vlaçš„åšæ³•
            batch_weight = len(batch_samples) / total_samples
            # è€ƒè™‘æ¢¯åº¦ç´¯ç§¯æ­¥æ•°çš„å½’ä¸€åŒ–
            normalized_batch_loss = batch_loss * batch_weight / gradient_accumulation_steps
            
            # ä¿å­˜lossæ•°å€¼ç”¨äºæ—¥å¿—
            batch_loss_value = batch_loss.detach().item()
            
            # ç«‹å³åå‘ä¼ æ’­ï¼Œç´¯ç§¯æ¢¯åº¦
            if scaler is not None:
                scaler.scale(normalized_batch_loss).backward()
            else:
                normalized_batch_loss.backward()
            
            gradient_step += 1
            
            # ç´¯ç§¯æ•°å€¼ï¼ˆä¸ä¿ç•™æ¢¯åº¦ï¼‰
            total_loss += batch_loss_value * batch_weight
            
            # ğŸ”¥ å‚æ•°æ›´æ–°é€»è¾‘ï¼šè¾¾åˆ°ç´¯ç§¯æ­¥æ•°æˆ–æœ€åä¸€ä¸ªbatchæ—¶æ›´æ–°
            if gradient_step == gradient_accumulation_steps or batch_idx == num_batches - 1:
                if optimizer is not None:
                    # æ¢¯åº¦è£å‰ª
                    if scaler is not None:
                        scaler.unscale_(optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
                    
                    # å‚æ•°æ›´æ–°
                    if scaler is not None:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                    
                    optimizer.zero_grad()
                    print(f"  âœ“ å‚æ•°æ›´æ–°å®Œæˆ (æ­¥éª¤ {gradient_step}/{gradient_accumulation_steps}, æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f})")
                    gradient_step = 0
            
            # æ¸…ç†batchè®¡ç®—å›¾
            del batch_loss, normalized_batch_loss
            torch.cuda.empty_cache()

            processed_samples += len(batch_samples)

            if batch_idx % 10 == 0 or batch_idx == num_batches - 1:
                print(f"  Batch {batch_idx + 1}/{num_batches}: {len(batch_samples)} samples, loss={batch_loss_value:.6f}")

        print(f"âœ… Sample pool processing complete:")
        print(f"  - Processed samples: {processed_samples}")
        print(f"  - Total loss: {total_loss:.6f}")

        return total_loss

    def _collate_samples_to_batch(self, samples: List[Dict[str, Any]], device: torch.device) -> Dict[str, Any]:
        """
        å°†æ ·æœ¬åˆ—è¡¨æ•´ç†æˆæ¨¡å‹è¾“å…¥çš„batchæ ¼å¼ã€‚

        Args:
            samples: æ ·æœ¬åˆ—è¡¨
            device: ç›®æ ‡è®¾å¤‡

        Returns:
            batch: æ¨¡å‹è¾“å…¥æ ¼å¼çš„batch
        """
        if not samples:
            raise ValueError("Empty samples list")

        # ä½¿ç”¨sample_generatorçš„collateæ–¹æ³•
        return self.sample_generator.collate_samples_to_batch(samples, device)

    def _compute_cfg_loss_for_batch(
        self,
        batch: Dict[str, Any],
        advantages: torch.Tensor,
        device: torch.device
    ) -> torch.Tensor:
        """
        ä¸ºå•ä¸ªbatchè®¡ç®—CFGæŸå¤±ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰ã€‚

        Args:
            batch: æ¨¡å‹è¾“å…¥batch
            advantages: è¯¥batchçš„ä¼˜åŠ¿å€¼
            device: ç›®æ ‡è®¾å¤‡

        Returns:
            loss: è¯¥batchçš„å¹³å‡æŸå¤±
        """
        # ğŸ”¥ å†…å­˜ä¼˜åŒ–ï¼šå¼ºåˆ¶æ¸…ç†æ˜¾å­˜ç¢ç‰‡
        torch.cuda.empty_cache()

        # è·å–CFGå‚æ•°
        cfg_alpha = getattr(self.policy.config, 'cfg_uncond_weight', 0.1)

        # ğŸ”¥ ç¡®ä¿ä¼˜åŠ¿tensoråœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        advantages = advantages.to(device)

        # äºŒå€¼åŒ–ä¼˜åŠ¿
        w_pos = (advantages > 0).float().to(device)

        B = batch.get('batch_size', batch['state'].shape[0])

        # CFGåˆ†æ”¯è®¡ç®—ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
        if getattr(self.policy.model, 'cfg_enabled', True):
            print(f"ğŸ”® CFGåŒåˆ†æ”¯è®¡ç®—: batch_size={B}")

            # ğŸ”¥ åˆ†é˜¶æ®µè®¡ç®—é¿å…å†…å­˜å³°å€¼
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                # Step 1: æ¡ä»¶åˆ†æ”¯
                outputs = self.policy.forward(batch)
                # å…¼å®¹ (pred, dict) æˆ– dict ä¸¤ç§è¿”å›
                if isinstance(outputs, tuple):
                    loss_dict_pos = outputs[1]
                else:
                    loss_dict_pos = outputs
                per_step_per_dim_pos = loss_dict_pos['losses']  # (B, T, D)

                # ç«‹å³æ¸…ç†ä¸­é—´ç»“æœ
                del outputs, loss_dict_pos
                torch.cuda.empty_cache()

                # Step 2: æ— æ¡ä»¶åˆ†æ”¯
                uncond_batch = batch.copy()
                uncond_batch['prompt'] = [''] * B
                uncond_outputs = self.policy.forward(uncond_batch)
                if isinstance(uncond_outputs, tuple):
                    loss_dict_uncond = uncond_outputs[1]
                else:
                    loss_dict_uncond = uncond_outputs
                per_step_per_dim_uncond = loss_dict_uncond['losses']

                # ç«‹å³æ¸…ç†
                del uncond_outputs, loss_dict_uncond, uncond_batch
                torch.cuda.empty_cache()

                # Step 3: CFGç»„åˆï¼ˆåœ¨autocastå†…å®Œæˆï¼‰
                combined_loss_per_step = w_pos.view(B, 1, 1) * per_step_per_dim_pos + cfg_alpha * per_step_per_dim_uncond

                # ç«‹å³æ¸…ç†åˆ†æ”¯ç»“æœ
                del per_step_per_dim_pos, per_step_per_dim_uncond
                torch.cuda.empty_cache()
        else:
            print(f"ğŸ“ å•åˆ†æ”¯è®¡ç®—: batch_size={B}")

            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                # éCFGæ¨¡å¼ï¼šåªä½¿ç”¨æ¡ä»¶åˆ†æ”¯
                outputs = self.policy.forward(batch)
                if isinstance(outputs, tuple):
                    loss_dict = outputs[1]
                else:
                    loss_dict = outputs
                losses = loss_dict['losses']

                del outputs, loss_dict
                torch.cuda.empty_cache()

                combined_loss_per_step = w_pos.view(B, 1, 1) * losses

                del losses
                torch.cuda.empty_cache()

        # è®¡ç®—å¹³å‡æŸå¤±
        loss = combined_loss_per_step.mean()

        # æœ€ç»ˆæ¸…ç†
        del combined_loss_per_step
        torch.cuda.empty_cache()

        return loss

    def compute_weighted_loss_unified(
        self,
        episodes: List[Dict[str, Any]],
        advantages: torch.Tensor,
        device: Optional[torch.device] = None,
        batch_size: Optional[int] = None,
        shuffle_samples: Optional[bool] = None,
        scaler: Optional[torch.cuda.amp.GradScaler] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        gradient_accumulation_steps: int = 1
    ) -> torch.Tensor:
        """
        ğŸš€ ç»Ÿä¸€æ ·æœ¬æ± è®­ç»ƒæ¥å£ï¼šä½ æƒ³è¦çš„ç†æƒ³æ¶æ„

        è¿™æ˜¯å®Œæ•´çš„ç»Ÿä¸€æ ·æœ¬æ± è®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†ï¼š
        1. ç»Ÿä¸€æ ·æœ¬ç”Ÿæˆ
        2. æ ·æœ¬éšæœºåŒ–
        3. å›ºå®šbatchè®­ç»ƒ
        4. æ ‡å‡†æ¢¯åº¦ç´¯ç§¯

        Args:
            episodes: episodeåˆ—è¡¨
            advantages: episodeçº§ä¼˜åŠ¿
            device: ç›®æ ‡è®¾å¤‡
            batch_size: å›ºå®šbatchå¤§å°
            shuffle_samples: æ˜¯å¦æ‰“æ•£æ ·æœ¬

        Returns:
            loss: æ€»æŸå¤±
        """
        if not self.use_so100_processing:
            # å¦‚æœæ²¡æœ‰å¯ç”¨SO100ï¼Œå›é€€åˆ°åŸæœ‰æ–¹æ³•
            return self.compute_weighted_loss(episodes, advantages, device)

        if device is None:
            device = next(self.policy.parameters()).device

        # ä»é…ç½®è¯»å–å¯è°ƒå‚æ•°ï¼Œé¿å…ç¡¬ç¼–ç 
        if batch_size is None:
            batch_size = getattr(self.policy.config, 'unified_pool_batch_size', 32)
        if shuffle_samples is None:
            shuffle_samples = getattr(self.policy.config, 'unified_pool_shuffle', True)

        print(f"ğŸš€ Unified Sample Pool Training:")
        print(f"  - Episodes: {len(episodes)}")
        print(f"  - Batch size: {batch_size}")
        print(f"  - Shuffle samples: {shuffle_samples}")

        # 1. åˆ›å»ºç»Ÿä¸€æ ·æœ¬æ± 
        samples, sample_advantages = self.create_unified_sample_pool(
            episodes, advantages, device, shuffle_samples
        )

        # 2. ä»æ ·æœ¬æ± è®¡ç®—æŸå¤±ï¼ˆå†…éƒ¨å·²è¿›è¡Œæ¢¯åº¦ç´¯ç§¯ï¼‰
        loss_value = self.compute_loss_from_sample_pool(
            samples, sample_advantages, batch_size, device, scaler, optimizer, gradient_accumulation_steps
        )

        print(f"âœ… Unified training complete, loss: {loss_value:.6f}")

        # è¿”å›é›¶æ¢¯åº¦tensorï¼Œå› ä¸ºæ¢¯åº¦å·²åœ¨å†…éƒ¨ç´¯ç§¯
        return torch.tensor(loss_value, device=device, requires_grad=False)

    def process_episodes(
        self,
        episodes: List[Dict[str, Any]],
        device: Optional[torch.device] = None,
    ) -> Tuple[Dict[str, Any], List[int]]:
        """
        ğŸ”¥ Phase 2: Unified episode processing method.

        Routes to either SO100-style processing or legacy windowing based on configuration.

        Args:
            episodes: List of episode dictionaries
            device: Target device for tensors

        Returns:
            Tuple of:
                - batch: Training batch
                - owner_indices: List mapping batch indices to episode indices
        """
        if self.use_so100_processing:
            # Use SO100-style processing
            batch, episode_to_samples_map = self.process_episodes_to_samples_so100(episodes, device)
            owner_indices = batch['owner_indices']
            return batch, owner_indices
        else:
            # Use legacy windowing processing
            return self._extract_microbatch_data(episodes, device)
    
    def normalize_state(self, state: np.ndarray) -> np.ndarray:
        """Normalize state using loaded statistics"""
        return (state - self.state_mean[:len(state)]) / (self.state_std[:len(state)] + 1e-6)
    
    def denormalize_action(self, action: np.ndarray) -> np.ndarray:
        """Denormalize action using loaded statistics"""
        return action * (self.action_std + 1e-6) + self.action_mean

    def process_episodes(
        self,
        episodes: List[Dict[str, Any]],
        device: Optional[torch.device] = None,
    ) -> Tuple[Dict[str, Any], List[int]]:
        """
        å°† episodes æ‰“åŒ…ä¸º PI0Policy æœŸæœ›çš„ batchï¼Œæ”¯æŒçª—å£åŒ–é‡‡æ ·ï¼š
        - æ ¹æ®windowing_modeä»æ¯æ¡è½¨è¿¹äº§ç”Ÿå¤šä¸ªçª—å£æ ·æœ¬
        - action å½¢çŠ¶ä¿æŒ (B, T, 7)ï¼ŒBç°åœ¨æ˜¯æ€»çª—å£æ•°
        - æä¾› action_is_pad: (B, T) å¸ƒå°”ï¼ŒTrue è¡¨ç¤ºè¯¥æ—¶é—´æ­¥æ˜¯ padding
        - çŠ¶æ€ç»Ÿä¸€ 8 ç»´ (3 pos + 3 axis-angle + 2 gripper)
        
        Returns:
            batch: çª—å£åŒ–åçš„è®­ç»ƒæ‰¹æ¬¡ï¼ŒBç»´=æ€»çª—å£æ•°
            owner_indices: æ¯ä¸ªçª—å£å¯¹åº”çš„åŸå§‹episodeç´¢å¼•ï¼Œç”¨äºä¼˜åŠ¿æ˜ å°„
        """
        if device is None:
            device = self.device
        if not episodes:
            raise ValueError("Empty episodes list provided")

        all_states, all_images, all_wrist_images, all_actions = [], [], [], []
        all_action_is_pad, all_tasks = [], []
        owner_indices = []  # ğŸ”¥ æ–°å¢ï¼šè®°å½•æ¯ä¸ªçª—å£æ¥è‡ªå“ªä¸ªepisode

        target_seq_len = getattr(self.policy.config, 'n_action_steps', 50)  # ğŸ”¥ çª—å£å¤§å°=50
        diffusion_steps = getattr(self.policy.config, 'num_steps', 10)
        print(f"ğŸ”§ çª—å£åŒ–é‡‡æ ·: target_seq_len={target_seq_len}, mode={self.windowing_mode}")

        for i, ep in enumerate(episodes):
            try:
                observations = ep['observations']
                actions = ep['actions']
                tasks = ep.get('task', "default task")
                if not observations:
                    raise ValueError(f"Episode {i} empty observations")

                # ğŸ”¥ æ–°å¢ï¼šæå–å®Œæ•´è½¨è¿¹æ•°æ®
                states_seq, base_images_seq, wrist_images_seq, actions_seq = [], [], [], []
                max_steps = min(len(observations), len(actions))
                
                # æå–å®Œæ•´åºåˆ—æ•°æ®
                for t in range(max_steps):
                    obs_t = observations[t] if t < len(observations) else {}
                    act_t = actions[t] if t < len(actions) else np.zeros(7, np.float32)

                    # çŠ¶æ€(8ç»´)
                    states_seq.append(self._extract_state_from_obs(obs_t, i, t))
                    # åˆ†åˆ«æå–ä¸¤ä¸ªç›¸æœºçš„å›¾åƒ
                    base_img, wrist_img = self._extract_dual_images_from_obs(obs_t, i, t)
                    base_images_seq.append(base_img)
                    wrist_images_seq.append(wrist_img)

                    # åŠ¨ä½œ(7ç»´)
                    act_t = np.array(act_t[0] if (isinstance(act_t, list) and len(act_t) > 0) else act_t,
                                    dtype=np.float32)
                    if act_t.size != 7:
                        buf = np.zeros(7, dtype=np.float32)
                        buf[:min(7, act_t.size)] = act_t[:min(7, act_t.size)]
                        act_t = buf
                    actions_seq.append(act_t)

                # ğŸ”¥ çª—å£åŒ–é‡‡æ ·ï¼šæ ¹æ®æ¨¡å¼äº§ç”Ÿå¤šä¸ªçª—å£
                windows = self._sample_windows_from_episode(
                    states_seq, base_images_seq, wrist_images_seq, actions_seq, target_seq_len
                )
                
                task_str = tasks[0] if isinstance(tasks, list) else str(tasks)
                
                # ä¸ºæ¯ä¸ªçª—å£æ·»åŠ æ•°æ®å’Œownerç´¢å¼•
                for window in windows:
                    all_states.append(window['state'])
                    all_images.append(window['base_image'])
                    all_wrist_images.append(window['wrist_image'])
                    all_actions.append(window['actions'])
                    all_action_is_pad.append(window['action_is_pad'])
                    all_tasks.append(task_str)
                    owner_indices.append(i)  # è®°å½•çª—å£æ¥æº

            except Exception as e:
                print(f"Error processing episode {i}: {e}")
                # å…œåº•ï¼šè‡³å°‘äº§ç”Ÿä¸€ä¸ªé»˜è®¤çª—å£
                all_states.append(np.zeros(8, np.float32))
                all_images.append(np.ones((224, 224, 3), np.uint8) * 128)
                all_wrist_images.append(np.ones((224, 224, 3), np.uint8) * 128)
                all_actions.append(np.zeros((target_seq_len, 7), np.float32))
                all_action_is_pad.append(np.ones((target_seq_len,), dtype=bool))
                all_tasks.append("default task")
                owner_indices.append(i)

        batch = {
            "state": torch.from_numpy(np.stack(all_states)).to(device, dtype=torch.float32),            # (B,8)
            "image": {
                "base_0_rgb": torch.from_numpy(np.stack(all_images)).to(device, dtype=torch.uint8),       # (B,H,W,3)
                "left_wrist_0_rgb": torch.from_numpy(np.stack(all_wrist_images)).to(device, dtype=torch.uint8),  # (B,H,W,3) ğŸ”¥ æ–°å¢
            },
            "action": torch.from_numpy(np.stack(all_actions)).to(device, dtype=torch.float32),          # (B,T,7)
            "action_is_pad": torch.from_numpy(np.stack(all_action_is_pad)).to(device),                  # (B,T) bool
            "prompt": all_tasks,
        }

        num_windows = len(all_states)
        assert batch["state"].shape[0] == num_windows
        assert batch["image"]["base_0_rgb"].shape[0] == num_windows
        assert batch["action"].shape[0] == num_windows
        assert batch["action_is_pad"].shape[0] == num_windows
        assert len(owner_indices) == num_windows
        
        print(f"ğŸ”§ çª—å£åŒ–æ‰¹æ¬¡: {len(episodes)} episodes â†’ {num_windows} windows")
        print(f"   state {batch['state'].shape}, image {batch['image']['base_0_rgb'].shape}")
        print(f"   action {batch['action'].shape}, action_is_pad {batch['action_is_pad'].shape}")
        
        return batch, owner_indices
    
    def _sample_windows_from_episode(self, states_seq, base_images_seq, wrist_images_seq, actions_seq, target_seq_len):
        """ğŸ”¥ æ–°å¢ï¼šæ ¹æ®çª—å£åŒ–æ¨¡å¼ä»ä¸€æ¡è½¨è¿¹ä¸­é‡‡æ ·å¤šä¸ªçª—å£"""
        if len(actions_seq) == 0:
            # ç©ºè½¨è¿¹ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤çª—å£
            return [self._create_empty_window(target_seq_len)]
        
        windows = []
        
        if self.windowing_mode == 'last':
            # åŸæœ‰é€»è¾‘ï¼šåªå–æœ€åä¸€æ®µ
            window = self._create_window_from_range(
                states_seq, base_images_seq, wrist_images_seq, actions_seq, 
                -target_seq_len, len(actions_seq), target_seq_len
            )
            windows.append(window)
            
        elif self.windowing_mode == 'random':
            # éšæœºé‡‡æ ·ï¼šä»è½¨è¿¹ä¸­éšæœºå–ä¸€ä¸ªçª—å£
            if len(actions_seq) <= target_seq_len:
                # è½¨è¿¹å¤ªçŸ­ï¼Œæ•´æ¡è½¨è¿¹å°±æ˜¯ä¸€ä¸ªçª—å£
                window = self._create_window_from_range(
                    states_seq, base_images_seq, wrist_images_seq, actions_seq,
                    0, len(actions_seq), target_seq_len
                )
                windows.append(window)
            else:
                # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
                import random
                max_start = len(actions_seq) - target_seq_len
                start_idx = random.randint(0, max_start)
                window = self._create_window_from_range(
                    states_seq, base_images_seq, wrist_images_seq, actions_seq,
                    start_idx, start_idx + target_seq_len, target_seq_len
                )
                windows.append(window)
                
        elif self.windowing_mode == 'slide':
            # æ»‘åŠ¨çª—å£ï¼šæŒ‰æ­¥é•¿é‡‡æ ·å¤šä¸ªçª—å£
            if len(actions_seq) <= target_seq_len:
                # è½¨è¿¹å¤ªçŸ­ï¼Œåªæœ‰ä¸€ä¸ªçª—å£
                window = self._create_window_from_range(
                    states_seq, base_images_seq, wrist_images_seq, actions_seq,
                    0, len(actions_seq), target_seq_len
                )
                windows.append(window)
            else:
                # æ»‘åŠ¨é‡‡æ ·
                window_count = 0
                for start_idx in range(0, len(actions_seq) - target_seq_len + 1, self.window_stride):
                    if window_count >= self.max_windows_per_episode:
                        break
                    
                    window = self._create_window_from_range(
                        states_seq, base_images_seq, wrist_images_seq, actions_seq,
                        start_idx, start_idx + target_seq_len, target_seq_len
                    )
                    windows.append(window)
                    window_count += 1
                
                # å¦‚æœæ²¡æœ‰é‡‡æ ·åˆ°ä»»ä½•çª—å£ï¼ˆç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼‰ï¼Œå›é€€åˆ°lastæ¨¡å¼
                if not windows:
                    window = self._create_window_from_range(
                        states_seq, base_images_seq, wrist_images_seq, actions_seq,
                        -target_seq_len, len(actions_seq), target_seq_len
                    )
                    windows.append(window)
        
        else:
            raise ValueError(f"Unknown windowing_mode: {self.windowing_mode}")
        
        return windows
    
    def _create_window_from_range(self, states_seq, base_images_seq, wrist_images_seq, actions_seq, start_idx, end_idx, target_seq_len):
        """ä»æŒ‡å®šèŒƒå›´åˆ›å»ºä¸€ä¸ªçª—å£ - ä¿®å¤æ—¶åºå¯¹é½é—®é¢˜
        
        æ­£ç¡®çš„æ—¶åºå¯¹é½ï¼šobs[t] â†’ actions[t:t+H]
        å³ï¼šç”¨çª—å£èµ·ç‚¹çš„è§‚æµ‹ï¼Œå»ç›‘ç£æ¥ä¸‹æ¥è¿™Hæ­¥åŠ¨ä½œ
        """
        # å¤„ç†è´Ÿç´¢å¼•
        if start_idx < 0:
            start_idx = max(0, len(actions_seq) + start_idx)
        if end_idx < 0:
            end_idx = len(actions_seq) + end_idx
        
        # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
        start_idx = max(0, min(start_idx, len(actions_seq)))
        end_idx = max(start_idx, min(end_idx, len(actions_seq)))
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ­£ç¡®çš„æ—¶åºå¯¹é½
        # çª—å£èµ·ç‚¹çš„obs -> è¯¥obsä¹‹åçš„actions
        state_idx = start_idx  # ç”¨çª—å£èµ·ç‚¹çš„obs
        
        # æå–çª—å£åŠ¨ä½œæ•°æ®ï¼ˆä¸start_idxå¯¹åº”çš„obsä¹‹åçš„actionsï¼‰
        window_actions = actions_seq[start_idx:end_idx]
        valid_len = len(window_actions)
        
        # åˆ›å»ºå›ºå®šé•¿åº¦çš„åŠ¨ä½œåºåˆ—
        final_actions = np.zeros((target_seq_len, 7), dtype=np.float32)
        if valid_len > 0:
            final_actions[:valid_len] = np.asarray(window_actions, dtype=np.float32)
        
        # åˆ›å»º padding æ©ç 
        action_is_pad = np.ones((target_seq_len,), dtype=bool)
        action_is_pad[:valid_len] = False
        
        # ğŸ”¥ ä½¿ç”¨çª—å£èµ·ç‚¹çš„çŠ¶æ€å’Œå›¾åƒ
        if state_idx < len(states_seq):
            final_state = np.asarray(states_seq[state_idx], dtype=np.float32)
            final_base_image = base_images_seq[state_idx] if state_idx < len(base_images_seq) else (np.ones((224, 224, 3), np.uint8) * 128)
            final_wrist_image = wrist_images_seq[state_idx] if state_idx < len(wrist_images_seq) else (np.ones((224, 224, 3), np.uint8) * 128)
        else:
            # å…œåº•ï¼šå¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤å€¼
            final_state = np.zeros(8, np.float32)
            final_base_image = np.ones((224, 224, 3), np.uint8) * 128
            final_wrist_image = np.ones((224, 224, 3), np.uint8) * 128
        
        return {
            'state': final_state,
            'base_image': final_base_image,
            'wrist_image': final_wrist_image,
            'actions': final_actions,
            'action_is_pad': action_is_pad
        }
    
    def _create_empty_window(self, target_seq_len):
        """åˆ›å»ºä¸€ä¸ªç©ºçš„é»˜è®¤çª—å£"""
        return {
            'state': np.zeros(8, np.float32),
            'base_image': np.ones((224, 224, 3), np.uint8) * 128,
            'wrist_image': np.ones((224, 224, 3), np.uint8) * 128,
            'actions': np.zeros((target_seq_len, 7), np.float32),
            'action_is_pad': np.ones((target_seq_len,), dtype=bool)
        }
    def compute_weighted_loss(
        self,
        episodes: List[Dict[str, Any]],
        advantages: torch.Tensor,
        device: Optional[torch.device] = None,
    ) -> torch.Tensor:
        """
        CFGé£æ ¼è®­ç»ƒæ”¯æŒçª—å£åŒ–ï¼šåŒæ—¶è®¡ç®—æ¡ä»¶å’Œæ— æ¡ä»¶æŸå¤±
        
        ä¸¥æ ¼å¥‘çº¦æ¨¡å¼ï¼šæ‰€æœ‰è¾“å…¥/è¾“å‡ºå¿…é¡»æ»¡è¶³é¢„å®šä¹‰å¥‘çº¦ï¼Œè¿ååˆ™ç«‹å³å¤±è´¥
        
        ğŸ”¥ æ–°å¢çª—å£åŒ–æ”¯æŒï¼š
        - episodes: Eä¸ªåŸå§‹è½¨è¿¹
        - advantages: (E,) episodeçº§ä¼˜åŠ¿
        - é€šè¿‡owner_indicesæ˜ å°„åˆ°Bä¸ªçª—å£çº§ä¼˜åŠ¿
        
        è¾“å…¥å¥‘çº¦:
        - episodes: éç©ºlistï¼Œæ¯ä¸ªepisodeåŒ…å«å¿…éœ€å­—æ®µ
        - advantages: (E,) tensorï¼ŒEä¸episodesæ•°é‡åŒ¹é…
        
        è¾“å‡ºå¥‘çº¦:
        - policy.forward()å¿…é¡»è¿”å›åŒ…å«"losses"å­—æ®µçš„dictï¼Œshapeä¸º(B,T,D)
        - action_is_padå¿…é¡»ä¸º(B,T) bool tensorï¼Œè‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆæ­¥
        """
        if device is None:
            device = self.device

        # === ä¸¥æ ¼è¾“å…¥å¥‘çº¦éªŒè¯ ===
        assert episodes and len(episodes) > 0, "episodesä¸èƒ½ä¸ºç©º"
        assert advantages is not None and len(advantages) > 0, "advantagesä¸èƒ½ä¸ºç©º"
        assert len(episodes) == len(advantages), f"episodesæ•°é‡({len(episodes)})å¿…é¡»ä¸advantagesæ•°é‡({len(advantages)})åŒ¹é…"
        assert advantages.dim() == 1, f"advantageså¿…é¡»æ˜¯1ç»´tensorï¼Œå½“å‰ç»´åº¦: {advantages.dim()}"
        assert isinstance(advantages, torch.Tensor), f"advantageså¿…é¡»æ˜¯torch.Tensorç±»å‹ï¼Œå½“å‰ç±»å‹: {type(advantages)}"

        # ğŸ”¥ Phase 2: Choose processing method based on configuration
        if self.use_so100_processing:
            print("ğŸš€ Using SO100-style sample processing...")
            batch, episode_to_samples_map = self.process_episodes_to_samples_so100(episodes, device)

            # Map episode advantages to sample advantages
            sample_advantages = self.map_episode_advantages_to_samples_so100(advantages, episode_to_samples_map)

            # Create owner indices for compatibility
            owner_indices = batch['owner_indices']

            # Use sample advantages instead of episode advantages
            window_advantages = sample_advantages

        else:
            print("ğŸ”§ Using legacy windowing processing...")
            # ğŸ”¥ Legacy: çª—å£åŒ–æ‰¹æ¬¡å¤„ç†
            batch, owner_indices = self.process_episodes(episodes, device)

            # ğŸ”¥ ä¼˜åŠ¿æ˜ å°„å’Œå½’ä¸€åŒ–å¤„ç† (legacy approach)
            B = batch["state"].shape[0]
            window_advantages = torch.zeros(B, device=device, dtype=advantages.dtype)
            for window_idx, episode_idx in enumerate(owner_indices):
                window_advantages[window_idx] = advantages[episode_idx]
        
        # === çª—å£åŒ–æ‰¹æ¬¡éªŒè¯ ===
        assert "action_is_pad" in batch, "batchä¸­å¿…é¡»åŒ…å«action_is_padå­—æ®µ"
        action_is_pad = batch["action_is_pad"]
        assert action_is_pad.dtype == torch.bool, f"action_is_padå¿…é¡»æ˜¯boolç±»å‹ï¼Œå½“å‰ç±»å‹: {action_is_pad.dtype}"
        assert action_is_pad.dim() == 2, f"action_is_padå¿…é¡»æ˜¯2ç»´tensor (B,T)ï¼Œå½“å‰ç»´åº¦: {action_is_pad.dim()}"
        
        # Get batch size (works for both processing methods)
        if self.use_so100_processing:
            B = batch["batch_size"]  # SO100 processing provides batch_size directly
        else:
            B = batch["state"].shape[0]  # Legacy windowing uses tensor shape

        assert len(owner_indices) == B, f"owner_indicesé•¿åº¦({len(owner_indices)})å¿…é¡»ä¸æ‰¹æ¬¡å¤§å°({B})åŒ¹é…"
        
        # éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆæ­¥
        valid_steps = (~action_is_pad).sum()
        assert valid_steps > 0, "action_is_padæ˜¾ç¤ºæ‰€æœ‰æ­¥éª¤éƒ½æ˜¯paddingï¼Œå¿…é¡»è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆæ­¥éª¤"
        
        # === å…³é”®ä¿®å¤ï¼šç»Ÿä¸€é‡‡æ ·noiseå’Œtimeç”¨äºCFGåŒåˆ†æ”¯ ===
        n, d = self.policy.config.n_action_steps, self.policy.config.max_action_dim
        dtype = batch["state"].dtype
        
        # é‡‡æ ·ä¸€æ¬¡noiseå’Œtimeï¼Œä¸¤ä¸ªåˆ†æ”¯å…±äº«ï¼ˆä¸æœ€åˆCFGRLå®ç°å¯¹é½ï¼‰
        noise = torch.randn(B, n, d, device=device, dtype=dtype)
        time = self.policy.model.sample_time(B, device).to(dtype)
        
        # Advantage mapping is now handled above based on processing method
        
        # ğŸ”¥ äºŒå€¼ä¼˜åŠ¿ï¼šç®€å•åˆ¤æ–­æ­£è´Ÿï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚ä¿æŒäºŒå€¼åŒ–ï¼‰
        w_pos = (window_advantages > 0).float()
        
        # è®°å½•æ­£ä¼˜åŠ¿çª—å£å æ¯”
        positive_ratio = w_pos.mean()
        
        processing_type = "samples" if self.use_so100_processing else "windows"
        print(f"ğŸ”§ ä¼˜åŠ¿æ˜ å°„: {len(episodes)} episodes â†’ {B} {processing_type}")
        print(f"   episodeä¼˜åŠ¿: {advantages.shape} = {advantages[:3].tolist()[:3]}...")
        print(f"   {processing_type}ä¼˜åŠ¿: {window_advantages.shape} = {window_advantages[:3].tolist()[:3]}...")
        print(f"   äºŒå€¼ä¼˜åŠ¿: {w_pos[:3].tolist()[:3]}...")
        print(f"   æ­£ä¼˜åŠ¿{processing_type}å æ¯”: {positive_ratio:.2%}")
        
        # === CFGé£æ ¼åŒåˆ†æ”¯æŸå¤±è®¡ç®— ===
        
        # 1. æ¡ä»¶åˆ†æ”¯ï¼ˆæ­£æ ·æœ¬æŒ‡ç¤ºï¼‰- ä½¿ç”¨å…±äº«çš„noiseå’Œtime
        batch_positive = batch.copy()
        batch_positive["is_positive"] = torch.ones(B, device=device, dtype=torch.long)
        batch_positive["noise"] = noise
        batch_positive["time"] = time
        
        out_positive = self.policy.forward(batch_positive)
        
        # === ä¸¥æ ¼è¾“å‡ºå¥‘çº¦éªŒè¯ - æ¡ä»¶åˆ†æ”¯ ===
        assert isinstance(out_positive, (dict, tuple)), f"policy.forward()å¿…é¡»è¿”å›dictæˆ–tupleï¼Œæ¡ä»¶åˆ†æ”¯è¿”å›ç±»å‹: {type(out_positive)}"
        
        if isinstance(out_positive, tuple):
            assert len(out_positive) >= 2, f"tupleè¿”å›å€¼å¿…é¡»è‡³å°‘åŒ…å«2ä¸ªå…ƒç´ ï¼Œå½“å‰é•¿åº¦: {len(out_positive)}"
            loss_scalar_pos, loss_dict_pos = out_positive[0], out_positive[1]
        else:
            loss_dict_pos = out_positive
            loss_scalar_pos = out_positive.get("loss")
        
        assert isinstance(loss_dict_pos, dict), f"loss_dictå¿…é¡»æ˜¯dictç±»å‹ï¼Œæ¡ä»¶åˆ†æ”¯ç±»å‹: {type(loss_dict_pos)}"
        assert "losses" in loss_dict_pos, "policy forwardè¾“å‡ºå¿…é¡»åŒ…å«'losses'å­—æ®µ"
        
        per_step_per_dim_pos = loss_dict_pos["losses"]
        assert isinstance(per_step_per_dim_pos, torch.Tensor), f"losseså¿…é¡»æ˜¯torch.Tensorï¼Œæ¡ä»¶åˆ†æ”¯ç±»å‹: {type(per_step_per_dim_pos)}"
        assert per_step_per_dim_pos.dim() == 3, f"losseså¿…é¡»æ˜¯3ç»´tensor (B,T,D)ï¼Œæ¡ä»¶åˆ†æ”¯ç»´åº¦: {per_step_per_dim_pos.dim()}"
        assert per_step_per_dim_pos.shape[0] == B, f"lossesæ‰¹æ¬¡ç»´åº¦({per_step_per_dim_pos.shape[0]})å¿…é¡»ä¸çª—å£æ•°é‡({B})åŒ¹é…"
        assert per_step_per_dim_pos.shape[1] == action_is_pad.shape[1], f"lossesæ—¶é—´ç»´åº¦({per_step_per_dim_pos.shape[1]})å¿…é¡»ä¸action_is_padæ—¶é—´ç»´åº¦({action_is_pad.shape[1]})åŒ¹é…"

        # 2. æ— æ¡ä»¶åˆ†æ”¯ï¼ˆæ— æŒ‡ç¤ºï¼‰- ä½¿ç”¨ç›¸åŒçš„noiseå’Œtime
        batch_uncond = batch.copy()
        batch_uncond["is_positive"] = torch.zeros(B, device=device, dtype=torch.long)
        batch_uncond["noise"] = noise  # å…³é”®ï¼šä¸æ¡ä»¶åˆ†æ”¯å…±äº«ç›¸åŒçš„noise
        batch_uncond["time"] = time    # å…³é”®ï¼šä¸æ¡ä»¶åˆ†æ”¯å…±äº«ç›¸åŒçš„time
        
        out_uncond = self.policy.forward(batch_uncond)
        
        # === ä¸¥æ ¼è¾“å‡ºå¥‘çº¦éªŒè¯ - æ— æ¡ä»¶åˆ†æ”¯ ===
        assert isinstance(out_uncond, (dict, tuple)), f"policy.forward()å¿…é¡»è¿”å›dictæˆ–tupleï¼Œæ— æ¡ä»¶åˆ†æ”¯è¿”å›ç±»å‹: {type(out_uncond)}"
        
        if isinstance(out_uncond, tuple):
            assert len(out_uncond) >= 2, f"tupleè¿”å›å€¼å¿…é¡»è‡³å°‘åŒ…å«2ä¸ªå…ƒç´ ï¼Œå½“å‰é•¿åº¦: {len(out_uncond)}"
            loss_scalar_uncond, loss_dict_uncond = out_uncond[0], out_uncond[1]
        else:
            loss_dict_uncond = out_uncond
            loss_scalar_uncond = out_uncond.get("loss")
        
        assert isinstance(loss_dict_uncond, dict), f"loss_dictå¿…é¡»æ˜¯dictç±»å‹ï¼Œæ— æ¡ä»¶åˆ†æ”¯ç±»å‹: {type(loss_dict_uncond)}"
        assert "losses" in loss_dict_uncond, "policy forwardè¾“å‡ºå¿…é¡»åŒ…å«'losses'å­—æ®µ"
        
        per_step_per_dim_uncond = loss_dict_uncond["losses"]
        assert isinstance(per_step_per_dim_uncond, torch.Tensor), f"losseså¿…é¡»æ˜¯torch.Tensorï¼Œæ— æ¡ä»¶åˆ†æ”¯ç±»å‹: {type(per_step_per_dim_uncond)}"
        assert per_step_per_dim_uncond.dim() == 3, f"losseså¿…é¡»æ˜¯3ç»´tensor (B,T,D)ï¼Œæ— æ¡ä»¶åˆ†æ”¯ç»´åº¦: {per_step_per_dim_uncond.dim()}"
        assert per_step_per_dim_uncond.shape == per_step_per_dim_pos.shape, f"æ— æ¡ä»¶åˆ†æ”¯losseså½¢çŠ¶({per_step_per_dim_uncond.shape})å¿…é¡»ä¸æ¡ä»¶åˆ†æ”¯({per_step_per_dim_pos.shape})å®Œå…¨åŒ¹é…"

        # 3. è®¡ç®—CFGç»„åˆæŸå¤±
        per_step_pos = per_step_per_dim_pos.mean(dim=-1)  # (B,T)
        per_step_uncond = per_step_per_dim_uncond.mean(dim=-1)  # (B,T)
        
        # è·å–æœ‰æ•ˆæ­¥æ©ç ï¼Œæ’é™¤paddingæ­¥
        mask = (~action_is_pad).float()  # (B,T)
        
        # ğŸ”¥ CFGæƒé‡è®¡ç®—ï¼šä½¿ç”¨äºŒå€¼ä¼˜åŠ¿
        w_pos = w_pos.unsqueeze(1).expand_as(mask)  # (B,T) äºŒå€¼åŒ–
        
        # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šæ ‡å‡†CFGRLå…¬å¼ - L = w_pos * L_pos + w_uncond * L_uncond
        cfg_alpha = getattr(self.policy.config, 'cfg_uncond_weight', 0.1)
        combined_loss_per_step = w_pos * per_step_pos + cfg_alpha * per_step_uncond
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šPaddingæ„ŸçŸ¥çš„æŸå¤±å½’çº¦ - æŒ‰æœ‰æ•ˆæ­¥æ•°å½’ä¸€åŒ–
        # æ¯ä¸ªçª—å£çš„æœ‰æ•ˆæŸå¤± = æ€»æŸå¤± / æœ‰æ•ˆæ­¥æ•°
        window_valid_steps = mask.sum(dim=1)  # (B,) æ¯ä¸ªçª—å£çš„æœ‰æ•ˆæ­¥æ•°
        window_losses = (combined_loss_per_step * mask).sum(dim=1) / (window_valid_steps + 1e-8)  # (B,)
        
        # æœ€ç»ˆæŸå¤±ï¼šæ‰€æœ‰çª—å£çš„å¹³å‡æŸå¤±ï¼ˆæ¯ä¸ªçª—å£æƒé‡ç›¸ç­‰ï¼‰
        final_loss = window_losses.mean()
        
        assert torch.isfinite(final_loss), f"CFGæŸå¤±è®¡ç®—ç»“æœå¿…é¡»æ˜¯æœ‰é™æ•°å€¼ï¼Œå½“å‰å€¼: {final_loss}"
        assert not torch.isnan(final_loss), "CFGæŸå¤±è®¡ç®—ç»“æœä¸èƒ½æ˜¯NaN"
        assert not torch.isinf(final_loss), "CFGæŸå¤±è®¡ç®—ç»“æœä¸èƒ½æ˜¯Inf"
        
        return final_loss
    
    def compute_weighted_loss_microbatch(
        self,
        episodes: List[Dict[str, Any]],
        advantages: torch.Tensor,
        device: Optional[torch.device] = None,
        micro_batch_size: int = 8,
        grad_accum_steps: int = 1,
        use_amp: bool = True,
        optimizer = None,
        scaler = None
    ) -> torch.Tensor:
        """
        ğŸ”¥ çª—å£çº§å¾®æ‰¹æ¢¯åº¦ç´¯ç§¯ç‰ˆæœ¬ - è§£å†³æ˜¾å­˜OOMé—®é¢˜
        
        æ ¸å¿ƒæ€è·¯ï¼š
        1. å…ˆprocess_episodeså¾—åˆ°æ‰€æœ‰çª—å£çš„batch (B_windows)
        2. æ²¿B_windowsç»´åº¦åˆ‡åˆ†ä¸ºmicro_batch_sizeçš„å°å—
        3. æ¯ä¸ªå¾®æ‰¹æ¬¡ï¼šå…±äº«noise/timeï¼Œåˆ†åˆ«è®¡ç®—æ¡ä»¶/æ— æ¡ä»¶åˆ†æ”¯ï¼Œç»„åˆæŸå¤±
        4. æŸå¤±/grad_accum_stepsåbackwardï¼ˆç´¯ç§¯æ¢¯åº¦ï¼‰
        5. æ»¡grad_accum_stepsæ—¶ç»Ÿä¸€step
        
        Args:
            episodes: Eä¸ªåŸå§‹è½¨è¿¹
            advantages: (E,) episodeçº§ä¼˜åŠ¿
            micro_batch_size: å¾®æ‰¹æ¬¡çª—å£æ•°ï¼ˆæ§åˆ¶æ˜¾å­˜å³°å€¼ï¼‰
            grad_accum_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
            optimizer: ä¼˜åŒ–å™¨ï¼ˆç”¨äºstepï¼‰
            scaler: GradScalerï¼ˆAMPæ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
        
        Returns:
            å¹³å‡æŸå¤±å€¼
        """
        if device is None:
            device = self.device
        
        print(f"ğŸ”§ çª—å£çº§å¾®æ‰¹å¤„ç†: micro_batch_size={micro_batch_size}, grad_accum_steps={grad_accum_steps}, AMP={use_amp}")
        
        # 1. å¤„ç†episodeså¾—åˆ°çª—å£batch
        batch, owner_indices = self.process_episodes(episodes, device)
        
        B_windows = batch["state"].shape[0]
        print(f"   æ€»çª—å£æ•°: {B_windows}")
        
        if B_windows == 0:
            print("âš ï¸ æ— æœ‰æ•ˆçª—å£æ•°æ®")
            return torch.tensor(0.0, device=device)
        
        # 2. å‡†å¤‡ä¼˜åŠ¿æ˜ å°„
        window_advantages = torch.zeros(B_windows, device=device, dtype=advantages.dtype)
        for window_idx, episode_idx in enumerate(owner_indices):
            window_advantages[window_idx] = advantages[episode_idx]
        
        w_pos = (window_advantages > 0).float()
        
        # 3. å‡†å¤‡å…¨å±€noiseå’Œtimeï¼ˆæ‰€æœ‰å¾®æ‰¹æ¬¡å…±äº«ï¼‰
        n, d = self.policy.config.n_action_steps, self.policy.config.max_action_dim
        dtype = batch["state"].dtype
        
        # ğŸ”¥ å…³é”®ï¼šä¸ºæ•´ä¸ªbatchç”Ÿæˆç»Ÿä¸€çš„noiseå’Œtime
        global_noise = torch.randn(B_windows, n, d, device=device, dtype=dtype)
        global_time = self.policy.model.sample_time(B_windows, device).to(dtype)
        
        # 4. å¾®æ‰¹æ¬¡å¤„ç†
        total_loss = 0.0
        num_micro_batches = 0
        
        for start_idx in range(0, B_windows, micro_batch_size):
            end_idx = min(start_idx + micro_batch_size, B_windows)
            micro_B = end_idx - start_idx
            
            # æå–å¾®æ‰¹æ¬¡æ•°æ®
            micro_batch = {}
            for key, value in batch.items():
                if key == 'states':
                    # å¤„ç†åµŒå¥—çš„stateså­—å…¸
                    micro_batch[key] = {}
                    for sub_key, sub_value in value.items():
                        if isinstance(sub_value, torch.Tensor) and len(sub_value.shape) > 0:
                            micro_batch[key][sub_key] = sub_value[start_idx:end_idx]
                        else:
                            micro_batch[key][sub_key] = sub_value
                elif isinstance(value, torch.Tensor) and len(value.shape) > 0:
                    micro_batch[key] = value[start_idx:end_idx]
                else:
                    micro_batch[key] = value
            
            # æå–å¾®æ‰¹æ¬¡çš„ä¼˜åŠ¿å’Œnoise/time
            micro_w_pos = w_pos[start_idx:end_idx]
            micro_noise = global_noise[start_idx:end_idx]
            micro_time = global_time[start_idx:end_idx]
            
            # 5. ğŸ”¥ ä½¿ç”¨autocastè¿›è¡Œå¾®æ‰¹æ¬¡çš„åŒåˆ†æ”¯è®¡ç®—
            try:
                if use_amp:
                    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
                        micro_loss = self._compute_micro_batch_loss(
                            micro_batch, micro_w_pos, micro_noise, micro_time, device
                        )
                else:
                    micro_loss = self._compute_micro_batch_loss(
                        micro_batch, micro_w_pos, micro_noise, micro_time, device
                    )
                
                # æŸå¤±å½’ä¸€åŒ–å¹¶ç´¯ç§¯æ¢¯åº¦
                normalized_loss = micro_loss / grad_accum_steps
                
                if optimizer is not None:
                    if use_amp and scaler is not None:
                        scaler.scale(normalized_loss).backward()
                    else:
                        normalized_loss.backward()
                
                total_loss += micro_loss.item()
                num_micro_batches += 1
                
                print(f"   å¾®æ‰¹æ¬¡ {num_micro_batches}: B={micro_B}, loss={micro_loss.item():.6f}")
                
            except Exception as e:
                print(f"âŒ å¾®æ‰¹æ¬¡ {start_idx}:{end_idx} å¤„ç†å¤±è´¥: {e}")
                continue
        
        # 6. ç»Ÿä¸€è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼ˆå¦‚æœæä¾›äº†optimizerï¼‰
        if optimizer is not None and num_micro_batches >= grad_accum_steps:
            try:
                if use_amp and scaler is not None:
                    scaler.unscale_(optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
                    optimizer.step()
                
                optimizer.zero_grad()
                print(f"âœ… çª—å£çº§å¾®æ‰¹æ›´æ–°å®Œæˆ: æ¢¯åº¦èŒƒæ•°={grad_norm:.6f}")
            except Exception as e:
                print(f"âŒ æ¢¯åº¦æ›´æ–°å¤±è´¥: {e}")
                optimizer.zero_grad()
        
        avg_loss = total_loss / max(1, num_micro_batches)
        print(f"ğŸ¯ çª—å£çº§å¾®æ‰¹å¤„ç†å®Œæˆ: å¹³å‡æŸå¤±={avg_loss:.6f}")
        
        return torch.tensor(avg_loss, device=device)
    
    def _compute_micro_batch_loss(
        self, 
        micro_batch: Dict[str, Any], 
        w_pos: torch.Tensor, 
        noise: torch.Tensor, 
        time: torch.Tensor,
        device: torch.device
    ) -> torch.Tensor:
        """
        è®¡ç®—å•ä¸ªå¾®æ‰¹æ¬¡çš„CFGæŸå¤±
        
        Args:
            micro_batch: å¾®æ‰¹æ¬¡æ•°æ®
            w_pos: å¾®æ‰¹æ¬¡æ­£ä¼˜åŠ¿æ©ç 
            noise: å¾®æ‰¹æ¬¡noise
            time: å¾®æ‰¹æ¬¡time
            device: è®¡ç®—è®¾å¤‡
        
        Returns:
            å¾®æ‰¹æ¬¡æŸå¤±
        """
        B = micro_batch["state"].shape[0]
        
        # 1. æ¡ä»¶åˆ†æ”¯
        batch_positive = micro_batch.copy()
        batch_positive["is_positive"] = torch.ones(B, device=device, dtype=torch.long)
        batch_positive["noise"] = noise
        batch_positive["time"] = time
        
        out_positive = self.policy.forward(batch_positive)
        if isinstance(out_positive, tuple):
            loss_dict_pos = out_positive[1]
        else:
            loss_dict_pos = out_positive
        
        per_step_per_dim_pos = loss_dict_pos["losses"]
        
        # 2. æ— æ¡ä»¶åˆ†æ”¯ï¼ˆå…±äº«noiseå’Œtimeï¼‰
        batch_uncond = micro_batch.copy()
        batch_uncond["is_positive"] = torch.zeros(B, device=device, dtype=torch.long)
        batch_uncond["noise"] = noise  # ğŸ”¥ å…±äº«ç›¸åŒçš„noise
        batch_uncond["time"] = time    # ğŸ”¥ å…±äº«ç›¸åŒçš„time
        
        out_uncond = self.policy.forward(batch_uncond)
        if isinstance(out_uncond, tuple):
            loss_dict_uncond = out_uncond[1]
        else:
            loss_dict_uncond = out_uncond
        
        per_step_per_dim_uncond = loss_dict_uncond["losses"]
        
        # 3. CFGç»„åˆæŸå¤±è®¡ç®—
        per_step_pos = per_step_per_dim_pos.mean(dim=-1)  # (B,T)
        per_step_uncond = per_step_per_dim_uncond.mean(dim=-1)  # (B,T)
        
        # è·å–æœ‰æ•ˆæ­¥æ©ç 
        mask = (~micro_batch["action_is_pad"]).float()  # (B,T)
        
        # CFGæƒé‡è®¡ç®—
        w_pos_expanded = w_pos.unsqueeze(1).expand_as(mask)  # (B,T)
        
        # æ ‡å‡†CFGRLå…¬å¼
        cfg_alpha = getattr(self.policy.config, 'cfg_uncond_weight', 0.1)
        combined_loss_per_step = w_pos_expanded * per_step_pos + cfg_alpha * per_step_uncond
        
        # Paddingæ„ŸçŸ¥çš„æŸå¤±å½’çº¦
        window_valid_steps = mask.sum(dim=1)  # (B,)
        window_losses = (combined_loss_per_step * mask).sum(dim=1) / (window_valid_steps + 1e-8)  # (B,)
        
        # å¾®æ‰¹æ¬¡æœ€ç»ˆæŸå¤±
        micro_loss = window_losses.mean()
        
        return micro_loss
    
    def compute_act_logits(self, model, episodes: List[Dict[str, Any]], device: Optional[torch.device] = None):
        """
        This method is required by the RLModelInterface, but for our CFG-style
        optimizer, we use `compute_weighted_loss` instead.
        We can raise an error or return a placeholder.
        """
        raise NotImplementedError(
            "compute_act_logits is not used for PI0_CFG_Adapter. "
            "Use compute_weighted_loss instead."
        )

    @property
    def device(self):
        return next(self.policy.parameters()).device

    def get_policy_model(self):
        """Return the policy model, which is the PI0Policy instance itself."""
        return self.policy
        
    def _extract_state_from_obs(self, obs, episode_idx, step_idx):
        """æå– 8 ç»´çŠ¶æ€: 3 pos + 3 axis-angle + 2 gripperï¼Œå¹¶æŒ‰ norm_stats å½’ä¸€åŒ–"""
        try:
            if isinstance(obs, np.ndarray) and obs.dtype == object:
                obs = (obs.item() if obs.size == 1 else obs[0]) if obs.size > 0 else {}
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]
            if not isinstance(obs, dict) or not obs:
                return np.zeros(8, np.float32)

            if "robot0_eef_pos" in obs and "robot0_eef_quat" in obs:
                eef_pos = np.array(obs["robot0_eef_pos"], dtype=np.float32)
                eef_quat = np.array(obs["robot0_eef_quat"], dtype=np.float32)
                if eef_pos.size != 3: eef_pos = np.zeros(3, np.float32)
                if eef_quat.size != 4: eef_quat = np.array([0,0,0,1], np.float32)
                try:
                    from pi0.ript.utils.pi0_libero_utils import quat2axisangle
                    axis_angle = quat2axisangle(eef_quat).astype(np.float32)
                except Exception:
                    axis_angle = np.zeros(3, np.float32)

                gr = obs.get("robot0_gripper_qpos", [0.0, 0.0])
                gr = np.array(gr, dtype=np.float32)
                if gr.size < 2:
                    gr = np.pad(gr, (0, 2 - gr.size))
                else:
                    gr = gr[:2]

                unnorm = np.concatenate([eef_pos[:3], axis_angle[:3], gr[:2]], dtype=np.float32)  # (8,)
                state = self.normalize_state(unnorm)

            else:
                # æ— å…³é”®å­—æ®µåˆ™å›é€€ä¸º 0ï¼ˆä¸ç»Ÿè®¡ç›¸å®¹ï¼‰
                state = np.zeros(8, np.float32)

            return np.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0).astype(np.float32)

        except Exception as e:
            print(f"æå–çŠ¶æ€æ—¶å‡ºé”™ (episode {episode_idx}, step {step_idx}): {e}")
            return np.zeros(8, np.float32)
    
    def _extract_dual_images_from_obs(self, obs, episode_idx, step_idx):
        """ğŸ”¥ æ–°å¢ï¼šä»è§‚æµ‹ä¸­æå–ä¸¤ä¸ªç›¸æœºçš„å›¾åƒä¿¡æ¯"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†SubprocVectorEnvè¿”å›çš„numpy.arrayåŒ…è£…çš„è§‚æµ‹
            if isinstance(obs, np.ndarray) and obs.dtype == object:
                if obs.size == 1:
                    obs = obs.item()  # æå–å•ä¸ªå…ƒç´ 
                elif obs.size > 0:
                    obs = obs[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            
            # é»˜è®¤å›¾åƒï¼ˆå…œåº•ï¼‰
            default_img = np.ones((224, 224, 3), np.uint8) * 128
            base_img = default_img.copy()
            wrist_img = default_img.copy()
            
            if isinstance(obs, dict):
                # æå–base_0_rgb (agentview_image)
                if "agentview_image" in obs:
                    raw_img = obs["agentview_image"]
                    if isinstance(raw_img, np.ndarray) and raw_img.size > 0:
                        base_img = self._process_single_image(raw_img, "base")
                
                # æå–left_wrist_0_rgb (robot0_eye_in_hand_image)
                if "robot0_eye_in_hand_image" in obs:
                    raw_img = obs["robot0_eye_in_hand_image"]
                    if isinstance(raw_img, np.ndarray) and raw_img.size > 0:
                        wrist_img = self._process_single_image(raw_img, "wrist")
            
            return base_img, wrist_img
            
        except Exception as e:
            print(f"æå–åŒå›¾åƒæ—¶å‡ºé”™ (episode {episode_idx}, step {step_idx}): {e}")
            default_img = np.ones((224, 224, 3), np.uint8) * 128
            return default_img.copy(), default_img.copy()
    
    def _process_single_image(self, img, cam_type):
        """å¤„ç†å•ä¸ªå›¾åƒçš„é€šç”¨é€»è¾‘ï¼Œä¸"2_pi0_on_libero.py"çš„to_hwc_hmirrorå®Œå…¨å¯¹é½"""
        try:
            # ğŸ”§ ç»Ÿä¸€å›¾åƒå¤„ç†ï¼šå…ˆè§„èŒƒåˆ°HWCï¼Œå†åšæ°´å¹³é•œåƒï¼ˆä¸åšé€šé“äº¤æ¢ï¼‰
            if img.ndim == 3:
                # CHW -> HWCï¼ˆå¦‚æœéœ€è¦ï¼‰
                if img.shape[0] == 3 and img.shape[-1] != 3:
                    img = img.transpose(1, 2, 0)
                elif img.shape[-1] != 3:  # æ—¢ä¸æ˜¯CHWä¹Ÿä¸æ˜¯HWC
                    print(f"âœ— æœªçŸ¥{cam_type}å›¾åƒæ ¼å¼: {img.shape}")
                    raise ValueError(f"Unexpected {cam_type} image format: {img.shape}")
                
                # æ°´å¹³é•œåƒï¼ˆç¿»è½¬å®½åº¦ç»´ï¼‰
                img_processed = img[:, ::-1, :].copy()
                
                # ç¡®ä¿æ•°æ®ç±»å‹å’ŒèŒƒå›´æ­£ç¡®
                if img_processed.dtype != np.uint8:
                    if img_processed.max() <= 1.0:  # å½’ä¸€åŒ–çš„å›¾åƒ
                        img_processed = (img_processed * 255).astype(np.uint8)
                    else:
                        img_processed = img_processed.astype(np.uint8)
                
                # ç¡®ä¿å›¾åƒå°ºå¯¸æ­£ç¡®
                if img_processed.shape[:2] != (224, 224):
                    try:
                        from skimage.transform import resize
                        img_processed = resize(img_processed, (224, 224), preserve_range=True).astype(np.uint8)
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰skimageï¼Œä½¿ç”¨ç®€å•çš„è£å‰ª/å¡«å……
                        h, w = img_processed.shape[:2]
                        if h != 224 or w != 224:
                            # ç®€å•å±…ä¸­è£å‰ªæˆ–å¡«å……åˆ°224x224
                            resized = np.ones((224, 224, 3), dtype=np.uint8) * 128
                            start_h = max(0, (224 - h) // 2)
                            start_w = max(0, (224 - w) // 2)
                            end_h = min(224, start_h + h)
                            end_w = min(224, start_w + w)
                            src_h = min(h, 224)
                            src_w = min(w, 224)
                            resized[start_h:end_h, start_w:end_w] = img_processed[:src_h, :src_w]
                            img_processed = resized
                
                return img_processed
            else:
                print(f"âœ— {cam_type}å›¾åƒç»´åº¦é”™è¯¯: {img.shape}")
                return np.ones((224, 224, 3), np.uint8) * 128
                
        except Exception as e:
            print(f"å¤„ç†{cam_type}å›¾åƒæ—¶å‡ºé”™: {e}")
            return np.ones((224, 224, 3), np.uint8) * 128

    def _extract_image_from_obs(self, obs, episode_idx, step_idx):
        """ä»è§‚æµ‹ä¸­æå–å›¾åƒä¿¡æ¯"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†SubprocVectorEnvè¿”å›çš„numpy.arrayåŒ…è£…çš„è§‚æµ‹
            if isinstance(obs, np.ndarray) and obs.dtype == object:
                if obs.size == 1:
                    obs = obs.item()  # æå–å•ä¸ªå…ƒç´ 
                elif obs.size > 0:
                    obs = obs[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            
            if isinstance(obs, list) and len(obs) > 0:
                obs = obs[0]  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„è§‚æµ‹
            
            if isinstance(obs, dict) and "agentview_image" in obs:
                img = obs["agentview_image"]
                if isinstance(img, np.ndarray) and img.size > 0:
                    # ğŸ”§ ä¿®å¤å›¾åƒæ ¼å¼æ£€æŸ¥ï¼šå¤„ç†CHWå’ŒHWCä¸¤ç§æ ¼å¼
                    if img.ndim == 3:
                        # æ£€æŸ¥æ˜¯CHWæ ¼å¼ (3, H, W) è¿˜æ˜¯HWCæ ¼å¼ (H, W, 3)
                        if img.shape[0] == 3 and img.shape[-1] != 3:  # CHWæ ¼å¼
                            # è½¬æ¢CHW â†’ HWC
                            img = img.transpose(1, 2, 0)
                        elif img.shape[-1] != 3:  # æ—¢ä¸æ˜¯CHWä¹Ÿä¸æ˜¯HWC
                            print(f"âœ— æœªçŸ¥å›¾åƒæ ¼å¼: {img.shape}")
                            raise ValueError(f"Unexpected image format: {img.shape}")
                        
                        # ç°åœ¨imgåº”è¯¥æ˜¯HWCæ ¼å¼
                        # é™é»˜å¤„ç†å›¾åƒï¼ˆå‡å°‘å†—ä½™æ—¥å¿—ï¼‰
                        
                        # ä¸ºæ¨¡å‹æ¨ç†å‡†å¤‡å›¾åƒï¼ˆç¡®ä¿æ¥æ”¶æ­£ç¡®æ–¹å‘çš„å›¾åƒï¼‰
                        # æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œç¡®ä¿æ¨¡å‹æ¥æ”¶çš„æ˜¯æ­£ç€çš„å›¾åƒ
                        model_img = img.copy()  # ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼Œä¸è¿›è¡Œæ—‹è½¬
                        
                        # æ”¶é›†å›¾åƒç”¨äºè§†é¢‘ç”Ÿæˆï¼ˆéœ€è¦æ—‹è½¬ä»¥æ˜¾ç¤ºæ­£ç¡®æ–¹å‘ï¼‰
                        video_img = img[::-1, ::-1]  # 180åº¦æ—‹è½¬ç”¨äºè§†é¢‘æ˜¾ç¤º
                        self._collect_image_for_video(video_img, episode_idx, step_idx)
                        
                        # è¿”å›ç”¨äºæ¨¡å‹æ¨ç†çš„å›¾åƒï¼ˆå·²è¿›è¡Œ180åº¦æ—‹è½¬ï¼‰
                        # å¦‚æœå›¾åƒå°ºå¯¸ä¸æ˜¯ 224x224ï¼Œéœ€è¦è°ƒæ•´å¤§å°
                        if model_img.shape[:2] != (224, 224):
                            try:
                                from skimage.transform import resize
                                model_img = resize(model_img, (224, 224), preserve_range=True).astype(np.uint8)
                            except ImportError:
                                # å¦‚æœæ²¡æœ‰ skimageï¼Œä½¿ç”¨ç®€å•çš„æ’å€¼
                                import cv2
                                model_img = cv2.resize(model_img, (224, 224))
                        return model_img
                    else:
                        print(f"âœ— å›¾åƒç»´åº¦å¼‚å¸¸: {img.shape}")
                else:
                    print(f"âœ— å›¾åƒæ•°æ®æ— æ•ˆ: type={type(img)}, size={getattr(img, 'size', 'N/A')}")
            else:
                available_keys = list(obs.keys()) if isinstance(obs, dict) else "éå­—å…¸ç±»å‹"
                print(f"âœ— è§‚æµ‹ä¸­æ— agentview_imageé”®, å¯ç”¨é”®: {available_keys}")
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå›¾åƒï¼Œè¿”å›å ä½ç¬¦
            print(f"âœ— ä½¿ç”¨å ä½ç¬¦å›¾åƒ (episode {episode_idx}, step {step_idx})")
            return np.ones((224, 224, 3), dtype=np.uint8) * 128
            
        except Exception as e:
            print(f"æå–å›¾åƒæ—¶å‡ºé”™ (episode {episode_idx}, step {step_idx}): {e}")
            return np.ones((224, 224, 3), dtype=np.uint8) * 128

    def _collect_image_for_video(self, img, episode_idx, step_idx):
        """æ”¶é›†å›¾åƒç”¨äºè§†é¢‘ç”Ÿæˆ"""
        if not self.video_save_enabled:
            return
            
        try:
            if episode_idx not in self.video_frames:
                self.video_frames[episode_idx] = []
            
            # ä¿å­˜åŸå§‹æ–¹å‘çš„å›¾åƒç”¨äºè§†é¢‘
            self.video_frames[episode_idx].append(img.copy())
            
        except Exception as e:
            print(f"æ”¶é›†è§†é¢‘å¸§æ—¶å‡ºé”™: {e}")

    def finalize_episode_video(self, episode_idx, task_description="default_task"):
        """å®Œæˆepisodeæ—¶ç”Ÿæˆè§†é¢‘"""
        if not self.video_save_enabled or episode_idx not in self.video_frames:
            return None
            
        frames = self.video_frames[episode_idx]
        if len(frames) > 0:
            video_path = self._generate_episode_video(frames, episode_idx, task_description)
            # æ¸…ç†å·²ä½¿ç”¨çš„å¸§
            del self.video_frames[episode_idx]
            return video_path
        return None

    def _generate_episode_video(self, episode_images, episode_idx, task_description):
        """ä»episodeå›¾åƒç”Ÿæˆè§†é¢‘"""
        try:
            import os
            import imageio
            from datetime import datetime
            
            # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
            debug_dir = os.path.join(os.getcwd(), "ript", "debug_images")
            video_dir = os.path.join(debug_dir, "videos")
            os.makedirs(video_dir, exist_ok=True)
            
            # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            task_str = str(task_description).replace(" ", "_").replace("/", "_")[:30]
            video_path = os.path.join(video_dir, f"episode_{episode_idx}_{timestamp}_{task_str}.mp4")
            
            # ç¡®ä¿æ‰€æœ‰å›¾åƒå°ºå¯¸ä¸€è‡´
            processed_images = []
            for img in episode_images:
                if isinstance(img, np.ndarray) and img.size > 0:
                    if img.ndim == 3 and img.shape[-1] == 3:
                        # è°ƒæ•´åˆ°ç»Ÿä¸€å°ºå¯¸
                        if img.shape[:2] != (224, 224):
                            try:
                                from skimage.transform import resize
                                img = resize(img, (224, 224), preserve_range=True).astype(np.uint8)
                            except ImportError:
                                import cv2
                                img = cv2.resize(img, (224, 224))
                        
                        # å›¾åƒå·²ç»åœ¨pi0_libero_utils.pyä¸­å¤„ç†è¿‡180åº¦æ—‹è½¬
                        # ä½†æ˜¯ä¸ºäº†è§†é¢‘æ˜¾ç¤ºæ­£ç¡®ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨åŸå§‹æ–¹å‘çš„å›¾åƒ
                        processed_images.append(img)  # ä½¿ç”¨åŸå§‹æ–¹å‘çš„å›¾åƒ
            
            if len(processed_images) >= 5:  # è‡³å°‘5å¸§æ‰ç”Ÿæˆè§†é¢‘
                # ä¿å­˜è§†é¢‘
                imageio.mimsave(video_path, processed_images, fps=10)
                print(f"âœ“ Episodeè§†é¢‘å·²ç”Ÿæˆ: {video_path} ({len(processed_images)} å¸§)")
                return video_path
            else:
                print(f"âš ï¸ Episode {episode_idx} å›¾åƒæ•°é‡ä¸è¶³ï¼Œè·³è¿‡è§†é¢‘ç”Ÿæˆ ({len(processed_images)} å¸§)")
                return None
                
        except Exception as e:
            print(f"ç”Ÿæˆepisodeè§†é¢‘æ—¶å‡ºé”™: {e}")
            return None