"""
OpenPI-RIPTé›†æˆè®­ç»ƒç³»ç»Ÿ
å°†æ‰€æœ‰ç»„ä»¶æ•´åˆåˆ°ç»Ÿä¸€çš„è®­ç»ƒæ¡†æ¶ä¸­
"""

import os
import time
import torch
import numpy as np
import traceback
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from tqdm import tqdm

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from pi0.modeling_pi0 import PI0Policy
from lerobot.configs.policies import PreTrainedConfig

# æˆ‘ä»¬çš„ç»„ä»¶å¯¼å…¥
from utils.openpi_ript_dataset_wrapper import create_openpi_ript_dataset
from utils.state_dimension_adapter import create_pi0_state_adapter
from ript.collectors.openpi_rollout_collector import create_openpi_rollout_collector
from ript.utils.advantage_processor import create_advantage_processor
from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter

# ç°æœ‰RIPTç»„ä»¶å¯¼å…¥
try:
    from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
    from pi0.ript.utils.rollout_stats_tracker import RolloutStatsTracker
    RIPT_ENV_AVAILABLE = True
except ImportError:
    print("âš ï¸ RIPTç¯å¢ƒç»„ä»¶ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    RIPT_ENV_AVAILABLE = False


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ï¼ˆä¸´æ—¶ä½¿ç”¨ï¼Œåç»­å°†æ›¿æ¢ä¸ºYAMLï¼‰"""
    # å®éªŒé…ç½®
    experiment_name: str = "openpi_ript_integration"
    output_dir: str = "./output/integrated_training"
    seed: int = 42
    
    # æ¨¡å‹é…ç½®
    checkpoint_path: str = "/home/dzb/.cache/openpi/openpi-assets/checkpoints/pi0_base_pytorch"
    freeze_vision_encoder: bool = True
    train_expert_only: bool = True
    train_state_proj: bool = True
    
    # æ•°æ®é…ç½®
    dataset_id: str = "ZibinDong/so100_grab_screwdriver"
    action_chunk_size: int = 50
    image_size: Tuple[int, int] = (224, 224)
    target_state_dim: int = 14
    
    # RIPTé…ç½®
    rloo_batch_size: int = 8
    demo_batch_size: int = 4
    enable_dynamic_sampling: bool = True
    enable_state_skipping: bool = True
    rollout_goal_per_step: int = 100
    rollout_skip_threshold: int = 3
    
    # ä¼˜åŠ¿å¤„ç†é…ç½®
    advantage_normalization: str = "standard"
    advantage_clipping: str = "symmetric"
    advantage_clip_value: float = 3.0
    advantage_negative_handling: str = "softplus"
    
    # CFGé…ç½®
    cfg_alpha: float = 0.1
    enable_cfg_safe_copy: bool = True
    
    # è®­ç»ƒé…ç½®
    num_train_steps: int = 1000
    gradient_accumulation_steps: int = 4
    learning_rate: float = 5e-5
    weight_decay: float = 1e-2
    gradient_clip_norm: float = 1.0
    enable_amp: bool = True
    
    # è®¾å¤‡é…ç½®
    device: str = "cuda"
    device_id: int = 0
    
    # ç›‘æ§é…ç½®
    enable_wandb: bool = False  # æš‚æ—¶å…³é—­ï¼Œåç»­å¼€å¯
    log_interval: int = 10
    eval_interval: int = 100
    save_interval: int = 500
    verbose: bool = True


class OpenPIRiptTrainer:
    """
    OpenPI-RIPTé›†æˆè®­ç»ƒå™¨
    
    æ•´åˆæ‰€æœ‰å·²å®Œæˆçš„ç»„ä»¶ï¼š
    1. OpenPIæ ‡å‡†æ•°æ®åŒ…è£…å™¨
    2. çŠ¶æ€ç»´åº¦é€‚é…å™¨
    3. RIPT Rolloutæ”¶é›†å™¨
    4. ä¼˜åŠ¿å€¼å¤„ç†å™¨
    5. CFGå®‰å…¨æ‹·è´æœºåˆ¶
    """
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.output_dir)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = self.output_dir / f"{config.experiment_name}_{timestamp}"
        self.run_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.policy = None
        self.optimizer = None
        self.cfg_adapter = None
        self.rollout_collector = None
        self.advantage_processor = None
        self.env_runner = None
        self.stats_tracker = None
        self.dataset = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_step = 0
        self.training_metrics = []
        self.best_success_rate = 0.0
        
        print(f"ğŸš€ OpenPI-RIPTè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   å®éªŒåç§°: {config.experiment_name}")
        print(f"   è¾“å‡ºç›®å½•: {self.run_dir}")
        print(f"   è®¾å¤‡: {self.device}")
        
    def setup_components(self):
        """è®¾ç½®å’Œåˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        print("ğŸ”§ è®¾ç½®è®­ç»ƒç»„ä»¶...")
        
        # 1. è®¾ç½®éšæœºç§å­
        self._setup_seeds()
        
        # 2. åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
        self._setup_model_and_optimizer()
        
        # 3. åˆå§‹åŒ–CFGé€‚é…å™¨
        self._setup_cfg_adapter()
        
        # 4. åˆå§‹åŒ–æ•°æ®ç»„ä»¶
        self._setup_data_components()
        
        # 5. åˆå§‹åŒ–ç¯å¢ƒç»„ä»¶
        self._setup_environment_components()
        
        # 6. åˆå§‹åŒ–ä¼˜åŠ¿å¤„ç†å™¨
        self._setup_advantage_processor()
        
        # 7. åˆå§‹åŒ–ç»Ÿè®¡è·Ÿè¸ªå™¨
        self._setup_stats_tracker()
        
        print("âœ… æ‰€æœ‰ç»„ä»¶è®¾ç½®å®Œæˆ")
    
    def _setup_seeds(self):
        """è®¾ç½®éšæœºç§å­"""
        torch.manual_seed(self.config.seed)
        np.random.seed(self.config.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(self.config.seed)
        print(f"   ğŸ² éšæœºç§å­: {self.config.seed}")
    
    def _setup_model_and_optimizer(self):
        """åˆå§‹åŒ–PI0æ¨¡å‹å’Œä¼˜åŒ–å™¨"""
        try:
            # åŠ è½½é¢„è®­ç»ƒé…ç½®
            model_config = PreTrainedConfig.from_pretrained(self.config.checkpoint_path)
            model_config.device = "cpu"  # å…ˆåœ¨CPUä¸ŠåŠ è½½
            model_config.freeze_vision_encoder = self.config.freeze_vision_encoder
            model_config.train_expert_only = self.config.train_expert_only
            model_config.train_state_proj = self.config.train_state_proj
            
            # åŠ è½½PI0æ¨¡å‹
            self.policy = PI0Policy.from_pretrained(
                self.config.checkpoint_path, 
                config=model_config
            )
            self.policy = self.policy.to(self.device)
            self.policy.train()
            
            # è®¾ç½®ä¼˜åŒ–å™¨
            self.optimizer = torch.optim.AdamW(
                self.policy.get_optim_params(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
                eps=1e-6
            )
            
            print(f"   âœ… PI0æ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"   âœ… ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ (lr={self.config.learning_rate})")
            
        except Exception as e:
            print(f"   âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _setup_cfg_adapter(self):
        """åˆå§‹åŒ–CFGé€‚é…å™¨"""
        try:
            # æ„é€ norm_statsè·¯å¾„
            norm_stats_path = Path(self.config.checkpoint_path) / "norm_stats.json"
            if not norm_stats_path.exists():
                norm_stats_path = None
                print(f"   âš ï¸ æœªæ‰¾åˆ°norm_stats.jsonï¼Œå°†ä½¿ç”¨é»˜è®¤å½’ä¸€åŒ–")
            
            self.cfg_adapter = PI0_CFG_Adapter(
                policy=self.policy,
                norm_stats_path=str(norm_stats_path) if norm_stats_path else None,
                windowing_mode="last",
                window_stride=10,
                max_windows_per_episode=1
            )
            
            print(f"   âœ… CFGé€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ CFGé€‚é…å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _setup_data_components(self):
        """åˆå§‹åŒ–æ•°æ®ç›¸å…³ç»„ä»¶"""
        try:
            # åˆ›å»ºOpenPIå…¼å®¹æ•°æ®é›†
            self.dataset = create_openpi_ript_dataset(
                repo_id=self.config.dataset_id,
                enable_ript=True,
                action_chunk_size=self.config.action_chunk_size,
                target_state_dim=self.config.target_state_dim,
                image_size=self.config.image_size
            )
            
            print(f"   âœ… OpenPIæ•°æ®é›†åˆ›å»ºå®Œæˆ")
            print(f"   âœ… æ•°æ®é›†å¤§å°: {len(self.dataset)}")
            
        except Exception as e:
            print(f"   âŒ æ•°æ®ç»„ä»¶è®¾ç½®å¤±è´¥: {e}")
            # å¦‚æœæ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿæ•°æ®é›†ç”¨äºæµ‹è¯•
            print(f"   ğŸ”§ åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†ç”¨äºæµ‹è¯•...")
            self.dataset = None
    
    def _setup_environment_components(self):
        """åˆå§‹åŒ–ç¯å¢ƒç›¸å…³ç»„ä»¶"""
        try:
            if RIPT_ENV_AVAILABLE:
                # ä½¿ç”¨çœŸå®çš„ç¯å¢ƒrunner
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„RIPTç¯å¢ƒé…ç½®
                print(f"   ğŸ”„ RIPTç¯å¢ƒç»„ä»¶å¯ç”¨ï¼Œä½†éœ€è¦å…·ä½“é…ç½®")
                self.env_runner = None  # æš‚æ—¶è®¾ä¸ºNoneï¼Œéœ€è¦å…·ä½“ç¯å¢ƒé…ç½®
            else:
                print(f"   âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿç¯å¢ƒæ¨¡å¼")
                self.env_runner = None
            
            # åˆ›å»ºrolloutæ”¶é›†å™¨
            rollout_config = {
                'rloo_batch_size': self.config.rloo_batch_size,
                'action_chunk_size': self.config.action_chunk_size,
                'enable_dynamic_sampling': self.config.enable_dynamic_sampling,
                'enable_state_skipping': self.config.enable_state_skipping,
                'image_size': self.config.image_size,
                'target_state_dim': self.config.target_state_dim,
                'action_dim': 7,  # æ ‡å‡†æœºå™¨äººåŠ¨ä½œç»´åº¦
                'rollout_skip_threshold': self.config.rollout_skip_threshold,
                'rollout_stats_path': str(self.run_dir / 'rollout_stats.json'),
                'task_id': 0,
            }
            
            self.rollout_collector = create_openpi_rollout_collector(
                config_dict=rollout_config,
                env_runner=self.env_runner,
                stats_tracker=None  # ç¨åè®¾ç½®
            )
            
            print(f"   âœ… Rolloutæ”¶é›†å™¨åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ ç¯å¢ƒç»„ä»¶è®¾ç½®å¤±è´¥: {e}")
            traceback.print_exc()
    
    def _setup_advantage_processor(self):
        """åˆå§‹åŒ–ä¼˜åŠ¿å€¼å¤„ç†å™¨"""
        try:
            self.advantage_processor = create_advantage_processor(
                normalization=self.config.advantage_normalization,
                clipping=self.config.advantage_clipping,
                clip_value=self.config.advantage_clip_value,
                negative_handling=self.config.advantage_negative_handling,
                verbose=self.config.verbose
            )
            
            print(f"   âœ… ä¼˜åŠ¿å¤„ç†å™¨åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ ä¼˜åŠ¿å¤„ç†å™¨è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _setup_stats_tracker(self):
        """åˆå§‹åŒ–ç»Ÿè®¡è·Ÿè¸ªå™¨"""
        try:
            if RIPT_ENV_AVAILABLE:
                self.stats_tracker = RolloutStatsTracker(
                    rollout_skip_threshold=self.config.rollout_skip_threshold,
                    stats_path=str(self.run_dir / 'rollout_stats.json')
                )
                print(f"   âœ… ç»Ÿè®¡è·Ÿè¸ªå™¨åˆ›å»ºå®Œæˆ")
            else:
                print(f"   âš ï¸ ç»Ÿè®¡è·Ÿè¸ªå™¨ä¸å¯ç”¨ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰")
                self.stats_tracker = None
                
        except Exception as e:
            print(f"   âŒ ç»Ÿè®¡è·Ÿè¸ªå™¨è®¾ç½®å¤±è´¥: {e}")
            self.stats_tracker = None
    
    def collect_and_process_data(self) -> Optional[Dict[str, Any]]:
        """
        æ”¶é›†å’Œå¤„ç†è®­ç»ƒæ•°æ®
        
        Returns:
            å¤„ç†åçš„è®­ç»ƒæ‰¹æ¬¡ï¼ŒåŒ…å«OpenPIæ ¼å¼æ•°æ®å’Œå¤„ç†è¿‡çš„ä¼˜åŠ¿å€¼
        """
        try:
            if self.config.verbose:
                print(f"ğŸ”„ æ”¶é›†è®­ç»ƒæ•°æ® (æ­¥éª¤ {self.current_step + 1})")
            
            # 1. ä½¿ç”¨rolloutæ”¶é›†å™¨æ”¶é›†OpenPIæ ¼å¼æ•°æ®
            if self.rollout_collector and self.env_runner:
                # çœŸå®ç¯å¢ƒæ•°æ®æ”¶é›†
                openpi_samples = self.rollout_collector.collect_rollouts_openpi_format(
                    task_name=f"task_step_{self.current_step}",
                    num_rollouts=self.config.rloo_batch_size
                )
                
                if not openpi_samples:
                    print("âš ï¸ æœªæ”¶é›†åˆ°æœ‰æ•ˆæ•°æ®")
                    return None
                    
            else:
                # æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆï¼ˆç”¨äºæµ‹è¯•ï¼‰
                openpi_samples = self._generate_mock_training_data()
                if self.config.verbose:
                    print("ğŸ”§ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            
            # 2. æå–å¥–åŠ±å¹¶è®¡ç®—RLOOä¼˜åŠ¿
            rewards = [sample.get("rollout_reward", np.random.random()) for sample in openpi_samples]
            advantages = self._compute_rloo_advantages(rewards)
            
            # 3. ä½¿ç”¨ä¼˜åŠ¿å¤„ç†å™¨å¤„ç†ä¼˜åŠ¿å€¼
            processed_advantages = self.advantage_processor.process_advantages(
                advantages,
                batch_info={"step": self.current_step}
            )
            
            # 4. æ›´æ–°æ ·æœ¬çš„ä¼˜åŠ¿å€¼
            for i, sample in enumerate(openpi_samples):
                if i < len(processed_advantages):
                    sample["advantages"] = torch.full_like(
                        sample["action"][:, 0], 
                        processed_advantages[i].item()
                    )
            
            # 5. æ„é€ è®­ç»ƒæ‰¹æ¬¡
            training_batch = self._prepare_training_batch(openpi_samples, processed_advantages)
            
            if self.config.verbose:
                print(f"   âœ… æ•°æ®æ”¶é›†å®Œæˆ: {len(openpi_samples)} æ ·æœ¬")
                print(f"   âœ… ä¼˜åŠ¿å¤„ç†å®Œæˆ: èŒƒå›´ [{processed_advantages.min():.4f}, {processed_advantages.max():.4f}]")
            
            return training_batch
            
        except Exception as e:
            print(f"âŒ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            traceback.print_exc()
            return None
    
    def _generate_mock_training_data(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        mock_samples = []
        
        for i in range(self.config.rloo_batch_size):
            sample = {
                "image": {
                    "base_0_rgb": torch.randint(0, 255, self.config.image_size + (3,), dtype=torch.uint8),
                    "left_wrist_0_rgb": torch.randint(0, 255, self.config.image_size + (3,), dtype=torch.uint8)
                },
                "state": torch.randn(self.config.target_state_dim),
                "action": torch.randn(self.config.action_chunk_size, 7),
                "action_is_pad": torch.zeros(self.config.action_chunk_size, dtype=torch.bool),
                "prompt": f"mock_task_{i}",
                "advantages": torch.randn(self.config.action_chunk_size),
                "init_hash": f"mock_hash_{i}_{self.current_step}",
                "rollout_success": np.random.random() > 0.3,
                "rollout_reward": np.random.random(),
            }
            mock_samples.append(sample)
        
        return mock_samples
    
    def _compute_rloo_advantages(self, rewards: List[float]) -> torch.Tensor:
        """
        è®¡ç®—RLOO (Reward-Ranked Leave-One-Out) ä¼˜åŠ¿
        
        Args:
            rewards: å¥–åŠ±åˆ—è¡¨
            
        Returns:
            RLOOä¼˜åŠ¿å¼ é‡
        """
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
        num_samples = len(rewards)
        
        if num_samples <= 1:
            return torch.zeros_like(rewards_tensor)
        
        # RLOOå…¬å¼ï¼šæ¯ä¸ªæ ·æœ¬çš„ä¼˜åŠ¿ = è‡ªå·±çš„å¥–åŠ± - å…¶ä»–æ ·æœ¬çš„å¹³å‡å¥–åŠ±
        advantages = torch.zeros_like(rewards_tensor)
        
        for i in range(num_samples):
            # è®¡ç®—é™¤è‡ªå·±å¤–å…¶ä»–æ ·æœ¬çš„å¹³å‡å¥–åŠ±
            other_rewards = torch.cat([rewards_tensor[:i], rewards_tensor[i+1:]])
            baseline = other_rewards.mean()
            advantages[i] = rewards_tensor[i] - baseline
        
        return advantages
    
    def _prepare_training_batch(
        self, 
        openpi_samples: List[Dict[str, Any]], 
        advantages: torch.Tensor
    ) -> Dict[str, Any]:
        """å°†OpenPIæ ·æœ¬è½¬æ¢ä¸ºè®­ç»ƒæ‰¹æ¬¡"""
        batch_size = len(openpi_samples)
        
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        images_base = []
        images_wrist = []
        states = []
        actions = []
        action_is_pad = []
        prompts = []
        
        for sample in openpi_samples:
            images_base.append(sample["image"]["base_0_rgb"])
            images_wrist.append(sample["image"]["left_wrist_0_rgb"])
            states.append(sample["state"])
            actions.append(sample["action"])
            action_is_pad.append(sample["action_is_pad"])
            prompts.append(sample["prompt"])
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        training_batch = {
            "image": {
                "base_0_rgb": torch.stack(images_base).to(self.device),
                "left_wrist_0_rgb": torch.stack(images_wrist).to(self.device)
            },
            "state": torch.stack(states).to(self.device),
            "action": torch.stack(actions).to(self.device),
            "action_is_pad": torch.stack(action_is_pad).to(self.device),
            "prompt": prompts,
            "advantages": advantages.to(self.device)
        }
        
        return training_batch
    
    def train_step(self, training_batch: Dict[str, Any]) -> Dict[str, float]:
        """
        æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤
        
        Args:
            training_batch: è®­ç»ƒæ‰¹æ¬¡æ•°æ®
            
        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        step_start_time = time.time()
        
        try:
            # å‡†å¤‡ä¼˜åŠ¿å€¼
            advantages = training_batch["advantages"]
            
            # ä½¿ç”¨CFGé€‚é…å™¨è®¡ç®—æŸå¤±ï¼ˆåŒ…å«å®‰å…¨æ‹·è´ï¼‰
            with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=self.config.enable_amp):
                # å°†OpenPIæ‰¹æ¬¡è½¬æ¢ä¸ºCFGé€‚é…å™¨æœŸæœ›çš„æ ¼å¼
                episodes = self._batch_to_episodes(training_batch)
                
                # ä½¿ç”¨CFGé€‚é…å™¨çš„å®‰å…¨æŸå¤±è®¡ç®—
                loss = self.cfg_adapter.compute_weighted_loss(episodes, advantages, self.device)
            
            # æ¢¯åº¦æ›´æ–°
            if self.config.gradient_accumulation_steps > 1:
                loss = loss / self.config.gradient_accumulation_steps
            
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.policy.parameters(), 
                self.config.gradient_clip_norm
            )
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            if (self.current_step + 1) % self.config.gradient_accumulation_steps == 0:
                self.optimizer.step()
                self.optimizer.zero_grad()
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            step_time = time.time() - step_start_time
            
            metrics = {
                "loss": loss.item() * self.config.gradient_accumulation_steps,
                "grad_norm": grad_norm.item(),
                "advantages_mean": advantages.mean().item(),
                "advantages_std": advantages.std().item(),
                "step_time": step_time,
                "learning_rate": self.optimizer.param_groups[0]['lr'],
            }
            
            if self.config.verbose and (self.current_step + 1) % self.config.log_interval == 0:
                print(f"   æ­¥éª¤ {self.current_step + 1}: æŸå¤±={metrics['loss']:.6f}, "
                      f"æ¢¯åº¦èŒƒæ•°={metrics['grad_norm']:.4f}, "
                      f"æ—¶é—´={metrics['step_time']:.2f}s")
            
            return metrics
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
            traceback.print_exc()
            return {"loss": float('inf'), "error": str(e)}
    
    def _batch_to_episodes(self, batch: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å°†æ‰¹æ¬¡æ•°æ®è½¬æ¢ä¸ºCFGé€‚é…å™¨æœŸæœ›çš„episodeæ ¼å¼"""
        batch_size = batch["state"].shape[0]
        episodes = []
        
        for i in range(batch_size):
            episode = {
                'observations': [{
                    'agentview_image': batch["image"]["base_0_rgb"][i].cpu().numpy().transpose(1, 2, 0),
                    'robot0_eye_in_hand_image': batch["image"]["left_wrist_0_rgb"][i].cpu().numpy().transpose(1, 2, 0),
                    'robot0_eef_pos': batch["state"][i][:3].cpu().numpy(),
                    'robot0_eef_quat': [0, 0, 0, 1],  # é»˜è®¤å››å…ƒæ•°
                    'robot0_gripper_qpos': batch["state"][i][-2:].cpu().numpy(),
                }],
                'actions': [batch["action"][i].cpu().numpy()],
                'task': batch["prompt"][i] if isinstance(batch["prompt"], list) else "default_task",
                'success': True,  # å‡è®¾æˆåŠŸ
                'total_reward': batch["advantages"][i].mean().item(),
            }
            episodes.append(episode)
        
        return episodes
    
    def run_training(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒå¾ªç¯")
        print(f"   æ€»æ­¥æ•°: {self.config.num_train_steps}")
        print(f"   æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.config.gradient_accumulation_steps}")
        print(f"   ä¿å­˜é—´éš”: {self.config.save_interval}")
        print()
        
        # è®¾ç½®è¿›åº¦æ¡
        progress_bar = tqdm(
            range(self.config.num_train_steps),
            desc="è®­ç»ƒè¿›åº¦",
            unit="step",
            dynamic_ncols=True
        )
        
        try:
            for step in progress_bar:
                self.current_step = step
                
                # 1. æ•°æ®æ”¶é›†å’Œå¤„ç†
                training_batch = self.collect_and_process_data()
                
                if training_batch is None:
                    print(f"âš ï¸ æ­¥éª¤ {step + 1}: è·³è¿‡ï¼ˆæ•°æ®æ”¶é›†å¤±è´¥ï¼‰")
                    continue
                
                # 2. è®­ç»ƒæ­¥éª¤
                metrics = self.train_step(training_batch)
                
                # 3. è®°å½•æŒ‡æ ‡
                self.training_metrics.append(metrics)
                
                # 4. æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'Loss': f"{metrics.get('loss', 0):.4f}",
                    'Adv_Mean': f"{metrics.get('advantages_mean', 0):.3f}",
                    'Time': f"{metrics.get('step_time', 0):.2f}s"
                })
                
                # 5. å®šæœŸä¿å­˜
                if (step + 1) % self.config.save_interval == 0:
                    self.save_checkpoint(step + 1)
                
                # 6. å†…å­˜æ¸…ç†
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
            self.save_final_results()
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­")
            self.save_checkpoint(self.current_step, is_interrupt=True)
            
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
            traceback.print_exc()
            
        finally:
            progress_bar.close()
    
    def save_checkpoint(self, step: int, is_interrupt: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_name = f"checkpoint_step_{step}"
        if is_interrupt:
            checkpoint_name += "_interrupted"
        
        checkpoint_path = self.run_dir / f"{checkpoint_name}.pt"
        
        try:
            checkpoint = {
                'step': step,
                'model_state_dict': self.policy.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config,
                'training_metrics': self.training_metrics,
                'advantage_processor_stats': self.advantage_processor.get_processing_stats(),
            }
            
            if self.rollout_collector:
                checkpoint['rollout_collector_stats'] = self.rollout_collector.get_collection_stats()
            
            torch.save(checkpoint, checkpoint_path)
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        results_path = self.run_dir / "training_results.json"
        
        try:
            import json
            
            results = {
                'config': {
                    'experiment_name': self.config.experiment_name,
                    'num_train_steps': self.config.num_train_steps,
                    'final_step': self.current_step + 1,
                    'model_checkpoint': self.config.checkpoint_path,
                },
                'final_metrics': self.training_metrics[-10:] if self.training_metrics else [],
                'component_stats': {
                    'advantage_processor': self.advantage_processor.get_processing_stats(),
                },
                'summary': {
                    'total_steps': len(self.training_metrics),
                    'average_loss': np.mean([m.get('loss', 0) for m in self.training_metrics[-100:]]) if self.training_metrics else 0,
                    'final_loss': self.training_metrics[-1].get('loss', 0) if self.training_metrics else 0,
                }
            }
            
            if self.rollout_collector:
                results['component_stats']['rollout_collector'] = self.rollout_collector.get_collection_stats()
            
            with open(results_path, 'w') as f:
                json.dump(results, f, indent=2)
            
            print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜: {results_path}")
            
            # æ‰“å°ç»Ÿè®¡æ‘˜è¦
            self.print_training_summary()
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def print_training_summary(self):
        """æ‰“å°è®­ç»ƒæ‘˜è¦"""
        if not self.training_metrics:
            return
        
        print(f"\nğŸ“Š è®­ç»ƒæ‘˜è¦:")
        print(f"   æ€»è®­ç»ƒæ­¥æ•°: {len(self.training_metrics)}")
        
        # æŸå¤±ç»Ÿè®¡
        losses = [m.get('loss', 0) for m in self.training_metrics if 'loss' in m]
        if losses:
            print(f"   å¹³å‡æŸå¤±: {np.mean(losses):.6f}")
            print(f"   æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}")
            print(f"   æœ€ä½³æŸå¤±: {min(losses):.6f}")
        
        # ä¼˜åŠ¿å€¼ç»Ÿè®¡
        print(f"\nğŸ“ˆ ç»„ä»¶ç»Ÿè®¡:")
        self.advantage_processor.print_stats()
        
        if self.rollout_collector:
            self.rollout_collector.print_stats()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ OpenPI-RIPTé›†æˆè®­ç»ƒç³»ç»Ÿ")
    print("="*50)
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = OpenPIRiptTrainer(config)
    
    try:
        # è®¾ç½®ç»„ä»¶
        trainer.setup_components()
        
        # è¿è¡Œè®­ç»ƒ
        trainer.run_training()
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()
