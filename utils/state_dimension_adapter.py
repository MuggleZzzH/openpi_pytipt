"""
çŠ¶æ€ç»´åº¦é€‚é…å™¨
å¤„ç†ä¸åŒæ•°æ®æºä¸PI0æ¨¡å‹ä¹‹é—´çš„çŠ¶æ€ç»´åº¦å¯¹é½é—®é¢˜
"""

import torch
import numpy as np
from typing import Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
import warnings


@dataclass
class StateDimensionConfig:
    """çŠ¶æ€ç»´åº¦é…ç½®"""
    target_state_dim: int  # ç›®æ ‡çŠ¶æ€ç»´åº¦ï¼ˆPI0æ¨¡å‹æœŸæœ›çš„ç»´åº¦ï¼‰
    source_state_dim: Optional[int] = None  # æºçŠ¶æ€ç»´åº¦ï¼ˆæ•°æ®é›†å®é™…ç»´åº¦ï¼‰
    padding_mode: str = "zero"  # å¡«å……æ¨¡å¼: "zero", "repeat_last", "mean"
    truncation_mode: str = "first"  # æˆªæ–­æ¨¡å¼: "first", "last", "middle"
    normalize_after_align: bool = False  # å¯¹é½åæ˜¯å¦å½’ä¸€åŒ–
    enable_dimension_check: bool = True  # æ˜¯å¦å¯ç”¨ç»´åº¦æ£€æŸ¥
    
    def __post_init__(self):
        assert self.padding_mode in ["zero", "repeat_last", "mean"], f"ä¸æ”¯æŒçš„å¡«å……æ¨¡å¼: {self.padding_mode}"
        assert self.truncation_mode in ["first", "last", "middle"], f"ä¸æ”¯æŒçš„æˆªæ–­æ¨¡å¼: {self.truncation_mode}"


class StateDimensionAdapter:
    """
    çŠ¶æ€ç»´åº¦é€‚é…å™¨
    
    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨æ£€æµ‹æºçŠ¶æ€ç»´åº¦ä¸ç›®æ ‡ç»´åº¦çš„å·®å¼‚
    2. æ™ºèƒ½å¡«å……æˆ–æˆªæ–­çŠ¶æ€å‘é‡
    3. æä¾›å¤šç§å¯¹é½ç­–ç•¥
    4. è®°å½•ç»´åº¦è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
    """
    
    def __init__(self, config: StateDimensionConfig):
        self.config = config
        self.conversion_stats = {
            "total_conversions": 0,
            "expansions": 0,
            "truncations": 0,
            "no_change": 0,
            "dimension_history": {}
        }
        self._warned_dimensions = set()
        
        print(f"ğŸ”§ StateDimensionAdapter åˆå§‹åŒ–")
        print(f"   - ç›®æ ‡ç»´åº¦: {config.target_state_dim}")
        print(f"   - å¡«å……æ¨¡å¼: {config.padding_mode}")
        print(f"   - æˆªæ–­æ¨¡å¼: {config.truncation_mode}")
        
    def align_state_dimension(
        self, 
        state: torch.Tensor, 
        batch_idx: Optional[int] = None
    ) -> torch.Tensor:
        """
        å¯¹é½çŠ¶æ€ç»´åº¦åˆ°ç›®æ ‡ç»´åº¦
        
        Args:
            state: è¾“å…¥çŠ¶æ€å¼ é‡ (..., state_dim)
            batch_idx: æ‰¹æ¬¡ç´¢å¼•ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
            
        Returns:
            å¯¹é½åçš„çŠ¶æ€å¼ é‡ (..., target_state_dim)
        """
        if not isinstance(state, torch.Tensor):
            state = torch.tensor(state, dtype=torch.float32)
            
        original_shape = state.shape
        current_dim = original_shape[-1]
        target_dim = self.config.target_state_dim
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.conversion_stats["total_conversions"] += 1
        dim_key = f"{current_dim}->{target_dim}"
        self.conversion_stats["dimension_history"][dim_key] = \
            self.conversion_stats["dimension_history"].get(dim_key, 0) + 1
        
        # å¦‚æœç»´åº¦å·²ç»åŒ¹é…ï¼Œç›´æ¥è¿”å›
        if current_dim == target_dim:
            self.conversion_stats["no_change"] += 1
            return state
            
        # ç»´åº¦æ£€æŸ¥å’Œè­¦å‘Š
        if self.config.enable_dimension_check and current_dim not in self._warned_dimensions:
            if current_dim < target_dim:
                print(f"ğŸ”§ çŠ¶æ€ç»´åº¦æ‰©å±•: {current_dim} -> {target_dim} (å¡«å……æ¨¡å¼: {self.config.padding_mode})")
            else:
                print(f"âš ï¸ çŠ¶æ€ç»´åº¦æˆªæ–­: {current_dim} -> {target_dim} (æˆªæ–­æ¨¡å¼: {self.config.truncation_mode})")
            self._warned_dimensions.add(current_dim)
        
        if current_dim < target_dim:
            # éœ€è¦æ‰©å±•ç»´åº¦
            aligned_state = self._expand_state_dimension(state, current_dim, target_dim)
            self.conversion_stats["expansions"] += 1
        else:
            # éœ€è¦æˆªæ–­ç»´åº¦
            aligned_state = self._truncate_state_dimension(state, current_dim, target_dim)
            self.conversion_stats["truncations"] += 1
            
        # å¯é€‰çš„åå¤„ç†å½’ä¸€åŒ–
        if self.config.normalize_after_align:
            aligned_state = self._normalize_state(aligned_state)
            
        # éªŒè¯è¾“å‡ºç»´åº¦
        assert aligned_state.shape[-1] == target_dim, \
            f"ç»´åº¦å¯¹é½å¤±è´¥: æœŸæœ›{target_dim}, å®é™…{aligned_state.shape[-1]}"
            
        return aligned_state
    
    def _expand_state_dimension(
        self, 
        state: torch.Tensor, 
        current_dim: int, 
        target_dim: int
    ) -> torch.Tensor:
        """æ‰©å±•çŠ¶æ€ç»´åº¦"""
        expand_size = target_dim - current_dim
        
        if self.config.padding_mode == "zero":
            # é›¶å¡«å……
            padding_shape = list(state.shape)
            padding_shape[-1] = expand_size
            padding = torch.zeros(padding_shape, dtype=state.dtype, device=state.device)
            
        elif self.config.padding_mode == "repeat_last":
            # é‡å¤æœ€åä¸€ä¸ªå…ƒç´ 
            last_element = state[..., -1:].expand(*state.shape[:-1], expand_size)
            padding = last_element
            
        elif self.config.padding_mode == "mean":
            # ä½¿ç”¨å‡å€¼å¡«å……
            state_mean = state.mean(dim=-1, keepdim=True)
            padding = state_mean.expand(*state.shape[:-1], expand_size)
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¡«å……æ¨¡å¼: {self.config.padding_mode}")
            
        return torch.cat([state, padding], dim=-1)
    
    def _truncate_state_dimension(
        self, 
        state: torch.Tensor, 
        current_dim: int, 
        target_dim: int
    ) -> torch.Tensor:
        """æˆªæ–­çŠ¶æ€ç»´åº¦"""
        if self.config.truncation_mode == "first":
            # ä¿ç•™å‰target_dimä¸ªç»´åº¦
            return state[..., :target_dim]
            
        elif self.config.truncation_mode == "last":
            # ä¿ç•™åtarget_dimä¸ªç»´åº¦
            return state[..., -target_dim:]
            
        elif self.config.truncation_mode == "middle":
            # ä¿ç•™ä¸­é—´target_dimä¸ªç»´åº¦
            start_idx = (current_dim - target_dim) // 2
            end_idx = start_idx + target_dim
            return state[..., start_idx:end_idx]
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æˆªæ–­æ¨¡å¼: {self.config.truncation_mode}")
    
    def _normalize_state(self, state: torch.Tensor) -> torch.Tensor:
        """å½’ä¸€åŒ–çŠ¶æ€"""
        # ç®€å•çš„æ ‡å‡†åŒ–ï¼šå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
        state_mean = state.mean(dim=-1, keepdim=True)
        state_std = state.std(dim=-1, keepdim=True)
        return (state - state_mean) / (state_std + 1e-8)
    
    def detect_state_dimension(self, dataset_or_sample: Union[torch.utils.data.Dataset, Dict[str, Any]]) -> int:
        """
        è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†æˆ–æ ·æœ¬çš„çŠ¶æ€ç»´åº¦
        
        Args:
            dataset_or_sample: æ•°æ®é›†å¯¹è±¡æˆ–æ ·æœ¬å­—å…¸
            
        Returns:
            æ£€æµ‹åˆ°çš„çŠ¶æ€ç»´åº¦
        """
        if hasattr(dataset_or_sample, '__getitem__'):
            # æ•°æ®é›†å¯¹è±¡
            sample = dataset_or_sample[0]
            state = sample["state"]
        else:
            # æ ·æœ¬å­—å…¸
            state = dataset_or_sample["state"]
            
        if isinstance(state, (list, np.ndarray)):
            state = torch.tensor(state)
            
        detected_dim = state.shape[-1]
        print(f"ğŸ” æ£€æµ‹åˆ°çŠ¶æ€ç»´åº¦: {detected_dim}")
        return detected_dim
    
    def update_source_dimension(self, source_dim: int):
        """æ›´æ–°æºçŠ¶æ€ç»´åº¦é…ç½®"""
        self.config.source_state_dim = source_dim
        print(f"ğŸ”„ æ›´æ–°æºçŠ¶æ€ç»´åº¦: {source_dim}")
    
    def get_conversion_stats(self) -> Dict[str, Any]:
        """è·å–ç»´åº¦è½¬æ¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.conversion_stats,
            "config": {
                "target_state_dim": self.config.target_state_dim,
                "source_state_dim": self.config.source_state_dim,
                "padding_mode": self.config.padding_mode,
                "truncation_mode": self.config.truncation_mode,
            }
        }
    
    def print_stats(self):
        """æ‰“å°è½¬æ¢ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.conversion_stats
        print("\nğŸ“Š çŠ¶æ€ç»´åº¦è½¬æ¢ç»Ÿè®¡:")
        print(f"   æ€»è½¬æ¢æ¬¡æ•°: {stats['total_conversions']}")
        print(f"   æ‰©å±•æ¬¡æ•°: {stats['expansions']}")
        print(f"   æˆªæ–­æ¬¡æ•°: {stats['truncations']}")
        print(f"   æ— å˜åŒ–æ¬¡æ•°: {stats['no_change']}")
        
        if stats['dimension_history']:
            print("   ç»´åº¦è½¬æ¢å†å²:")
            for conversion, count in stats['dimension_history'].items():
                print(f"     {conversion}: {count}æ¬¡")


def create_pi0_state_adapter(
    target_state_dim: int,
    source_state_dim: Optional[int] = None,
    **kwargs
) -> StateDimensionAdapter:
    """
    åˆ›å»ºPI0ä¸“ç”¨çš„çŠ¶æ€ç»´åº¦é€‚é…å™¨
    
    Args:
        target_state_dim: PI0æ¨¡å‹æœŸæœ›çš„çŠ¶æ€ç»´åº¦
        source_state_dim: æ•°æ®é›†çš„çŠ¶æ€ç»´åº¦ï¼ˆå¯é€‰ï¼‰
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„çŠ¶æ€ç»´åº¦é€‚é…å™¨
    """
    config = StateDimensionConfig(
        target_state_dim=target_state_dim,
        source_state_dim=source_state_dim,
        **kwargs
    )
    
    adapter = StateDimensionAdapter(config)
    
    if source_state_dim is not None and source_state_dim != target_state_dim:
        print(f"âš ï¸ æ£€æµ‹åˆ°ç»´åº¦ä¸åŒ¹é…: æº({source_state_dim}) vs ç›®æ ‡({target_state_dim})")
        
    return adapter


def batch_align_states(
    batch: Dict[str, Any], 
    adapter: StateDimensionAdapter
) -> Dict[str, Any]:
    """
    æ‰¹é‡å¯¹é½çŠ¶æ€ç»´åº¦
    
    Args:
        batch: åŒ…å«stateå­—æ®µçš„æ‰¹æ¬¡æ•°æ®
        adapter: çŠ¶æ€ç»´åº¦é€‚é…å™¨
        
    Returns:
        å¯¹é½åçš„æ‰¹æ¬¡æ•°æ®
    """
    if "state" in batch:
        batch["state"] = adapter.align_state_dimension(batch["state"])
    
    return batch


# ä¾¿æ·å‡½æ•°
def auto_align_state_for_pi0(state: torch.Tensor, target_dim: int = 14) -> torch.Tensor:
    """
    ä¸ºPI0æ¨¡å‹è‡ªåŠ¨å¯¹é½çŠ¶æ€ç»´åº¦çš„ä¾¿æ·å‡½æ•°
    
    Args:
        state: è¾“å…¥çŠ¶æ€
        target_dim: ç›®æ ‡ç»´åº¦ï¼ˆé»˜è®¤14ï¼Œå¯æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´ï¼‰
        
    Returns:
        å¯¹é½åçš„çŠ¶æ€
    """
    config = StateDimensionConfig(
        target_state_dim=target_dim,
        padding_mode="zero",
        truncation_mode="first"
    )
    adapter = StateDimensionAdapter(config)
    return adapter.align_state_dimension(state)
