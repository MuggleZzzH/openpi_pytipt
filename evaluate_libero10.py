#!/usr/bin/env python3
"""
LIBERO-10 Checkpoint Evaluation Script
=====================================

ç”¨äºè¯„ä¼°PI0 checkpointåœ¨LIBERO_GOALå…¨éƒ¨10ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½
å€Ÿé‰´ript-vla_copyçš„è¯„ä¼°æ¡†æ¶ï¼Œè®¡ç®—å‡†ç¡®çš„æˆåŠŸç‡æŒ‡æ ‡

ä½¿ç”¨æ–¹æ³•:
    cd /zhaohan/ZJH/openpi_pytorch
    python evaluate_libero10.py --config_path pi0/ript/config/libero10_eval.yaml
    
    # è‡ªå®šä¹‰checkpointè·¯å¾„
    python evaluate_libero10.py --config_path pi0/ript/config/libero10_eval.yaml --checkpoint_path /path/to/your/checkpoint
"""

import os
import sys
import json
import time
import yaml
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
from collections import defaultdict

import torch
import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print("=== LIBERO-10 Checkpoint è¯„ä¼°è„šæœ¬ ===")
print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print()

# å¯¼å…¥å¿…è¦æ¨¡å—
try:
    from pi0.modeling_pi0 import PI0Policy
    from pi0.ript.reward_function import BinarySuccessReward
    from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
    print("âœ“ æˆåŠŸå¯¼å…¥æ‰€æœ‰å¿…è¦æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

class LIBERO10Evaluator:
    """LIBERO-10ä»»åŠ¡è¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str, checkpoint_path: str = None):
        self.config = self._load_config(config_path)
        self.checkpoint_path = checkpoint_path or self.config['policy_path']
        self.output_dir = Path(self.config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯„ä¼°é…ç½®
        self.eval_config = self.config['evaluation']
        self.rollouts_per_task = self.eval_config['rollouts_per_task']
        
        # ä»»åŠ¡åˆ—è¡¨
        self.tasks = self.config['task']['task_names_to_use']
        print(f"ğŸ“‹ å°†è¯„ä¼° {len(self.tasks)} ä¸ªLIBERO_GOALä»»åŠ¡")
        for i, task in enumerate(self.tasks):
            print(f"  {i+1:2d}. {task}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.policy = None
        self.env_runner = None
        self.results = defaultdict(list)
        
        # åˆ›å»ºè¯¦ç»†è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_dir = self.output_dir / f"eval_session_{timestamp}"
        self.session_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜è¯„ä¼°é…ç½®
        with open(self.session_dir / "eval_config.yaml", 'w') as f:
            yaml.dump(self.config, f, default_flow_style=False)
            
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½è¯„ä¼°é…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def load_policy(self):
        """åŠ è½½PI0ç­–ç•¥"""
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½PI0æ¨¡å‹: {self.checkpoint_path}")
        
        try:
            # ä½¿ç”¨æ­£ç¡®çš„PI0PolicyåŠ è½½æ–¹å¼
            self.policy = PI0Policy.from_pretrained(self.checkpoint_path)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.policy.eval()
            
            # ç§»åŠ¨åˆ°GPU
            if torch.cuda.is_available():
                self.policy = self.policy.cuda()
                print(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name()}")
            else:
                print("âš ï¸ ä½¿ç”¨CPUè¿è¡Œï¼Œå¯èƒ½è¾ƒæ…¢")
                
            print("âœ“ PI0ç­–ç•¥åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿checkpointè·¯å¾„æ­£ç¡®ä¸”åŒ…å«æ‰€æœ‰å¿…éœ€æ–‡ä»¶:")
            print(f"   - {self.checkpoint_path}/config.json")
            print(f"   - {self.checkpoint_path}/model.safetensors")
            print(f"   - {self.checkpoint_path}/tokenizeré…ç½®æ–‡ä»¶")
            raise
    
    def setup_env_runner(self):
        """è®¾ç½®ç¯å¢ƒè¿è¡Œå™¨"""
        print("ğŸ”„ æ­£åœ¨è®¾ç½®ç¯å¢ƒè¿è¡Œå™¨...")
        
        try:
            # åˆ›å»ºé…ç½®å¯¹è±¡ä»¥ä¼ é€’CFGå‚æ•°
            from types import SimpleNamespace
            runner_config = SimpleNamespace()
            runner_config.cfg_guidance_scale = self.config.get('algo', {}).get('cfg_guidance_scale', 3.0)
            
            self.env_runner = LIBEROEnvRunner(
                benchmark_name=self.config['task']['benchmark_name'],
                num_parallel_envs=self.config['task']['num_parallel_envs'],
                max_episode_length=self.config['task']['max_episode_length'],
                config=runner_config,  # ä¼ é€’CFGé…ç½®
                video_dir=str(self.session_dir / "videos")  # è§†é¢‘ä¿å­˜ç›®å½•
            )
            print("âœ“ ç¯å¢ƒè¿è¡Œå™¨è®¾ç½®æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ è®¾ç½®ç¯å¢ƒè¿è¡Œå™¨å¤±è´¥: {e}")
            raise
    
    def evaluate_single_task(self, task_name: str) -> Dict:
        """è¯„ä¼°å•ä¸ªä»»åŠ¡"""
        print(f"\nğŸ¯ è¯„ä¼°ä»»åŠ¡: {task_name}")
        
        task_results = {
            'task_name': task_name,
            'success_count': 0,
            'total_rollouts': self.rollouts_per_task,
            'episode_rewards': [],
            'episode_lengths': [],
            'success_episodes': [],
            'failure_episodes': []
        }
        
        # è¿›åº¦æ¡
        pbar = tqdm(range(self.rollouts_per_task), desc=f"Evaluating {task_name.split('_')[-1]}")
        
        for episode_idx in pbar:
            try:
                # è¿è¡Œå•ä¸ªepisode
                episode_result = self.env_runner.run_policy_in_env(
                    policy=self.policy,
                    task_name=task_name,
                    episode_idx=episode_idx,
                    deterministic=True,  # ç¡®å®šæ€§è¯„ä¼°
                    save_video=self.eval_config.get('save_videos', False)
                )
                
                # è®°å½•ç»“æœ
                reward = episode_result.get('reward', 0.0)
                length = episode_result.get('episode_length', 0)
                success = episode_result.get('success', False)
                
                task_results['episode_rewards'].append(reward)
                task_results['episode_lengths'].append(length)
                
                if success:
                    task_results['success_count'] += 1
                    task_results['success_episodes'].append(episode_idx)
                else:
                    task_results['failure_episodes'].append(episode_idx)
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                success_rate = task_results['success_count'] / (episode_idx + 1)
                pbar.set_postfix({
                    'Success Rate': f"{success_rate:.2%}",
                    'Success': f"{task_results['success_count']}/{episode_idx + 1}"
                })
                
            except Exception as e:
                print(f"âŒ Episode {episode_idx} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        task_results['success_rate'] = task_results['success_count'] / self.rollouts_per_task
        task_results['avg_reward'] = np.mean(task_results['episode_rewards']) if task_results['episode_rewards'] else 0.0
        task_results['avg_length'] = np.mean(task_results['episode_lengths']) if task_results['episode_lengths'] else 0.0
        
        print(f"âœ… {task_name}: {task_results['success_rate']:.2%} ({task_results['success_count']}/{self.rollouts_per_task})")
        
        return task_results
    
    def run_full_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("\nğŸš€ å¼€å§‹LIBERO-10å®Œæ•´è¯„ä¼°")
        print("=" * 80)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # é€ä¸ªè¯„ä¼°æ‰€æœ‰ä»»åŠ¡
        all_task_results = []
        total_success = 0
        total_episodes = 0
        
        for task_idx, task_name in enumerate(self.tasks):
            print(f"\n[{task_idx + 1}/{len(self.tasks)}] å¼€å§‹è¯„ä¼°: {task_name}")
            
            task_result = self.evaluate_single_task(task_name)
            all_task_results.append(task_result)
            
            # ç´¯ç§¯ç»Ÿè®¡
            total_success += task_result['success_count']
            total_episodes += task_result['total_rollouts']
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_success_rate = total_success / total_episodes if total_episodes > 0 else 0.0
        evaluation_time = time.time() - start_time
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        evaluation_report = {
            'timestamp': datetime.now().isoformat(),
            'checkpoint_path': self.checkpoint_path,
            'evaluation_config': self.config,
            'overall_metrics': {
                'success_rate': overall_success_rate,
                'total_success': total_success,
                'total_episodes': total_episodes,
                'num_tasks': len(self.tasks),
                'evaluation_time_seconds': evaluation_time
            },
            'per_task_results': all_task_results
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.session_dir / "evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆç®€æ´æŠ¥å‘Š
        self._generate_summary_report(evaluation_report)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ LIBERO-10è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.2%} ({total_success}/{total_episodes})")
        print(f"â±ï¸  è¯„ä¼°ç”¨æ—¶: {evaluation_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜è‡³: {results_file}")
        
        return evaluation_report
    
    def _generate_summary_report(self, evaluation_report: Dict):
        """ç”Ÿæˆç®€æ´çš„æ±‡æ€»æŠ¥å‘Š"""
        summary_file = self.session_dir / "evaluation_summary.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("LIBERO-10 Checkpoint è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"è¯„ä¼°æ—¶é—´: {evaluation_report['timestamp']}\n")
            f.write(f"Checkpoint: {self.checkpoint_path}\n")
            f.write(f"æ¯ä»»åŠ¡è¯„ä¼°æ¬¡æ•°: {self.rollouts_per_task}\n\n")
            
            # æ€»ä½“ç»“æœ
            overall = evaluation_report['overall_metrics']
            f.write("æ€»ä½“ç»“æœ:\n")
            f.write("-" * 20 + "\n")
            f.write(f"æˆåŠŸç‡: {overall['success_rate']:.2%}\n")
            f.write(f"æˆåŠŸæ¬¡æ•°: {overall['total_success']}/{overall['total_episodes']}\n")
            f.write(f"è¯„ä¼°ç”¨æ—¶: {overall['evaluation_time_seconds']/60:.1f} åˆ†é’Ÿ\n\n")
            
            # å„ä»»åŠ¡è¯¦ç»†ç»“æœ
            f.write("å„ä»»åŠ¡è¯¦ç»†ç»“æœ:\n")
            f.write("-" * 30 + "\n")
            
            for i, task_result in enumerate(evaluation_report['per_task_results']):
                task_short_name = task_result['task_name'].split('_')[-1]
                f.write(f"{i+1:2d}. {task_short_name:<15}: ")
                f.write(f"{task_result['success_rate']:6.1%} ")
                f.write(f"({task_result['success_count']:2d}/{task_result['total_rollouts']:2d})\n")
        
        print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Šä¿å­˜è‡³: {summary_file}")

def main():
    parser = argparse.ArgumentParser(description="LIBERO-10 Checkpoint Evaluation")
    parser.add_argument(
        '--config_path', 
        type=str, 
        default='pi0/ript/config/libero10_eval.yaml',
        help='è¯„ä¼°é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--checkpoint_path', 
        type=str, 
        default=None,
        help='PI0 checkpointè·¯å¾„ (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„)'
    )
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = LIBERO10Evaluator(args.config_path, args.checkpoint_path)
    
    # åŠ è½½æ¨¡å‹å’Œè®¾ç½®ç¯å¢ƒ
    evaluator.load_policy()
    evaluator.setup_env_runner()
    
    # è¿è¡Œå®Œæ•´è¯„ä¼°
    results = evaluator.run_full_evaluation()
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    overall_success_rate = results['overall_metrics']['success_rate']
    print(f"\nğŸ† æœ€ç»ˆè¯„ä¼°ç»“æœ: {overall_success_rate:.2%}")

if __name__ == "__main__":
    main()