#!/usr/bin/env python3
"""
Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–ç‰ˆæœ¬
åŸºäºRIPT-VLAçš„ç›´æ¥æ¶æ„æ¨¡å¼ï¼Œå»é™¤å¤šä½™çš„æŠ½è±¡å±‚

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
1. ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†rolloutæ”¶é›†å’Œä¼˜åŒ–
2. ç®€åŒ–çš„ç»„ä»¶æ¶æ„ï¼Œå‡å°‘ä¸­é—´å±‚
3. ç›´æ¥ä½¿ç”¨SubprocVectorEnvè¿›è¡Œå¹¶è¡Œ
4. æ¨¡ä»¿RIPT-VLAçš„æˆåŠŸæ¨¡å¼


python 11_train_ript_vla_style.py --config_path pi0/ript/config/stage11_parallel_test.yaml 
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from datetime import datetime
import argparse
from typing import List, Dict, Any, Optional
import yaml
import traceback
import time
from tqdm import tqdm
from collections import deque
import hashlib

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Šå’ŒEGLé”™è¯¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["EGL_LOG_LEVEL"] = "fatal"  # æŠ‘åˆ¶EGLé”™è¯¯è¾“å‡º

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(f"=== Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ ===")
print(f"è„šæœ¬ä½ç½®: {current_file}")
print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print()

# å¯¼å…¥é…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ OmegaConfé…ç½®ç®¡ç†å·²å¯ç”¨")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€YAMLåŠ è½½")

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    print("æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
    
    # PI0ç­–ç•¥
    from pi0.modeling_pi0 import PI0Policy
    print("âœ“ PI0ç­–ç•¥æ¨¡å—")
    
    # RIPTç»„ä»¶ - åªå¯¼å…¥å¿…éœ€çš„
    from pi0.ript.reward_function import BinarySuccessReward
    from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter
    print("âœ“ RIPTæ ¸å¿ƒç»„ä»¶")
    
    # # å¯¼å…¥ç®€åŒ–çš„ç¯å¢ƒrunner
    # try:
    #     from pi0.ript.env.pi0_libero_runner_ript_vla import PI0LiberoRunner
    #     RIPT_VLA_RUNNER_AVAILABLE = True
    #     print("âœ“ RIPT-VLA Runner")
    # except ImportError as e:
    #     print(f"âš ï¸ RIPT-VLA runnerå¯¼å…¥å¤±è´¥: {e}")
    #     RIPT_VLA_RUNNER_AVAILABLE = False
    # é»˜è®¤å…³é—­RIPT-VLA Runnerï¼ˆè‹¥ä¸Šæ–¹å¯¼å…¥è¢«æ³¨é‡Šæˆ–å¤±è´¥æ—¶ä¿æŒä¸ºFalseï¼‰
    RIPT_VLA_RUNNER_AVAILABLE = False
        
    # å¤‡ç”¨å¯¼å…¥åŸæœ‰runner
    try:
        from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
        ORIGINAL_RUNNER_AVAILABLE = True
        print("âœ“ åŸæœ‰LIBEROEnvRunner")
    except ImportError as e:
        print("âš ï¸ åŸæœ‰runnerä¹Ÿä¸å¯ç”¨")
        ORIGINAL_RUNNER_AVAILABLE = False
    
    print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥å®Œæˆ")
    
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def get_config_value(config, key, default=None, sources=['algo', 'features']):
    """
    ç»Ÿä¸€é…ç½®è¯»å–å‡½æ•°ï¼Œæ”¯æŒå¤šçº§å›é€€
    
    Args:
        config: é…ç½®å­—å…¸æˆ–å¯¹è±¡
        key: é…ç½®é”®å
        default: é»˜è®¤å€¼
        sources: æœç´¢çš„é…ç½®èŠ‚ç‚¹åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    
    Returns:
        é…ç½®å€¼
    """
    # å¤„ç†OmegaConfå’Œå­—å…¸ä¸¤ç§æƒ…å†µ
    if hasattr(config, 'get'):
        # å­—å…¸å¼è®¿é—®
        for source in sources:
            if source in config and config[source] is not None and key in config[source]:
                return config[source][key]
        # æ ¹èŠ‚ç‚¹è®¿é—®
        return config.get(key, default)
    else:
        # å±æ€§å¼è®¿é—®ï¼ˆé’ˆå¯¹ä¸€äº›æ—§çš„configå¯¹è±¡ï¼‰
        for source in sources:
            if hasattr(config, source):
                source_obj = getattr(config, source)
                if source_obj is not None and hasattr(source_obj, key):
                    return getattr(source_obj, key)
        # æ ¹èŠ‚ç‚¹è®¿é—®
        return getattr(config, key, default)

def set_config_value(config, key, value, target_source='algo'):
    """
    ç»Ÿä¸€é…ç½®å†™å›å‡½æ•°
    
    Args:
        config: é…ç½®å­—å…¸æˆ–å¯¹è±¡
        key: é…ç½®é”®å
        value: è¦è®¾ç½®çš„å€¼
        target_source: ç›®æ ‡é…ç½®èŠ‚ç‚¹
    """
    if hasattr(config, 'get'):
        # å­—å…¸å¼è®¿é—®
        if target_source not in config:
            config[target_source] = {}
        config[target_source][key] = value
    else:
        # å±æ€§å¼è®¿é—®
        if not hasattr(config, target_source):
            setattr(config, target_source, type('Config', (), {})())
        target_obj = getattr(config, target_source)
        setattr(target_obj, key, value)

def load_config(config_path: str):
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨OmegaConfï¼Œä¾¿äºå±æ€§è®¿é—®ï¼‰"""
    print(f"æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    if OMEGACONF_AVAILABLE:
        config = OmegaConf.load(config_path)
    else:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    
    # ç®€å•çš„ç±»å‹è½¬æ¢
    try:
        lr = config["algo"]["lr"]
        if isinstance(lr, str):
            if OMEGACONF_AVAILABLE:
                config.algo.lr = float(lr)
            else:
                config["algo"]["lr"] = float(lr)
    except Exception:
        pass
    
    print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    return config

def create_policy_and_optimizer(config: Dict[str, Any]):
    """åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨ï¼ˆRIPT-VLAé£æ ¼ï¼‰"""
    print("æ­£åœ¨åŠ è½½PI0ç­–ç•¥...")
    
    policy_path = config['policy_path']
    policy = PI0Policy.from_pretrained(policy_path)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶å¯ç”¨CFGï¼ˆè§£å†³åŸå§‹checkpointå…¼å®¹æ€§é—®é¢˜ï¼‰
    print("ğŸ”§ å¼ºåˆ¶å¯ç”¨CFGåŠŸèƒ½...")
    policy.model.cfg_enabled = True
    if hasattr(policy, 'config'):
        policy.config.cfg_enabled = True
    print("âœ… CFGå·²å¯ç”¨ï¼Œè®­ç»ƒå’Œæ¨ç†éƒ½å°†ä½¿ç”¨CFGåˆ†æ”¯")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    policy = policy.to(device)
    print(f"âœ“ ç­–ç•¥åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    # ğŸ”¥ ä¿®å¤ï¼šåªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå†»ç»“PaliGemmaå‰ç¼€ï¼ˆæå‡ç¨³å®šæ€§ï¼‰
    print("ğŸ”§ é…ç½®è®­ç»ƒå‚æ•°èŒƒå›´...")
    
    # 1. å†»ç»“PaliGemmaå‰ç¼€
    for p in policy.model.paligemma_with_expert.parameters():
        p.requires_grad = False
    
    # 2. åªæ”¶é›†éœ€è¦è®­ç»ƒçš„å‚æ•°
    trainable_params = []
    trainable_params += list(policy.model.action_in_proj.parameters())
    trainable_params += list(policy.model.action_time_mlp_in.parameters())
    trainable_params += list(policy.model.action_time_mlp_out.parameters())
    trainable_params += list(policy.model.action_out_proj.parameters())
    trainable_params += list(policy.model.state_proj.parameters())
    
    # 3. CFG embeddingå‚æ•°
    if hasattr(policy.model, "cfg_emb"):
        trainable_params += list(policy.model.cfg_emb.parameters())
        print("âœ… CFG embeddingå‚æ•°å·²åŠ å…¥è®­ç»ƒ")
    
    # 4. åˆ›å»ºä¼˜åŒ–å™¨
    print("æ­£åœ¨åˆ›å»ºä¼˜åŒ–å™¨...")
    lr = config['algo'].get('lr', 1e-5)
    optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)
    
    total_params = sum(p.numel() for p in trainable_params)
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼Œå­¦ä¹ ç‡: {lr}")
    print(f"ğŸ¯ åªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå‚æ•°æ•°é‡: {total_params:,}")
    
    return policy, optimizer, device

def compute_init_state_hash_from_obs(episode_data: Dict[str, Any]) -> str:
    """
    ä»åŸå§‹è§‚æµ‹è®¡ç®—åˆå§‹çŠ¶æ€å“ˆå¸Œï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
    
    Args:
        episode_data: episodeæ•°æ®ï¼ŒåŒ…å«observations
    
    Returns:
        åˆå§‹çŠ¶æ€çš„SHA256å“ˆå¸Œå­—ç¬¦ä¸²
    """
    try:
        # æå–ç¬¬ä¸€ä¸ªè§‚æµ‹ä½œä¸ºåˆå§‹çŠ¶æ€
        if 'observations' in episode_data and len(episode_data['observations']) > 0:
            first_obs = episode_data['observations'][0]
            
            # æå–å…³é”®çŠ¶æ€ä¿¡æ¯ç”¨äºå“ˆå¸Œ
            if isinstance(first_obs, dict):
                # æå– end-effector ä½ç½®å’Œå§¿æ€
                state_components = []
                if "robot0_eef_pos" in first_obs:
                    state_components.append(np.array(first_obs["robot0_eef_pos"], dtype=np.float32))
                if "robot0_eef_quat" in first_obs:
                    state_components.append(np.array(first_obs["robot0_eef_quat"], dtype=np.float32))
                if "robot0_gripper_qpos" in first_obs:
                    state_components.append(np.array(first_obs["robot0_gripper_qpos"], dtype=np.float32))
                
                if state_components:
                    # åˆå¹¶æ‰€æœ‰çŠ¶æ€ç»„ä»¶
                    combined_state = np.concatenate(state_components).astype(np.float32)
                    # è®¡ç®—å“ˆå¸Œ
                    return hashlib.sha256(combined_state.tobytes()).hexdigest()
        
        # å¦‚æœæ— æ³•æå–çŠ¶æ€ï¼Œè¿”å›é»˜è®¤å“ˆå¸Œ
        return "default_hash"
        
    except Exception as e:
        print(f"âš ï¸ ä»è§‚æµ‹è®¡ç®—åˆå§‹çŠ¶æ€å“ˆå¸Œå¤±è´¥: {e}")
        return "error_hash"

def compute_init_state_hash(batch_states: torch.Tensor, batch_pad_mask: torch.Tensor, batch_idx: int) -> str:
    """
    è®¡ç®—åˆå§‹çŠ¶æ€çš„å“ˆå¸Œå€¼ï¼ˆå®Œå…¨å¯¹æ ‡ ript-vla_ori çš„ compute_hash_from_stateï¼‰
    
    Args:
        batch_states: (B, T, state_dim) æ‰¹é‡çŠ¶æ€åºåˆ—
        batch_pad_mask: (B, T) æ‰¹é‡æœ‰æ•ˆæ©ç ï¼ŒTrueè¡¨ç¤ºæœ‰æ•ˆ
        batch_idx: åœ¨æ‰¹æ¬¡ä¸­çš„ç´¢å¼•
    
    Returns:
        åˆå§‹çŠ¶æ€çš„SHA256å“ˆå¸Œå­—ç¬¦ä¸²
    """
    try:
        # ğŸ”¥ å®Œå…¨å¯¹æ ‡ RIPT: state_data = state['states'][bidx][0]
        if batch_idx < batch_states.shape[0] and batch_states.shape[1] > 0:
            # å–æŒ‡å®šbatchç´¢å¼•çš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çŠ¶æ€
            state_data = batch_states[batch_idx, 0]  # (state_dim,)
            
            # å–å¯¹åº”çš„æ©ç 
            if batch_pad_mask is not None and batch_idx < batch_pad_mask.shape[0]:
                state_mask = batch_pad_mask[batch_idx, 0]  # ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„æ©ç 
                
                # ğŸ”¥ å®Œå…¨å¯¹æ ‡ RIPT: state_data[state_mask].cpu().numpy().tobytes()
                if state_mask.item():  # å¦‚æœç¬¬ä¸€ä¸ªæ—¶é—´æ­¥æ˜¯æœ‰æ•ˆçš„
                    state_bytes = state_data.cpu().numpy().astype(np.float32).tobytes()
                    return hashlib.sha256(state_bytes).hexdigest()
            else:
                # å¦‚æœæ²¡æœ‰æ©ç ï¼Œç›´æ¥ä½¿ç”¨çŠ¶æ€æ•°æ®
                state_bytes = state_data.cpu().numpy().astype(np.float32).tobytes()
                return hashlib.sha256(state_bytes).hexdigest()
        
        return "default_hash"
        
    except Exception as e:
        print(f"âš ï¸ è®¡ç®—åˆå§‹çŠ¶æ€å“ˆå¸Œå¤±è´¥: {e}")
        return "error_hash"

def load_rollout_stats(stats_path: str) -> Dict[str, List[int]]:
    """åŠ è½½ rollout ç»Ÿè®¡ä¿¡æ¯"""
    if Path(stats_path).exists():
        try:
            with open(stats_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ rollout_stats å¤±è´¥: {e}")
    return {}

def save_rollout_stats(stats: Dict[str, List[int]], stats_path: str):
    """ä¿å­˜ rollout ç»Ÿè®¡ä¿¡æ¯"""
    try:
        Path(stats_path).parent.mkdir(parents=True, exist_ok=True)
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ rollout_stats å¤±è´¥: {e}")

def should_skip_init_state(init_hash: str, rollout_stats: Dict[str, List[int]], 
                          rloo_batch_size: int, rollout_skip_cnt: Dict[str, int],
                          rollout_skip_threshold: int = 10) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æŸä¸ªåˆå§‹çŠ¶æ€ï¼ˆå¯¹æ ‡ ript-vla_ori çš„è·³è¿‡é€»è¾‘ï¼‰
    
    Args:
        init_hash: åˆå§‹çŠ¶æ€å“ˆå¸Œ
        rollout_stats: {init_hash: [success_list]} å†å²è®°å½•
        rloo_batch_size: RLOOæ‰¹æ¬¡å¤§å°
        rollout_skip_cnt: è·³è¿‡è®¡æ•°å™¨
        rollout_skip_threshold: è·³è¿‡é˜ˆå€¼
    
    Returns:
        æ˜¯å¦åº”è¯¥è·³è¿‡
    """
    if init_hash in rollout_stats:
        recent_successes = rollout_stats[init_hash][-rloo_batch_size:]
        
        # æ£€æŸ¥æ˜¯å¦è¿ç»­å…¨æˆåŠŸæˆ–å…¨å¤±è´¥
        if len(recent_successes) >= rloo_batch_size:
            if all(s == 1 for s in recent_successes):
                print(f"ğŸ”„ è·³è¿‡æ ·æœ¬: init_hash={init_hash[:8]}... (è¿ç»­ {rloo_batch_size} æ¬¡å…¨æˆåŠŸ)")
                return True
            elif all(s == 0 for s in recent_successes):
                print(f"ğŸ”„ è·³è¿‡æ ·æœ¬: init_hash={init_hash[:8]}... (è¿ç»­ {rloo_batch_size} æ¬¡å…¨å¤±è´¥)")
                return True
    
    return False

def update_rollout_stats(init_hash: str, success: bool, rollout_stats: Dict[str, List[int]],
                        rollout_skip_cnt: Dict[str, int], max_history_length: int = 100):
    """æ›´æ–° rollout ç»Ÿè®¡ä¿¡æ¯"""
    if init_hash not in rollout_stats:
        rollout_stats[init_hash] = []
        rollout_skip_cnt[init_hash] = 0
    
    # æ·»åŠ æ–°ç»“æœ
    rollout_stats[init_hash].append(1 if success else 0)
    
    # é™åˆ¶å†å²é•¿åº¦
    if len(rollout_stats[init_hash]) > max_history_length:
        rollout_stats[init_hash] = rollout_stats[init_hash][-max_history_length:]

def update_rollout_stats_with_correct_hash(episodes: List[Dict], batch_states: torch.Tensor, 
                                         batch_pad_mask: torch.Tensor, owner_indices: List[int],
                                         rollout_stats: Dict[str, List[int]], 
                                         rollout_skip_cnt: Dict[str, int]):
    """
    ä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œæ›´æ–° rollout ç»Ÿè®¡ä¿¡æ¯ï¼ˆåœ¨ CFG adapter å¤„ç†åè°ƒç”¨ï¼‰
    
    Args:
        episodes: åŸå§‹ episodes
        batch_states: CFG adapter å¤„ç†åçš„çŠ¶æ€ (B, state_dim)ï¼Œæ³¨æ„è¿™é‡Œæ˜¯å•ä¸ªæ—¶é—´æ­¥
        batch_pad_mask: å¯¹åº”çš„æ©ç ï¼Œå¦‚æœå¯ç”¨çš„è¯
        owner_indices: çª—å£åˆ° episode çš„æ˜ å°„
        rollout_stats: rollout ç»Ÿè®¡å­—å…¸
        rollout_skip_cnt: è·³è¿‡è®¡æ•°å­—å…¸
    """
    if batch_states is None or len(episodes) == 0:
        return
    
    try:
        # ä¸ºæ¯ä¸ª episode è®¡ç®—æ­£ç¡®çš„å“ˆå¸Œ
        episode_hash_map = {}  # episode_idx -> correct_hash
        
        # æ ¹æ® owner_indices æ˜ å°„ï¼Œä¸ºæ¯ä¸ª episode æ‰¾åˆ°å¯¹åº”çš„çŠ¶æ€
        for window_idx, episode_idx in enumerate(owner_indices):
            if window_idx < batch_states.shape[0] and episode_idx < len(episodes):
                # ä½¿ç”¨çª—å£å¯¹åº”çš„çŠ¶æ€è®¡ç®—å“ˆå¸Œ
                state_data = batch_states[window_idx]  # (state_dim,)
                
                # å¯¹äºå•ä¸ªçŠ¶æ€ï¼ˆä¸æ˜¯åºåˆ—ï¼‰ï¼Œç›´æ¥è®¡ç®—å“ˆå¸Œ
                state_bytes = state_data.cpu().numpy().astype(np.float32).tobytes()
                correct_hash = hashlib.sha256(state_bytes).hexdigest()
                
                episode_hash_map[episode_idx] = correct_hash
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        for episode_idx, episode in enumerate(episodes):
            if episode_idx in episode_hash_map:
                correct_hash = episode_hash_map[episode_idx]
                success = episode.get('success', False)
                
                # å¦‚æœä¹‹å‰æœ‰ä¸´æ—¶å“ˆå¸Œï¼Œéœ€è¦è¿ç§»ç»Ÿè®¡
                temp_hash = episode.get('temp_init_hash')
                if temp_hash and temp_hash != correct_hash and temp_hash in rollout_stats:
                    # è¿ç§»ç»Ÿè®¡æ•°æ®åˆ°æ­£ç¡®çš„å“ˆå¸Œ
                    if correct_hash not in rollout_stats:
                        rollout_stats[correct_hash] = rollout_stats[temp_hash].copy()
                        rollout_skip_cnt[correct_hash] = rollout_skip_cnt.get(temp_hash, 0)
                    else:
                        # åˆå¹¶ç»Ÿè®¡æ•°æ®
                        rollout_stats[correct_hash].extend(rollout_stats[temp_hash])
                    
                    # æ¸…ç†ä¸´æ—¶å“ˆå¸Œ
                    del rollout_stats[temp_hash]
                    if temp_hash in rollout_skip_cnt:
                        del rollout_skip_cnt[temp_hash]
                    
                    # ç”±äºå·²ç»è¿ç§»äº†ç»Ÿè®¡ï¼Œä¸éœ€è¦å†æ¬¡æ›´æ–°
                    print(f"ğŸ”„ è¿ç§»ç»Ÿè®¡: {temp_hash[:8]}... â†’ {correct_hash[:8]}...")
                else:
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåªæœ‰åœ¨æ²¡æœ‰è¿ç§»çš„æƒ…å†µä¸‹æ‰æ›´æ–°ç»Ÿè®¡ï¼Œé¿å…é‡å¤è®¡æ•°
                    # æ£€æŸ¥æ˜¯å¦å·²ç»ç”¨temp_hashæ›´æ–°è¿‡ç»Ÿè®¡ï¼ˆåœ¨åˆ†ç»„è·¯å¾„ä¸­å¯èƒ½å‘ç”Ÿï¼‰
                    if not temp_hash or temp_hash == correct_hash:
                        update_rollout_stats(correct_hash, success, rollout_stats, rollout_skip_cnt)
                    # å¦‚æœtemp_hashå­˜åœ¨ä¸”ä¸ç­‰äºcorrect_hashï¼Œè¯´æ˜ä¹‹å‰æ²¡æœ‰æ›´æ–°è¿‡ç»Ÿè®¡ï¼Œç°åœ¨æ›´æ–°
                    elif temp_hash and temp_hash not in rollout_stats:
                        update_rollout_stats(correct_hash, success, rollout_stats, rollout_skip_cnt)
                
                # è®°å½•æ­£ç¡®çš„å“ˆå¸Œåˆ° episodeï¼ˆç”¨äºè°ƒè¯•ï¼‰
                episode['correct_init_hash'] = correct_hash
                
    except Exception as e:
        print(f"âš ï¸ æ›´æ–°æ­£ç¡®å“ˆå¸Œç»Ÿè®¡æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def create_environment_runner(config: Dict[str, Any], policy):
    """åˆ›å»ºç¯å¢ƒrunnerï¼ˆRIPT-VLAé£æ ¼é€‰æ‹©ï¼‰"""
    use_ript_vla = config.get('features', {}).get('use_ript_vla_runner', False)
    
    print(f"ğŸ” Runneré€‰æ‹©: use_ript_vla_runner = {use_ript_vla}")
    
    if use_ript_vla and RIPT_VLA_RUNNER_AVAILABLE:
        print("ğŸš€ ä½¿ç”¨RIPT-VLAé£æ ¼ç¯å¢ƒrunner")
        
        runner = PI0LiberoRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            rank=0
        )
        
    elif ORIGINAL_RUNNER_AVAILABLE:
        print("ğŸ”„ ä½¿ç”¨åŸæœ‰ç¯å¢ƒrunner")
        
        # ç¡®ä¿norm_stats_pathå­˜åœ¨
        norm_stats_path = config.get('norm_stats_path')
        if not norm_stats_path:
            norm_stats_path = f"{config['policy_path']}/norm_stats.json"
        
        runner = LIBEROEnvRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            norm_stats_path=norm_stats_path,
            config=config,
            rank=0,
            world_size=1
        )
    else:
        raise RuntimeError("âŒ æ²¡æœ‰å¯ç”¨çš„ç¯å¢ƒrunnerï¼")
    
    print("âœ“ ç¯å¢ƒrunneråˆ›å»ºæˆåŠŸ")
    return runner

def _ript_dynamic_sampling(episodes: List[Dict], enable_dynamic_sampling: bool = True) -> List[Dict]:
    """
    ğŸ”¥ RIPTåŸç‰ˆåŠ¨æ€é‡‡æ ·ï¼šç®€å•çš„å…¨æˆåŠŸ/å…¨å¤±è´¥æ£€æŸ¥
    
    Args:
        episodes: æ”¶é›†çš„episodesï¼ˆé€šå¸¸æ˜¯8ä¸ªæ¥è‡ªç›¸åŒinit_hashçš„æ ·æœ¬ï¼‰
        enable_dynamic_sampling: æ˜¯å¦å¯ç”¨åŠ¨æ€é‡‡æ ·
    
    Returns:
        è¿‡æ»¤åçš„episodesï¼ˆå¦‚æœå…¨æˆåŠŸ/å…¨å¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨ï¼‰
    """
    if not enable_dynamic_sampling or not episodes:
        return episodes
    
    # æå–æˆåŠŸ/å¤±è´¥çŠ¶æ€
    successes = [bool(ep.get('success', False)) for ep in episodes]
    
    # ğŸ”¥ RIPTæ ¸å¿ƒé€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦å…¨æˆåŠŸæˆ–å…¨å¤±è´¥
    if len(set(successes)) == 1:  # åªæœ‰ä¸€ç§ç»“æœï¼ˆå…¨0æˆ–å…¨1ï¼‰
        uniform_result = "å…¨æˆåŠŸ" if successes[0] else "å…¨å¤±è´¥"
        print(f"âš ï¸ RIPTåŠ¨æ€é‡‡æ ·æ‹’ç»: {uniform_result} ({len(episodes)}ä¸ªæ ·æœ¬)ï¼Œç¼ºä¹å­¦ä¹ ä»·å€¼")
        return []  # ä¸¢å¼ƒæ•´ç»„
    
    # æ··åˆç»“æœï¼Œæœ‰å­¦ä¹ ä»·å€¼
    success_count = sum(successes)
    print(f"âœ… RIPTåŠ¨æ€é‡‡æ ·æ¥å—: {success_count}/{len(episodes)} æˆåŠŸï¼Œç»“æœæ··åˆæœ‰å­¦ä¹ ä»·å€¼")
    return episodes


def _dynamic_filter_rollouts_legacy(episodes: List[Dict], dynamic_sampling_config: Dict, 
                             recent_success_rates: deque) -> List[Dict]:
    """
    ğŸ”„ åŸæ¥çš„å¤æ‚åŠ¨æ€é‡‡æ ·ï¼ˆä¿ç•™ä½œä¸ºå¤‡ç”¨ï¼Œä½†ä¸æ¨èä½¿ç”¨ï¼‰
    å‡çº§ç‰ˆåŠ¨æ€é‡‡æ ·ï¼šåŒºé—´ç­–ç•¥ + å¹³æ»‘çª—å£æœºåˆ¶
    """
    if not dynamic_sampling_config.get('enabled', False) or not episodes:
        return episodes
    
    # è®¡ç®—å½“å‰æ‰¹æ¬¡æˆåŠŸç‡
    successes = [bool(ep.get('success', False)) for ep in episodes]
    current_success_rate = np.mean(successes) if successes else 0.0
    
    p_min = dynamic_sampling_config.get('p_min', 0.1)
    p_max = dynamic_sampling_config.get('p_max', 0.9)
    
    # ç¬¬ä¸€å±‚è¿‡æ»¤ï¼šå½“å‰æ‰¹æ¬¡åŒºé—´æ£€æŸ¥
    if current_success_rate < p_min or current_success_rate > p_max:
        print(f"âš ï¸ å¤æ‚åŠ¨æ€é‡‡æ ·ç¬¬ä¸€å±‚æ‹’ç»: success_rate={current_success_rate:.3f} ä¸åœ¨ [{p_min}, {p_max}] åŒºé—´å†…")
        return []
    
    # ç¬¬äºŒå±‚è¿‡æ»¤ï¼šå¹³æ»‘çª—å£æ£€æŸ¥ï¼ˆé™ä½æŠ–åŠ¨ï¼‰
    recent_success_rates.append(current_success_rate)
    if len(recent_success_rates) >= 2:  # è‡³å°‘æœ‰2ä¸ªæ ·æœ¬æ‰è¿›è¡Œçª—å£æ£€æŸ¥
        window_avg = np.mean(recent_success_rates)
        if window_avg < p_min or window_avg > p_max:
            print(f"âš ï¸ å¤æ‚åŠ¨æ€é‡‡æ ·ç¬¬äºŒå±‚æ‹’ç»: çª—å£å¹³å‡={window_avg:.3f} ä¸åœ¨åŒºé—´å†… (çª—å£å¤§å°={len(recent_success_rates)})")
            return []
    
    print(f"âœ… å¤æ‚åŠ¨æ€é‡‡æ ·é€šè¿‡: å½“å‰={current_success_rate:.3f}, çª—å£å¹³å‡={np.mean(recent_success_rates):.3f}")
    return episodes


def collect_rollouts_ript_vla_style_grouped(env_runner, task_name, demo_batch_size, rloo_batch_size,
                                           enable_ript_dynamic_sampling: bool = True,
                                           rollout_goal_per_step: int = None,
                                           rollout_stats: Dict[str, List[int]] = None,
                                           rollout_skip_cnt: Dict[str, int] = None):
    """
    ğŸ”¥ å®Œå…¨å¯¹é½ RIPT åŸç‰ˆï¼šæŒ‰åˆå§‹çŠ¶æ€åˆ†ç»„æ”¶é›†ï¼Œç¡®ä¿ RLOO ä¼˜åŠ¿è®¡ç®—çš„æ­£ç¡®æ€§
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. ç¡®å®šéœ€è¦æ”¶é›†çš„åˆå§‹çŠ¶æ€æ•°é‡ (demo_batch_size)
    2. å¯¹æ¯ä¸ªåˆå§‹çŠ¶æ€ï¼Œå¹¶è¡Œæ”¶é›† rloo_batch_size ä¸ªæ ·æœ¬
    3. ç¡®ä¿æ¯ç»„ rloo_batch_size ä¸ªæ ·æœ¬æ¥è‡ªç›¸åŒçš„ init_hash
    
    Args:
        demo_batch_size: éœ€è¦å¤šå°‘ä¸ªä¸åŒçš„åˆå§‹çŠ¶æ€ (æ¯ä¸ªçŠ¶æ€æ”¶é›† rloo_batch_size ä¸ªæ ·æœ¬)
        rloo_batch_size: æ¯ä¸ªåˆå§‹çŠ¶æ€æ”¶é›†å¤šå°‘ä¸ªæ ·æœ¬
    
    Returns:
        episodes: æ€»å…± demo_batch_size * rloo_batch_size ä¸ªæ ·æœ¬ï¼ŒæŒ‰ç»„ç»‡æ’åˆ—
        valid_mask: å¯¹åº”çš„æœ‰æ•ˆæ€§æ©ç 
    """
    total_target_samples = demo_batch_size * rloo_batch_size
    print(f"ğŸ¯ RIPTé£æ ¼æ”¶é›†ï¼š{demo_batch_size} ä¸ªåˆå§‹çŠ¶æ€ Ã— {rloo_batch_size} ä¸ªæ ·æœ¬ = {total_target_samples} ä¸ªæ ·æœ¬")
    
    # å¦‚æœè®¾ç½®äº†å…¨å±€æ ·æœ¬ç›®æ ‡ï¼Œæ˜¾ç¤ºå½“å‰è¿›åº¦
    if rollout_goal_per_step and hasattr(env_runner, 'file_counter') and env_runner.file_counter:
        current_count = env_runner.file_counter.get()
        print(f"ğŸ“Š å½“å‰å…¨å±€æ ·æœ¬æ•°: {current_count}/{rollout_goal_per_step}")
    
    all_episodes = []
    collected_groups = 0
    
    try:
        # æ¯ä¸ªè®­ç»ƒæ­¥å¼€å§‹å‰é‡ç½®è®¡æ•°å™¨ï¼ˆæŒ‰æ­¥æ—©åœï¼Œè€Œéç´¯è®¡ï¼‰
        if rollout_goal_per_step and hasattr(env_runner, 'file_counter') and env_runner.file_counter:
            try:
                env_runner.file_counter.set(0)
            except Exception:
                pass

        # è·å–ä»»åŠ¡çš„åˆå§‹çŠ¶æ€æ± 
        task_id = 0  # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
        if hasattr(env_runner, 'benchmark'):
            all_init_states = env_runner.benchmark.get_task_init_states(task_id)
            print(f"ğŸ“‹ å¯ç”¨åˆå§‹çŠ¶æ€æ•°é‡: {len(all_init_states) if all_init_states else 0}")
        else:
            all_init_states = None
        
        # ğŸ”¥ æ ¸å¿ƒå¾ªç¯ï¼šæŒ‰åˆå§‹çŠ¶æ€åˆ†ç»„æ”¶é›†
        for group_idx in range(demo_batch_size):
            # æ£€æŸ¥å…¨å±€æ—©åœæ¡ä»¶
            if rollout_goal_per_step and hasattr(env_runner, 'file_counter') and env_runner.file_counter:
                current_global_count = env_runner.file_counter.get()
                if current_global_count >= rollout_goal_per_step:
                    print(f"ğŸ¯ è¾¾åˆ°å…¨å±€æ ·æœ¬ç›®æ ‡ ({current_global_count}/{rollout_goal_per_step})ï¼Œæå‰ç»“æŸæ”¶é›†")
                    break
            
            print(f"ğŸ”„ æ”¶é›†ç¬¬ {group_idx + 1}/{demo_batch_size} ç»„æ ·æœ¬...")
            
            # æ­¥éª¤1: é€‰æ‹©/ç”Ÿæˆä¸€ä¸ªåˆå§‹çŠ¶æ€
            if all_init_states is not None and len(all_init_states) > 0:
                # ä»å¯ç”¨åˆå§‹çŠ¶æ€ä¸­éšæœºé€‰æ‹©
                init_state_idx = np.random.randint(0, len(all_init_states))
                init_state = all_init_states[init_state_idx]
                print(f"ğŸ“ ä½¿ç”¨åˆå§‹çŠ¶æ€ #{init_state_idx}")
            else:
                # ä½¿ç”¨é»˜è®¤åˆå§‹çŠ¶æ€æˆ–éšæœºç”Ÿæˆ
                init_state = None  # è®© runner è‡ªå·±å¤„ç†
                print(f"ğŸ“ ä½¿ç”¨é»˜è®¤åˆå§‹çŠ¶æ€")
            
            # æ­¥éª¤2: è®¡ç®—åˆå§‹çŠ¶æ€å“ˆå¸Œï¼ˆç”¨äºè·³è¿‡æ£€æŸ¥ï¼‰
            if init_state is not None:
                state_bytes = np.array(init_state).astype(np.float32).tobytes()
                init_hash = hashlib.sha256(state_bytes).hexdigest()
            else:
                init_hash = f"default_{group_idx}"  # é»˜è®¤å“ˆå¸Œ
            
            # æ­¥éª¤3: æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡è¿™ä¸ªåˆå§‹çŠ¶æ€
            enable_init_skip = (rollout_stats is not None and rollout_skip_cnt is not None)
            if enable_init_skip and should_skip_init_state(init_hash, rollout_stats, rloo_batch_size, 
                                                          rollout_skip_cnt, rollout_skip_threshold=10):
                print(f"â­ï¸ è·³è¿‡åˆå§‹çŠ¶æ€ {init_hash[:8]}ï¼ˆå†å²è¡¨ç°ä¸€è‡´ï¼‰")
                rollout_skip_cnt[init_hash] = rollout_skip_cnt.get(init_hash, 0) + 1
                continue  # è·³è¿‡è¿™ä¸ªåˆå§‹çŠ¶æ€ï¼Œä¸è®¡å…¥ collected_groups
            
            # æ­¥éª¤4: ä¸ºè¿™ä¸ªåˆå§‹çŠ¶æ€æ”¶é›† rloo_batch_size ä¸ªæ ·æœ¬
            group_episodes = []
            
            # ğŸ”¥ å…³é”®ï¼šåˆ›å»º rloo_batch_size ä¸ªç›¸åŒçš„åˆå§‹çŠ¶æ€
            if init_state is not None:
                # å¤åˆ¶ç›¸åŒçš„åˆå§‹çŠ¶æ€
                env_init_states = np.tile(init_state, (rloo_batch_size, 1))
                print(f"ğŸ”„ ç¬¬{group_idx+1}ç»„å¹¶è¡Œè¿è¡Œ {rloo_batch_size} ä¸ªç¯å¢ƒ")
                print(f"   ğŸ“ åˆå§‹çŠ¶æ€å“ˆå¸Œ: {init_hash[:8]}...")
                print(f"   ğŸ“Š çŠ¶æ€å€¼: {init_state[:4].round(3)}... (æ˜¾ç¤ºå‰4ç»´)")
                print(f"   ğŸ”¢ çŠ¶æ€å½¢çŠ¶: {init_state.shape}, å¤åˆ¶åˆ° {env_init_states.shape}")
                
                # è°ƒç”¨ runner çš„å¹¶è¡Œæ‰§è¡Œï¼ˆå¦‚æœæ”¯æŒï¼‰
                if hasattr(env_runner, 'run_policy_in_env_batch'):
                    # ä½¿ç”¨æ‰¹é‡æ¥å£
                    batch_episodes = env_runner.run_policy_in_env_batch(
                        env_name=task_name,
                        init_states=env_init_states
                    )
                    group_episodes.extend(batch_episodes)
                else:
                    # å›é€€åˆ°é€ä¸ªæ”¶é›†ï¼ˆä½†ä½¿ç”¨ç›¸åŒåˆå§‹çŠ¶æ€ï¼‰
                    for sample_idx in range(rloo_batch_size):
                        rollout_generator = env_runner.run_policy_in_env(
                            env_name=task_name,
                            all_init_states=[init_state]  # ä½¿ç”¨ç›¸åŒçš„åˆå§‹çŠ¶æ€
                        )
                        
                        # æ”¶é›†ä¸€ä¸ªæ ·æœ¬
                        for success, total_reward, episode_data in rollout_generator:
                            episode = {
                                'success': success,
                                'total_reward': total_reward,
                                'init_hash': init_hash,  # è®°å½•çœŸå®çš„åˆå§‹çŠ¶æ€å“ˆå¸Œ
                                **episode_data
                            }
                            group_episodes.append(episode)
                            break  # åªæ”¶é›†ä¸€ä¸ªæ ·æœ¬
                        
                        # è®¡æ•°å™¨åœ¨ç»„è¢«æ¥å—åç»Ÿä¸€æ›´æ–°
            else:
                # æ²¡æœ‰å…·ä½“åˆå§‹çŠ¶æ€ï¼Œé€ä¸ªæ”¶é›†ï¼ˆä½†ä¼šå¯¼è‡´ä¸åŒåˆå§‹çŠ¶æ€æ··åˆï¼‰
                print("âš ï¸ æ²¡æœ‰å…·ä½“åˆå§‹çŠ¶æ€ï¼Œå¯èƒ½å¯¼è‡´ RLOO åŸºçº¿ä¼°è®¡ä¸å‡†ç¡®")
                rollout_generator = env_runner.run_policy_in_env(
                    env_name=task_name,
                    all_init_states=all_init_states
                )
                
                sample_count = 0
                for success, total_reward, episode_data in rollout_generator:
                    episode = {
                        'success': success,
                        'total_reward': total_reward,
                        'init_hash': init_hash,
                        **episode_data
                    }
                    group_episodes.append(episode)
                    sample_count += 1
                    
                    # è®¡æ•°å™¨åœ¨ç»„è¢«æ¥å—åç»Ÿä¸€æ›´æ–°
                    
                    if sample_count >= rloo_batch_size:
                        break
            
            # æ­¥éª¤5: RIPTé£æ ¼åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒå¤šç§ç­–ç•¥ï¼‰
            group_accepted = True
            if enable_ript_dynamic_sampling and len(group_episodes) == rloo_batch_size:
                # ğŸ”¥ ä½¿ç”¨RIPTåŸç‰ˆçš„ç®€å•åŠ¨æ€é‡‡æ ·
                filtered_group = _ript_dynamic_sampling(group_episodes, enable_ript_dynamic_sampling)
                if not filtered_group:
                    group_accepted = False
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸ºåˆ†ç»„è·¯å¾„æ·»åŠ temp_init_hashå­—æ®µ
            if group_accepted and group_episodes:
                for ep in group_episodes:
                    if 'temp_init_hash' not in ep:
                        ep['temp_init_hash'] = init_hash  # ç¡®ä¿æ¯ä¸ªepisodeéƒ½æœ‰è¿™ä¸ªå­—æ®µ
                    if 'init_hash' not in ep:
                        ep['init_hash'] = init_hash
            
            # æ­¥éª¤6: å»¶è¿Ÿç»Ÿè®¡æ›´æ–°åˆ°correct_hashç”Ÿæˆåï¼ˆé¿å…é‡å¤è®¡æ•°ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œä¸å†ç›´æ¥æ›´æ–°rollout_statsï¼Œè€Œæ˜¯å»¶è¿Ÿåˆ°CFGå¤„ç†åç»Ÿä¸€æ›´æ–°
            
            # æ­¥éª¤7: æ·»åŠ åˆ°æ€»åˆ—è¡¨ï¼ˆä»…æ¥å—çš„ç»„ï¼‰
            if group_accepted and len(group_episodes) == rloo_batch_size:
                all_episodes.extend(group_episodes)
                collected_groups += 1
                print(f"âœ… ç¬¬ {group_idx + 1} ç»„æ”¶é›†å¹¶æ¥å—ï¼š{len(group_episodes)} ä¸ªæ ·æœ¬")
                # ğŸ”¥ å¯¹é½RIPTï¼šæ¯ä¸ªè¢«æ¥å—çš„ç»„è®¡æ•°+1
                if rollout_goal_per_step and hasattr(env_runner, 'file_counter') and env_runner.file_counter:
                    try:
                        env_runner.file_counter.update(1)
                        current_global_count = env_runner.file_counter.get()
                        if current_global_count >= rollout_goal_per_step:
                            print(f"ğŸ¯ è¾¾åˆ°å…¨å±€æ ·æœ¬ç›®æ ‡ ({current_global_count}/{rollout_goal_per_step} ç»„)ï¼Œæå‰ç»“æŸæ”¶é›†")
                            break
                    except Exception:
                        pass
            elif len(group_episodes) == rloo_batch_size:
                print(f"âš ï¸ ç¬¬ {group_idx + 1} ç»„æ”¶é›†ä½†è¢«è¿‡æ»¤ï¼š{len(group_episodes)} ä¸ªæ ·æœ¬")
            else:
                print(f"âš ï¸ ç¬¬ {group_idx + 1} ç»„æ ·æœ¬ä¸è¶³ï¼š{len(group_episodes)}/{rloo_batch_size}")
                # å¯ä»¥é€‰æ‹©å¡«å……æˆ–è·³è¿‡
                if len(group_episodes) > 0 and group_accepted:
                    # å¡«å……åˆ°ç›®æ ‡æ•°é‡
                    while len(group_episodes) < rloo_batch_size:
                        group_episodes.append(group_episodes[-1])  # ç”¨æœ€åä¸€ä¸ªæ ·æœ¬å¡«å……
                    all_episodes.extend(group_episodes)
                    collected_groups += 1
        
        print(f"ğŸ“Š æ”¶é›†å®Œæˆï¼š{collected_groups} ç»„ Ã— {rloo_batch_size} = {len(all_episodes)} ä¸ªæ ·æœ¬")
        
        # ğŸ”¥ RIPTé£æ ¼åŠ¨æ€é‡‡æ ·å·²åœ¨æ”¶é›†è¿‡ç¨‹ä¸­å®æ—¶åº”ç”¨
        if enable_ript_dynamic_sampling:
            print(f"ğŸ“Š RIPTåŠ¨æ€é‡‡æ ·: å·²è¿‡æ»¤å…¨æˆåŠŸ/å…¨å¤±è´¥çš„ç»„ï¼Œä¿ç•™æœ‰å­¦ä¹ ä»·å€¼çš„æ··åˆç»“æœ")
        
        # æ­¥éª¤8: ç”Ÿæˆæœ‰æ•ˆæ€§æ©ç 
        valid_mask = [True] * len(all_episodes)
        
        return all_episodes, valid_mask
        
    except Exception as e:
        print(f"âŒ æ”¶é›†è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return [], []


def collect_rollouts_ript_vla_style(env_runner, task_name, num_rollouts, 
                                     dynamic_sampling_config: Dict = None, 
                                     recent_success_rates: deque = None,
                                     rollout_goal_per_step: int = None,
                                     rollout_stats: Dict[str, List[int]] = None,
                                     rollout_skip_cnt: Dict[str, int] = None,
                                     rloo_batch_size: int = None):
    """
    å…¼å®¹æ€§åŒ…è£…å™¨ï¼šå°†æ—§çš„æ¥å£è½¬æ¢ä¸ºæ–°çš„RIPTé£æ ¼åˆ†ç»„æ”¶é›†
    """
    if rloo_batch_size is None or rloo_batch_size <= 0:
        rloo_batch_size = 8  # é»˜è®¤å€¼
    
    # è®¡ç®—éœ€è¦å¤šå°‘ç»„
    demo_batch_size = max(1, num_rollouts // rloo_batch_size)
    
    # ğŸ”¥ ä»å¤æ‚é…ç½®ä¸­æå–ç®€å•çš„å¸ƒå°”å¼€å…³
    if dynamic_sampling_config is not None:
        enable_ript_dynamic_sampling = dynamic_sampling_config.get('enabled', False)
        if enable_ript_dynamic_sampling:
            print(f"ğŸ”„ ä½¿ç”¨RIPTé£æ ¼åŠ¨æ€é‡‡æ · (ç®€å•çš„å…¨æˆåŠŸ/å…¨å¤±è´¥æ£€æŸ¥)")
    else:
        enable_ript_dynamic_sampling = True  # é»˜è®¤å¯ç”¨
    
    print(f"ğŸ”„ å…¼å®¹æ€§è½¬æ¢: {num_rollouts} ä¸ªæ ·æœ¬ â†’ {demo_batch_size} ç»„ Ã— {rloo_batch_size} ä¸ªæ ·æœ¬")
    
    return collect_rollouts_ript_vla_style_grouped(
        env_runner=env_runner,
        task_name=task_name, 
        demo_batch_size=demo_batch_size,
        rloo_batch_size=rloo_batch_size,
        enable_ript_dynamic_sampling=enable_ript_dynamic_sampling,
        rollout_goal_per_step=rollout_goal_per_step,
        rollout_stats=rollout_stats,
        rollout_skip_cnt=rollout_skip_cnt
    )


def collect_rollouts_ript_vla_style_old(env_runner, task_name, num_rollouts, 
                                     dynamic_sampling_config: Dict = None, 
                                     recent_success_rates: deque = None,
                                     rollout_goal_per_step: int = None,
                                     rollout_stats: Dict[str, List[int]] = None,
                                     rollout_skip_cnt: Dict[str, int] = None,
                                     rloo_batch_size: int = None):
    """
    åŸæœ‰çš„é€ä¸ªæ”¶é›†æ–¹å¼ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰
    âš ï¸ æ³¨æ„ï¼šè¿™ç§æ–¹å¼ä¼šå¯¼è‡´ RLOO åŸºçº¿ä¼°è®¡ä¸å‡†ç¡®ï¼Œå› ä¸ºä¸åŒåˆå§‹çŠ¶æ€çš„æ ·æœ¬è¢«æ··åˆåœ¨ä¸€èµ·
    """
    print(f"âš ï¸ ä½¿ç”¨æ—§ç‰ˆæ”¶é›†æ–¹å¼ï¼šé€ä¸ªæ”¶é›† {num_rollouts} ä¸ªrolloutsï¼ˆå¯èƒ½æ··åˆä¸åŒåˆå§‹çŠ¶æ€ï¼‰")
    
    # å¦‚æœè®¾ç½®äº†å…¨å±€æ ·æœ¬ç›®æ ‡ï¼Œæ˜¾ç¤ºå½“å‰è¿›åº¦
    if rollout_goal_per_step and hasattr(env_runner, 'file_counter') and env_runner.file_counter:
        current_count = env_runner.file_counter.get()
        print(f"ğŸ“Š å½“å‰å…¨å±€æ ·æœ¬æ•°: {current_count}/{rollout_goal_per_step}")
    
    try:
        # è·å–ä»»åŠ¡çš„åˆå§‹çŠ¶æ€
        task_id = 0  # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
        if hasattr(env_runner, 'benchmark'):
            all_init_states = env_runner.benchmark.get_task_init_states(task_id)
        else:
            all_init_states = None
        
        # ç›´æ¥è°ƒç”¨ç¯å¢ƒrunnerçš„æ–¹æ³•
        rollout_generator = env_runner.run_policy_in_env(
            env_name=task_name,
            all_init_states=all_init_states
        )
        
        # æ”¶é›†rolloutsï¼Œæ”¯æŒæ–‡ä»¶è®¡æ•°å™¨æ—©åœ
        collected_rollouts = []
        rollout_count = 0
        
        for success, total_reward, episode_data in rollout_generator:
            episode = {
                'success': success,
                'total_reward': total_reward,
                **episode_data
            }
            
            # ğŸ”¥ æ–°å¢ï¼šper-init å“ˆå¸Œè·³è¿‡æ£€æŸ¥
            enable_init_skip = (rollout_stats is not None and rollout_skip_cnt is not None and 
                               rloo_batch_size is not None and rloo_batch_size > 0)
            
            # ğŸ”¥ ä¿®æ”¹ï¼šæš‚æ—¶ä½¿ç”¨è§‚æµ‹å“ˆå¸Œï¼Œåç»­åœ¨è®­ç»ƒå¾ªç¯ä¸­ç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œ
            if enable_init_skip:
                # ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è®¡ç®—å“ˆå¸Œï¼ˆåŸºäºè§‚æµ‹ï¼‰
                init_hash = compute_init_state_hash_from_obs(episode)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡è¿™ä¸ªåˆå§‹çŠ¶æ€
                if should_skip_init_state(init_hash, rollout_stats, rloo_batch_size, 
                                        rollout_skip_cnt, rollout_skip_threshold=10):
                    rollout_skip_cnt[init_hash] = rollout_skip_cnt.get(init_hash, 0) + 1
                    
                    # å¦‚æœè·³è¿‡æ¬¡æ•°è¿‡å¤šï¼Œæ¸…ç†è¯¥å“ˆå¸Œè®°å½•
                    if rollout_skip_cnt[init_hash] > 10:
                        if init_hash in rollout_stats:
                            del rollout_stats[init_hash]
                        del rollout_skip_cnt[init_hash]
                        print(f"ğŸ§¹ æ¸…ç†è¿‡åº¦è·³è¿‡çš„ init_hash: {init_hash[:8]}...")
                    
                    continue  # è·³è¿‡è¿™ä¸ª episode
                
                # æš‚æ—¶è®°å½•å“ˆå¸Œï¼Œåç»­ä¼šç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œæ›´æ–°
                episode['temp_init_hash'] = init_hash
            
            collected_rollouts.append(episode)
            rollout_count += 1
            
            # ğŸ”¥ æ–°å¢ï¼šæ–‡ä»¶è®¡æ•°å™¨æ›´æ–°å’Œæ—©åœæ£€æŸ¥
            if hasattr(env_runner, 'file_counter') and env_runner.file_counter:
                env_runner.file_counter.update(1)
                current_global_count = env_runner.file_counter.get()
                
                # æ—©åœæ£€æŸ¥ï¼šè¾¾åˆ°å…¨å±€ç›®æ ‡æ ·æœ¬æ•°
                if rollout_goal_per_step and current_global_count >= rollout_goal_per_step:
                    print(f"ğŸ¯ è¾¾åˆ°å…¨å±€æ ·æœ¬ç›®æ ‡ ({current_global_count}/{rollout_goal_per_step})ï¼Œæå‰ç»“æŸæ”¶é›†")
                    break
            
            # åŸæœ‰çš„æ•°é‡é™åˆ¶
            if rollout_count >= num_rollouts:
                break
        
        # ğŸ”„ ä¼ ç»Ÿçš„å¤æ‚åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼ˆä¸æ¨èï¼Œä½†ä¿ç•™å…¼å®¹æ€§ï¼‰
        if dynamic_sampling_config and dynamic_sampling_config.get('use_legacy_filtering', False):
            # ä½¿ç”¨å¤æ‚çš„ä¼ ç»Ÿè¿‡æ»¤é€»è¾‘ï¼ˆéœ€è¦å¹³æ»‘çª—å£ï¼‰
            recent_success_rates = deque(maxlen=3)  # ä¸´æ—¶åˆ›å»ºçª—å£
            filtered = _dynamic_filter_rollouts_legacy(collected_rollouts, dynamic_sampling_config, recent_success_rates)
        else:
            # ğŸ”¥ é»˜è®¤ä½¿ç”¨RIPTé£æ ¼çš„ç®€å•åŠ¨æ€é‡‡æ ·
            enable_ript_dynamic_sampling = dynamic_sampling_config.get('enabled', True) if dynamic_sampling_config else True
            filtered = _ript_dynamic_sampling(collected_rollouts, enable_ript_dynamic_sampling)
            
        if not filtered:
            print("âš ï¸ æœ¬æ‰¹æ¬¡è¢«åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼Œè¿”å›ç©ºé›†")
            return filtered, []
        
        # ğŸ”¥ æ–°å¢ï¼špadding + valid_mask æœºåˆ¶ï¼ˆå¯¹æ ‡ ript-vla_oriï¼‰
        target_rollouts = num_rollouts  # ç›®æ ‡æ•°é‡
        valid_mask = [True] * len(filtered)
        
        if len(filtered) < target_rollouts:
            num_pad = target_rollouts - len(filtered)
            print(f"ğŸ“¦ Padding: éœ€è¦ {target_rollouts} ä¸ªrolloutsï¼Œå®é™…æ”¶é›† {len(filtered)} ä¸ªï¼Œå¡«å…… {num_pad} ä¸ª")
            
            if len(filtered) > 0:
                # ç”¨æœ€åä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¡«å……
                last_episode = filtered[-1].copy()
                for _ in range(num_pad):
                    padded_episode = last_episode.copy()
                    padded_episode['is_padded'] = True  # æ ‡è®°ä¸ºå¡«å……æ ·æœ¬
                    filtered.append(padded_episode)
                    valid_mask.append(False)  # æ ‡è®°ä¸ºæ— æ•ˆ
        else:
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ç”¨äºå¡«å……")
                return filtered, []
        
        print(f"âœ“ æˆåŠŸæ”¶é›†äº† {len(filtered)} ä¸ªrollouts (è¿‡æ»¤+å¡«å……åï¼Œ{sum(valid_mask)} ä¸ªæœ‰æ•ˆ)")
        return filtered, valid_mask
        
    except Exception as e:
        print(f"âŒ Rolloutæ”¶é›†å¤±è´¥: {e}")
        traceback.print_exc()
        return [], []

def compute_advantages_rloo_grouped(episodes: List[Dict], rloo_batch_size: int, 
                                   demo_batch_size: int = None) -> torch.Tensor:
    """
    ğŸ”¥ å¢å¼ºç‰ˆRLOOä¼˜åŠ¿è®¡ç®—ï¼šéªŒè¯åˆ†ç»„ç»“æ„ï¼Œç¡®ä¿åŸºçº¿ä¼°è®¡å‡†ç¡®æ€§
    
    ä¸“é—¨ä¸ºæŒ‰åˆå§‹çŠ¶æ€åˆ†ç»„çš„episodesè®¾è®¡ï¼Œç¡®ä¿æ¯ç»„å†…çš„æ ·æœ¬æ¥è‡ªç›¸åŒåˆå§‹çŠ¶æ€
    
    Args:
        episodes: æŒ‰åˆ†ç»„æ’åˆ—çš„episodesåˆ—è¡¨ (æ¯ç»„rloo_batch_sizeä¸ªæ ·æœ¬æ¥è‡ªç›¸åŒinit_hash)
        rloo_batch_size: æ¯ç»„çš„æ ·æœ¬æ•°é‡
        demo_batch_size: ç»„çš„æ•°é‡ï¼ˆå¯é€‰ï¼Œç”¨äºéªŒè¯ï¼‰
    
    Returns:
        torch.Tensor: è®¡ç®—å¾—åˆ°çš„ä¼˜åŠ¿å€¼
    """
    if not episodes:
        return torch.tensor([])
    
    num_rollouts = len(episodes)
    
    # éªŒè¯åˆ†ç»„ç»“æ„
    if demo_batch_size is not None:
        expected_total = demo_batch_size * rloo_batch_size
        if num_rollouts != expected_total:
            print(f"âš ï¸ åˆ†ç»„ç»“æ„ä¸åŒ¹é…: æœŸæœ› {expected_total} ä¸ªæ ·æœ¬ ({demo_batch_size}Ã—{rloo_batch_size}), å®é™… {num_rollouts} ä¸ª")
    
    # æå–å¥–åŠ±
    rewards = []
    init_hashes = []  # ç”¨äºéªŒè¯åˆ†ç»„
    for ep in episodes:
        reward = ep.get('total_reward', 0.0)
        rewards.append(float(reward))
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨correct_init_hashï¼Œå…¶æ¬¡temp_init_hashï¼Œæœ€åinit_hash
        hash_to_use = ep.get('correct_init_hash') or ep.get('temp_init_hash') or ep.get('init_hash', 'unknown')
        init_hashes.append(hash_to_use)
    
    rlhf_reward = torch.tensor(rewards, dtype=torch.float32)
    
    # ğŸ” éªŒè¯åˆ†ç»„çš„æ­£ç¡®æ€§
    if len(init_hashes) >= rloo_batch_size:
        print("ğŸ” éªŒè¯RLOOåˆ†ç»„ç»“æ„...")
        groups_verified = 0
        for group_start in range(0, num_rollouts, rloo_batch_size):
            group_end = min(group_start + rloo_batch_size, num_rollouts)
            if group_end - group_start == rloo_batch_size:
                group_hashes = init_hashes[group_start:group_end]
                unique_hashes = set(group_hashes)
                if len(unique_hashes) == 1:
                    groups_verified += 1
                else:
                    print(f"âš ï¸ ç¬¬ {group_start//rloo_batch_size + 1} ç»„åŒ…å«å¤šä¸ªä¸åŒåˆå§‹çŠ¶æ€: {unique_hashes}")
        
        total_groups = num_rollouts // rloo_batch_size
        if groups_verified == total_groups:
            print(f"âœ… åˆ†ç»„éªŒè¯é€šè¿‡: {groups_verified}/{total_groups} ç»„ç»“æ„æ­£ç¡®")
        else:
            print(f"âš ï¸ åˆ†ç»„éªŒè¯å¤±è´¥: ä»… {groups_verified}/{total_groups} ç»„ç»“æ„æ­£ç¡®")
    
    # ğŸš€ RLOOä¼˜åŠ¿è®¡ç®—
    if rloo_batch_size <= 1:
        print("âš ï¸ RLOO batch sizeè¿‡å°ï¼Œä½¿ç”¨ç®€å•ä¼˜åŠ¿è®¡ç®—")
        advantage = rlhf_reward - rlhf_reward.mean()
    else:
        try:
            # ç¡®ä¿å¯ä»¥æ•´é™¤
            effective_rollouts = (num_rollouts // rloo_batch_size) * rloo_batch_size
            if effective_rollouts != num_rollouts:
                print(f"ğŸ”§ RLOOè£å‰ªï¼š{num_rollouts} â†’ {effective_rollouts} rollouts")
                rlhf_reward = rlhf_reward[:effective_rollouts]
                num_rollouts = effective_rollouts
            
            num_batches = num_rollouts // rloo_batch_size
            rlhf_reward_reshaped = rlhf_reward.reshape(num_batches, rloo_batch_size)
            
            # ğŸ”¥ æ ¸å¿ƒRLOOè®¡ç®—ï¼šæ¯ä¸ªæ ·æœ¬ä¸åŒç»„å…¶ä»–æ ·æœ¬æ¯”è¾ƒ
            batch_sums = rlhf_reward_reshaped.sum(dim=1, keepdim=True)  # (num_batches, 1)
            baseline = (batch_sums - rlhf_reward_reshaped) / (rloo_batch_size - 1)  # (num_batches, rloo_batch_size)
            advantage = rlhf_reward_reshaped - baseline  # (num_batches, rloo_batch_size)
            advantage = advantage.flatten()  # å±•å¹³ä¸ºä¸€ç»´
            
            # å®‰å…¨æ€§æ£€æŸ¥
            if torch.isnan(advantage).any() or torch.isinf(advantage).any():
                print("âš ï¸ RLOOè®¡ç®—äº§ç”ŸNaN/Infï¼Œä½¿ç”¨å®‰å…¨æ›¿æ¢")
                advantage = torch.nan_to_num(advantage, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"ğŸ¯ åˆ†ç»„RLOOä¼˜åŠ¿è®¡ç®—å®Œæˆ:")
            print(f"   åˆ†ç»„é…ç½®: {num_rollouts} rollouts â†’ {num_batches} ç»„ Ã— {rloo_batch_size} æ ·æœ¬/ç»„")
            print(f"   ä¼˜åŠ¿ç»Ÿè®¡: mean={advantage.mean():.4f}, std={advantage.std():.4f}")
            print(f"   æ­£ä¼˜åŠ¿æ¯”ä¾‹: {(advantage > 0).float().mean():.2%}")
            
            # ğŸ” æŒ‰ç»„æ˜¾ç¤ºä¼˜åŠ¿åˆ†å¸ƒï¼ˆå‰å‡ ç»„ï¼‰
            if num_batches <= 5:
                for batch_idx in range(num_batches):
                    batch_advantages = advantage[batch_idx * rloo_batch_size:(batch_idx + 1) * rloo_batch_size]
                    batch_rewards = rlhf_reward_reshaped[batch_idx]
                    print(f"   ç»„ {batch_idx + 1}: å¥–åŠ± {batch_rewards.tolist()}, ä¼˜åŠ¿ {batch_advantages.tolist()}")
            
        except Exception as e:
            print(f"âŒ åˆ†ç»„RLOOè®¡ç®—å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•")
            advantage = rlhf_reward - rlhf_reward.mean()
    
    return advantage


def compute_advantages_rloo(episodes: List[Dict], rloo_batch_size: int = None) -> torch.Tensor:
    """
    å…¼å®¹æ€§åŒ…è£…å™¨ï¼šè°ƒç”¨å¢å¼ºç‰ˆåˆ†ç»„RLOOè®¡ç®—
    """
    if rloo_batch_size is None or rloo_batch_size <= 0:
        rloo_batch_size = 8  # é»˜è®¤å€¼
    
    # æ¨æ–­demo_batch_size
    demo_batch_size = len(episodes) // rloo_batch_size if len(episodes) >= rloo_batch_size else 1
    
    return compute_advantages_rloo_grouped(
        episodes=episodes,
        rloo_batch_size=rloo_batch_size,
        demo_batch_size=demo_batch_size
    )


def compute_advantages_rloo_old(episodes: List[Dict], rloo_batch_size: int = None) -> torch.Tensor:
    """
    åŸæœ‰çš„RLOOä¼˜åŠ¿è®¡ç®—ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰
    âš ï¸ æ³¨æ„ï¼šè¿™ä¸ªç‰ˆæœ¬ä¸éªŒè¯åˆ†ç»„ç»“æ„ï¼Œå¯èƒ½å¯¼è‡´åŸºçº¿ä¼°è®¡ä¸å‡†ç¡®
    """
    if not episodes:
        return torch.tensor([])
    
    # æå–å¥–åŠ±
    rewards = []
    for ep in episodes:
        reward = ep.get('total_reward', 0.0)
        rewards.append(float(reward))
    
    rlhf_reward = torch.tensor(rewards, dtype=torch.float32)
    num_rollouts = len(episodes)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨çœŸæ­£çš„RLOOæ‰¹æ¬¡å¤§å°è€Œä¸æ˜¯æ€»æ•°
    if rloo_batch_size is None or rloo_batch_size <= 1:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæˆ–batch sizeè¿‡å°ï¼Œé€€åŒ–ä¸ºç®€å•æ–¹æ³•
        print("âš ï¸ RLOO batch sizeæœªæŒ‡å®šæˆ–è¿‡å°ï¼Œä½¿ç”¨ç®€å•ä¼˜åŠ¿è®¡ç®—")
        advantage = rlhf_reward - rlhf_reward.mean()
    else:
        # ğŸš€ æ­£å®—RLOOè®¡ç®—
        try:
            # ç¡®ä¿å¯ä»¥æ•´é™¤ï¼Œå¦‚æœä¸èƒ½æ•´é™¤åˆ™è£å‰ªåˆ°æœ€å¤§å¯æ•´é™¤æ•°é‡
            effective_rollouts = (num_rollouts // rloo_batch_size) * rloo_batch_size
            if effective_rollouts != num_rollouts:
                print(f"ğŸ”§ RLOOè°ƒæ•´ï¼š{num_rollouts} â†’ {effective_rollouts} rollouts (batch_size={rloo_batch_size})")
                rlhf_reward = rlhf_reward[:effective_rollouts]
                num_rollouts = effective_rollouts
            
            num_batches = num_rollouts // rloo_batch_size
            rlhf_reward_reshaped = rlhf_reward.reshape(num_batches, rloo_batch_size)
            
            # æ ‡å‡†RLOOï¼šæ¯ä¸ªæ ·æœ¬çš„baseline = åŒæ‰¹æ¬¡å…¶ä»–æ ·æœ¬çš„å¹³å‡å€¼
            # baseline[i,j] = (sum(batch[i]) - reward[i,j]) / (batch_size - 1)
            batch_sums = rlhf_reward_reshaped.sum(dim=1, keepdim=True)  # (num_batches, 1)
            baseline = (batch_sums - rlhf_reward_reshaped) / (rloo_batch_size - 1)  # (num_batches, rloo_batch_size)
            
            # ä¼˜åŠ¿ = è‡ªå·±çš„å¥–åŠ± - å…¶ä»–äººçš„å¹³å‡å¥–åŠ±
            advantage = rlhf_reward_reshaped - baseline  # (num_batches, rloo_batch_size)
            advantage = advantage.flatten()  # å±•å¹³ä¸ºä¸€ç»´
            
            # NaNå’ŒInfæ£€æŸ¥
            if torch.isnan(advantage).any() or torch.isinf(advantage).any():
                print("âš ï¸ RLOOè®¡ç®—äº§ç”ŸNaN/Infï¼Œä½¿ç”¨å®‰å…¨æ›¿æ¢")
                advantage = torch.nan_to_num(advantage, nan=0.0, posinf=1.0, neginf=-1.0)
            
            print(f"ğŸ¯ æ­£å®—RLOOä¼˜åŠ¿è®¡ç®—å®Œæˆ:")
            print(f"   æ‰¹æ¬¡é…ç½®: {num_rollouts} rollouts â†’ {num_batches} batches Ã— {rloo_batch_size}")
            print(f"   ä¼˜åŠ¿ç»Ÿè®¡: mean={advantage.mean():.4f}, std={advantage.std():.4f}")
            print(f"   æ­£ä¼˜åŠ¿æ¯”ä¾‹: {(advantage > 0).float().mean():.2%}")
            
        except Exception as e:
            print(f"âŒ RLOOè®¡ç®—å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•")
            advantage = rlhf_reward - rlhf_reward.mean()
    
    return advantage

def update_policy_ript_vla_style(policy, optimizer, cfg_adapter, episodes, advantages, device):
    """
    RIPT-VLAé£æ ¼çš„ç­–ç•¥æ›´æ–°
    ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†ï¼Œæ— å¤æ‚ç»„ä»¶
    """
    if not episodes or len(advantages) == 0:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œç­–ç•¥æ›´æ–°")
        return 0.0
    
    print(f"æ­£åœ¨æ›´æ–°ç­–ç•¥ï¼ˆ{len(episodes)} ä¸ªepisodesï¼‰...")
    
    try:
        # è®¡ç®—åŠ æƒæŸå¤±
        advantages = advantages.to(device)
        loss = cfg_adapter.compute_weighted_loss(episodes, advantages, device)
        
        # æ¢¯åº¦æ›´æ–°
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
        
        optimizer.step()
        
        loss_value = loss.item()
        print(f"âœ“ ç­–ç•¥æ›´æ–°å®Œæˆï¼ŒæŸå¤±: {loss_value:.6f}")
        
        return loss_value
        
    except Exception as e:
        print(f"âŒ ç­–ç•¥æ›´æ–°å¤±è´¥: {e}")
        traceback.print_exc()
        return 0.0

def evaluate_with_cfg_sweep(policy, env_runner, task_name, config, eval_episodes=3):
    """ğŸ”¥ æ–°å¢ï¼šè¯„ä¼°ä¸åŒCFGå¼ºåº¦çš„æ•ˆæœï¼ˆå®Œå…¨é…ç½®åŒ–ï¼Œæ— ç¡¬ç¼–ç ï¼‰"""
    # ä»é…ç½®æ–‡ä»¶è¯»å–CFGæ‰«æå‚æ•°
    cfg_sweep_config = get_config_value(config, 'cfg_sweep_config', {}, ['features'])
    
    if not cfg_sweep_config.get('enabled', True):
        print("âš ï¸ CFGæ‰«æè¯„ä¼°å·²ç¦ç”¨")
        return get_config_value(config, 'collection_cfg_scale'), {}
    
    cfg_scales = cfg_sweep_config.get('scales', [1.0, 1.25, 1.5, 2.0, 3.0])
    best_cfg = cfg_scales[0] if cfg_scales else 1.0  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé…ç½®å€¼ä½œä¸ºé»˜è®¤
    best_success_rate = 0.0
    
    results = {}
    print(f"\nğŸ” å¼€å§‹CFGå¼ºåº¦æ‰«æè¯„ä¼°ï¼ˆå®Œå…¨é…ç½®åŒ–ï¼‰...")
    print(f"   æ‰«æèŒƒå›´: {cfg_scales}")
    print(f"   æ¯ä¸ªCFGè¯„ä¼°è½®æ•°: {eval_episodes}")
    
    for cfg_scale in cfg_scales:
        print(f"ğŸ“Š æµ‹è¯•CFG={cfg_scale}...")
        # ä¸´æ—¶è®¾ç½®CFGå¼ºåº¦
        original_cfg = get_config_value(config, 'collection_cfg_scale')
        set_config_value(config, 'collection_cfg_scale', cfg_scale)
        env_runner.config.collection_cfg_scale = cfg_scale
        
        # è¿è¡Œè¯„ä¼°episodes
        success_count = 0
        for ep_idx in range(eval_episodes):
            try:
                # ä½¿ç”¨ç°æœ‰çš„rolloutæ”¶é›†å‡½æ•° - æ­£ç¡®è§£åŒ…å’Œä¼ å‚
                episodes, valid_mask = collect_rollouts_ript_vla_style(
                    env_runner, task_name, 1,
                    dynamic_sampling_config=None,  # è¯„ä¼°æ—¶å…³é—­åŠ¨æ€é‡‡æ ·
                    recent_success_rates=None,
                    rollout_goal_per_step=None,
                    rollout_stats=None,
                    rollout_skip_cnt=None,
                    rloo_batch_size=1
                )
                if episodes and len(episodes) > 0:
                    if episodes[0].get('success', False):
                        success_count += 1
            except Exception as e:
                print(f"   è¯„ä¼°episode {ep_idx} å¤±è´¥: {e}")
                continue
        
        success_rate = success_count / eval_episodes
        results[cfg_scale] = success_rate
        
        if success_rate > best_success_rate:
            best_success_rate = success_rate
            best_cfg = cfg_scale
        
        # æ¢å¤åŸè®¾ç½®
        set_config_value(config, 'collection_cfg_scale', original_cfg)
        env_runner.config.collection_cfg_scale = original_cfg
        
        print(f"   CFG={cfg_scale}: æˆåŠŸç‡={success_rate:.2%} ({success_count}/{eval_episodes})")
    
    print(f"ğŸ¯ æœ€ä½³CFGå¼ºåº¦: {best_cfg} (æˆåŠŸç‡: {best_success_rate:.2%})")
    return best_cfg, results

def main_training_loop_ript_vla_style(config: Dict[str, Any]):
    """
    ä¸»è®­ç»ƒå¾ªç¯ï¼ˆRIPT-VLAé£æ ¼ï¼‰+ åŠ¨æ€é‡‡æ · + æ–‡ä»¶è®¡æ•°å™¨
    ç›´æ¥åœ¨ä¸»å‡½æ•°ä¸­å¤„ç†æ‰€æœ‰é€»è¾‘ï¼Œå‡å°‘æŠ½è±¡å±‚
    """
    print("ğŸš€ å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒå¾ªç¯")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(config['output_dir'])
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_name = config.get('exp_name', 'ript_vla_style_train')
    output_dir = output_dir / f"{exp_name}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # ğŸ”¥ æ–°å¢ï¼šè¯»å–å¢å¼ºé…ç½®
    features_config = config.get('features', {})
    dynamic_sampling_config = features_config.get('dynamic_sampling', {})
    enable_file_counter = features_config.get('enable_file_counter', False)
    adaptive_cfg_enabled = features_config.get('adaptive_cfg', False)
    
    # ğŸ”¥ ä¿®æ­£ï¼šç‹¬ç«‹é…ç½®demo_batch_sizeï¼Œä¸å†ç­‰äºrloo_batch_size
    demo_batch_size = get_config_value(config, 'demo_batch_size', None, ['algo', 'dataset'])
    if demo_batch_size is None:
        # å°è¯•ä»dataset.num_init_statesè·å–
        demo_batch_size = get_config_value(config, 'num_init_states', 22, ['dataset'])
        print(f"âš ï¸ demo_batch_sizeæœªé…ç½®ï¼Œä½¿ç”¨dataset.num_init_states={demo_batch_size}")
    else:
        print(f"âœ… ä½¿ç”¨é…ç½®çš„demo_batch_size={demo_batch_size}")
    
    # ğŸ”¥ ä¿®å¤ï¼šæå‰è¯»å–rloo_batch_sizeï¼ˆåœ¨ä½¿ç”¨å‰å®šä¹‰ï¼‰
    rloo_batch_size = config['algo']['rloo_batch_size']
    
    world_size = 1  # å½“å‰å•æœºï¼Œåç»­å¯ä»åˆ†å¸ƒå¼ç¯å¢ƒè¯»å–
    early_stop_percentage = features_config.get('early_stop_percentage', 0.8)  # æ–°å¢é…ç½®é¡¹
    
    # ğŸ”¥ æ–°å¢ï¼šç»Ÿä¸€é…ç½®è¯»å– - rollout_statsç›¸å…³
    enable_rollout_stats_tracking = get_config_value(config, 'enable_rollout_stats_tracking', False, ['features', 'algo'])
    rollout_stats_path = get_config_value(config, 'rollout_stats_path', str(output_dir / "rollout_stats.json"), ['features', 'algo'])
    
    if enable_file_counter:
        # å¯¹é½RIPTï¼šæŒ‰ç»„æ•°æ—©åœï¼ˆæ¯ä¸ªè¢«æ¥å—çš„ç»„è®¡1ï¼‰ï¼Œé˜ˆå€¼æŒ‰ç»„è®¡ç®—
        total_target_groups = demo_batch_size * world_size
        rollout_goal_per_step = int(np.ceil(total_target_groups * early_stop_percentage))
        print(f"ğŸ¯ æ—©åœé˜ˆå€¼è®¡ç®—: {rollout_goal_per_step} ç»„ = {demo_batch_size}ç»„ Ã— {world_size}GPU Ã— {early_stop_percentage:.0%}")
    else:
        rollout_goal_per_step = None
    
    print(f"\n=== RIPTå¯¹é½éªŒè¯ ===")
    print(f"demo_batch_size: {demo_batch_size} (æ¯æ­¥æ”¶é›†çš„ç»„æ•°)")
    print(f"rloo_batch_size: {rloo_batch_size} (æ¯ç»„å†…æ ·æœ¬æ•°)") 
    print(f"ç›®æ ‡æ ·æœ¬æ•°/æ­¥: {demo_batch_size * rloo_batch_size}")
    if rollout_goal_per_step:
        print(f"æ—©åœé˜ˆå€¼: {rollout_goal_per_step}")
    else:
        print("æ—©åœé˜ˆå€¼: æœªå¯ç”¨")
    
    print(f"\nğŸ”§ å¢å¼ºåŠŸèƒ½é…ç½®:")
    print(f"  åŠ¨æ€é‡‡æ ·: {'âœ…' if dynamic_sampling_config.get('enabled', False) else 'âŒ'}")
    if dynamic_sampling_config.get('enabled', False):
        print(f"    æ¨¡å¼: RIPTé£æ ¼ (ç®€å•å…¨æˆåŠŸ/å…¨å¤±è´¥æ£€æŸ¥)")
        print(f"    åŒºé—´: [{dynamic_sampling_config.get('p_min', 0.1)}, {dynamic_sampling_config.get('p_max', 0.9)}] (å¤‡ç”¨)")
        print(f"    æ³¨ï¼šRIPTé£æ ¼ä¸ä½¿ç”¨å¹³æ»‘çª—å£ï¼Œç›´æ¥æ£€æŸ¥å½“å‰ç»„çš„æˆåŠŸç‡")
    print(f"  æ–‡ä»¶è®¡æ•°å™¨: {'âœ…' if enable_file_counter else 'âŒ'}")
    if rollout_goal_per_step:
        print(f"    æ¯æ­¥å…¨å±€ç›®æ ‡: {rollout_goal_per_step}")
    print(f"  è‡ªé€‚åº”CFG: {'âœ…' if adaptive_cfg_enabled else 'âŒ'}")
    
    # ğŸ”¥ ç®€åŒ–ï¼šç§»é™¤å¤æ‚çš„å¹³æ»‘çª—å£ï¼ˆRIPTåŸç‰ˆä¸éœ€è¦ï¼‰
    # smooth_window_size = dynamic_sampling_config.get('smooth_window', 3)
    # recent_success_rates = deque(maxlen=smooth_window_size)
    
    # ğŸ”¥ æ–°å¢ï¼šper-init å“ˆå¸Œè·³è¿‡æœºåˆ¶åˆå§‹åŒ–
    
    rollout_stats = {}
    rollout_skip_cnt = {}
    
    if enable_rollout_stats_tracking:
        rollout_stats = load_rollout_stats(rollout_stats_path)
        rollout_skip_cnt = {k: 0 for k in rollout_stats.keys()}
        print(f"ğŸ“Š å·²åŠ è½½ {len(rollout_stats)} ä¸ªåˆå§‹çŠ¶æ€çš„å†å²è®°å½•")
        print(f"ğŸ’¾ rollout_stats è·¯å¾„: {rollout_stats_path}")
    
    # åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨
    print("æ­£åœ¨åŠ è½½PI0ç­–ç•¥...")
    
    # ğŸ”¥ æ­£ç¡®çš„ç­–ç•¥åŠ è½½é€»è¾‘ï¼Œä¸¥æ ¼éµå®ˆcfg_enabledé…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    policy_path = config['policy_path']
    
    # åŠ è½½ç­–ç•¥
    policy = PI0Policy.from_pretrained(policy_path, local_files_only=True)
    print("Loading weights from local directory")
    policy = policy.to(device)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§cfg_enabledè®¾ç½®CFGåŠŸèƒ½
    cfg_enabled = config.get('policy', {}).get('cfg_enabled', True)
    
    if cfg_enabled:
        print(f"ğŸ”§ å¯ç”¨CFGåŠŸèƒ½ï¼ˆæŒ‰é…ç½®è¦æ±‚ï¼‰...")
        # åŒæ­¥åˆ°Policyã€Modelä¸Configä¸‰å¤„ï¼Œç¡®ä¿sample_actionsèµ°åŒåˆ†æ”¯è·¯å¾„
        policy.cfg_enabled = True
        if hasattr(policy, 'model'):
            setattr(policy.model, 'cfg_enabled', True)
        if hasattr(policy, 'config'):
            setattr(policy.config, 'cfg_enabled', True)
        # ä»é…ç½®è¯»å–collection_cfg_scale
        policy.default_cfg_scale = config.get('algo', {}).get('collection_cfg_scale', 1.25)
        print(f"âœ… CFGå·²å¯ç”¨ï¼Œé»˜è®¤CFGå¼ºåº¦: {policy.default_cfg_scale}")
    else:
        print(f"ğŸ”§ ç¦ç”¨CFGåŠŸèƒ½ï¼ˆæŒ‰é…ç½®è¦æ±‚ï¼‰...")
        policy.cfg_enabled = False
        if hasattr(policy, 'model'):
            setattr(policy.model, 'cfg_enabled', False)
        if hasattr(policy, 'config'):
            setattr(policy.config, 'cfg_enabled', False)
        policy.default_cfg_scale = 1.0  # å¼ºåˆ¶è®¾ä¸º1.0ï¼Œå®Œå…¨ç¦ç”¨CFG
        print(f"âœ… CFGå·²ç¦ç”¨ï¼Œå¼ºåˆ¶CFGå¼ºåº¦: {policy.default_cfg_scale}")
    
    print(f"âœ“ ç­–ç•¥åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    print("æ­£åœ¨åˆ›å»ºä¼˜åŒ–å™¨...")
    if config.get('policy', {}).get('train_expert_only', False):
        # åªè®­ç»ƒä¸“å®¶å¤´éƒ¨
        expert_params = []
        for name, param in policy.named_parameters():
            if 'expert' in name or 'cfg_embedding' in name:  # åŒ…å«CFG embeddingå‚æ•°
                expert_params.append(param)
        
        print("ğŸ”§ é…ç½®è®­ç»ƒå‚æ•°èŒƒå›´...")
        if hasattr(policy, 'cfg_embedding') and policy.cfg_embedding is not None:
            cfg_params = list(policy.cfg_embedding.parameters())
            expert_params.extend(cfg_params)
            print("âœ… CFG embeddingå‚æ•°å·²åŠ å…¥è®­ç»ƒ")
        
        optimizer = torch.optim.AdamW(expert_params, lr=config['algo']['lr'])
        total_params = sum(p.numel() for p in expert_params)
        print(f"ğŸ¯ åªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå‚æ•°æ•°é‡: {total_params:,}")
    else:
        # è®­ç»ƒæ‰€æœ‰å‚æ•°
        optimizer = torch.optim.AdamW(policy.parameters(), lr=config['algo']['lr'])
        total_params = sum(p.numel() for p in policy.parameters())
        print(f"ğŸ¯ è®­ç»ƒæ‰€æœ‰å‚æ•°ï¼Œå‚æ•°æ•°é‡: {total_params:,}")
    
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼Œå­¦ä¹ ç‡: {config['algo']['lr']}")
    
    # è¿”å›åˆ›å»ºçš„ç»„ä»¶
    # policy, optimizer, device = policy, optimizer, device
    
    # åˆ›å»ºCFGé€‚é…å™¨ï¼ˆå¿…éœ€ï¼Œç”¨äºæŸå¤±è®¡ç®—ï¼‰
    # ğŸ”¥ æ–°å¢ï¼šçª—å£åŒ–é…ç½®æ”¯æŒ
    dataset_config = config.get('dataset', {})
    windowing_mode = dataset_config.get('windowing_mode', 'last')
    window_stride = dataset_config.get('window_stride', 10)
    max_windows_per_episode = dataset_config.get('max_windows_per_episode', 1)
    
    print(f"\nğŸ”§ CFGçª—å£åŒ–é…ç½®:")
    print(f"  æ¨¡å¼: {windowing_mode}")
    print(f"  æ­¥é•¿: {window_stride}")
    print(f"  æ¯episodeæœ€å¤§çª—å£æ•°: {max_windows_per_episode}")
    
    cfg_adapter = PI0_CFG_Adapter(
        policy=policy,
        norm_stats_path=f"{config['policy_path']}/norm_stats.json",
        windowing_mode=windowing_mode,
        window_stride=window_stride,
        max_windows_per_episode=max_windows_per_episode
    )
    
    # åˆ›å»ºç¯å¢ƒrunnerï¼ˆä½¿ç”¨æ ‡å‡†å·¥å‚ï¼Œç¡®ä¿å¹¶è¡Œä¸benchmarké…ç½®æ­£ç¡®ä¸‹å‘ï¼‰
    env_runner = create_environment_runner(config, policy)

    # ğŸ”¥ åŒæ­¥CFGå¼ºåº¦åˆ°runnerï¼ˆrunnerå†…éƒ¨è¯»å–çš„æ˜¯æ ¹çº§å­—æ®µï¼Œè¿™é‡Œæ˜ç¡®è®¾ç½®ï¼‰
    if hasattr(env_runner, 'config') and env_runner.config is not None:
        env_runner.config.collection_cfg_scale = (policy.default_cfg_scale if cfg_enabled else 1.0)
    else:
        env_runner.collection_cfg_scale = (policy.default_cfg_scale if cfg_enabled else 1.0)
    print(f"ğŸ”§ ç¯å¢ƒrunner CFGå¼ºåº¦è®¾ç½®ä¸º: {policy.default_cfg_scale if cfg_enabled else 1.0}")

    print("âœ“ ç¯å¢ƒrunneråˆ›å»ºæˆåŠŸ")
    
    # ğŸ”¥ æ–°å¢ï¼šéªŒè¯ç¯å¢ƒrunneråŠŸèƒ½
    print(f"\nğŸ”§ ç¯å¢ƒrunneråŠŸèƒ½éªŒè¯:")
    if hasattr(env_runner, 'run_policy_in_env_batch'):
        print("  æ‰¹é‡æ‰§è¡Œ: âœ…")
    else:
        print("  æ‰¹é‡æ‰§è¡Œ: âŒ (å°†å›é€€åˆ°ä¸²è¡Œ)")

    # éªŒè¯åˆå§‹çŠ¶æ€æ± 
    if hasattr(env_runner, 'get_task_init_states'):
        try:
            test_states = env_runner.get_task_init_states(0)
            if test_states:
                print(f"  åˆå§‹çŠ¶æ€æ± : âœ… ({len(test_states)}ä¸ªçŠ¶æ€)")
            else:
                print("  åˆå§‹çŠ¶æ€æ± : âš ï¸ (ä¸ºç©º)")
        except Exception as e:
            print(f"  åˆå§‹çŠ¶æ€æ± : âŒ (è·å–å¤±è´¥: {e})")
    else:
        print("  åˆå§‹çŠ¶æ€æ± : âŒ (ä¸æ”¯æŒget_task_init_states)")
    
    # ğŸ”¥ æ–°å¢ï¼šæ–‡ä»¶è®¡æ•°å™¨åˆå§‹åŒ–
    if enable_file_counter:
        file_counter = env_runner.setup_file_counter(counter_name="rollout", work_dir=str(output_dir))
        if file_counter:
            print(f"âœ… æ–‡ä»¶è®¡æ•°å™¨å·²å¯ç”¨: {output_dir}/rollout_counter")
        else:
            print(f"âš ï¸ æ–‡ä»¶è®¡æ•°å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ™®é€šæ¨¡å¼")
            enable_file_counter = False
    
    # è®­ç»ƒé…ç½®
    num_train_steps = config['training']['num_train_steps']
    # ä¸2_test_pi0_on_libero.pyå¯¹é½ï¼šä½¿ç”¨libero_goalåŸºå‡†é»˜è®¤task_id=1
    # è‹¥YAMLä¸­æ˜ç¡®ç»™äº†task_names_to_useï¼Œåˆ™ä»ç„¶ä½¿ç”¨ç¬¬ä¸€ä¸ªåç§°åšæ˜¾ç¤ºï¼Œä¸å½±å“ç¯å¢ƒå†…éƒ¨task_idé€‰æ‹©
    task_name = config['task'].get('task_names_to_use', ['libero_goal_default'])[0]
    
    print(f"\nå¼€å§‹è®­ç»ƒå¾ªç¯:")
    print(f"  è®­ç»ƒæ­¥æ•°: {num_train_steps}")
    print(f"  ä»»åŠ¡: {task_name}")
    print(f"  æ‰¹æ¬¡å¤§å°: {rloo_batch_size}")
    print()
    
    all_training_metrics = []
    
    # ğŸ”¥ ä¸»è®­ç»ƒå¾ªç¯ - RIPT-VLAé£æ ¼
    for step in range(num_train_steps):
        step_start_time = time.time()
        
        print(f"=== è®­ç»ƒæ­¥éª¤ {step + 1}/{num_train_steps} ===")
        
        # 1. æ”¶é›†rolloutsï¼ˆRIPTåŸç‰ˆé£æ ¼ï¼šæŒ‰åˆå§‹çŠ¶æ€åˆ†ç»„ï¼‰
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ä¼˜åŒ–çš„RIPTåŠ¨æ€é‡‡æ ·ï¼Œæ”¯æŒå¤šç§ç­–ç•¥
        enable_ript_dynamic_sampling = dynamic_sampling_config.get('enabled', True) if dynamic_sampling_config else True
        
        episodes, valid_mask = collect_rollouts_ript_vla_style_grouped(
            env_runner=env_runner,
            task_name=task_name, 
            demo_batch_size=demo_batch_size,  # éœ€è¦å¤šå°‘ä¸ªä¸åŒçš„åˆå§‹çŠ¶æ€
            rloo_batch_size=rloo_batch_size,  # æ¯ä¸ªåˆå§‹çŠ¶æ€æ”¶é›†å¤šå°‘ä¸ªæ ·æœ¬
            enable_ript_dynamic_sampling=enable_ript_dynamic_sampling,
            rollout_goal_per_step=rollout_goal_per_step,
            rollout_stats=rollout_stats if enable_rollout_stats_tracking else None,
            rollout_skip_cnt=rollout_skip_cnt if enable_rollout_stats_tracking else None
        )
        
        if not episodes:
            print("âš ï¸ æœªæ”¶é›†åˆ°æœ‰æ•ˆepisodesï¼Œè·³è¿‡æ­¤æ­¥")
            continue
        
        # è®°å½•æœ‰æ•ˆæ ·æœ¬æ•°é‡
        valid_count = sum(valid_mask) if valid_mask else len(episodes)
        step_metrics = {
            'step': step + 1,
            'num_episodes': len(episodes),
            'valid_episodes': valid_count,
            'padding_episodes': len(episodes) - valid_count
        }
        
        # 2. ğŸ”¥ æ–°å¢ï¼šä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œæ›´æ–°ç»Ÿè®¡ï¼ˆåœ¨ CFG adapter å¤„ç†åï¼‰
        if enable_rollout_stats_tracking:
            try:
                # é€šè¿‡ CFG adapter å¤„ç† episodes ä»¥è·å¾—æ­£ç¡®çš„çŠ¶æ€è¡¨ç¤º
                batch, owner_indices = cfg_adapter.process_episodes(episodes, device)
                batch_states = batch.get('state')  # (B, state_dim)
                
                # ä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œæ›´æ–°ç»Ÿè®¡
                update_rollout_stats_with_correct_hash(
                    episodes, batch_states, None, owner_indices, 
                    rollout_stats, rollout_skip_cnt
                )
                
                print(f"ğŸ”„ å·²ç”¨æ­£ç¡®çŠ¶æ€å“ˆå¸Œæ›´æ–° {len(episodes)} ä¸ªepisodesçš„ç»Ÿè®¡")
                
            except Exception as e:
                print(f"âš ï¸ æ­£ç¡®å“ˆå¸Œæ›´æ–°å¤±è´¥: {e}")
        
        # 3. è®¡ç®—ä¼˜åŠ¿ï¼ˆå¢å¼ºç‰ˆåˆ†ç»„RLOOæ–¹æ³•ï¼ŒéªŒè¯åˆå§‹çŠ¶æ€åˆ†ç»„ï¼‰
        advantages = compute_advantages_rloo_grouped(
            episodes=episodes, 
            rloo_batch_size=rloo_batch_size,
            demo_batch_size=demo_batch_size  # ä¼ é€’åˆ†ç»„ä¿¡æ¯ç”¨äºéªŒè¯
        )
        
        # 4. æ›´æ–°ç­–ç•¥ï¼ˆç›´æ¥æ›´æ–°ï¼Œæ— å¤æ‚ç»„ä»¶ï¼‰
        loss = update_policy_ript_vla_style(
            policy, optimizer, cfg_adapter, episodes, advantages, device
        )
        
        # 5. è®°å½•æŒ‡æ ‡
        avg_reward = np.mean([ep['total_reward'] for ep in episodes])
        success_rate = np.mean([ep['success'] for ep in episodes])
        step_time = time.time() - step_start_time
        
        # æ›´æ–°step_metrics
        step_metrics.update({
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'loss': loss,
            'step_time': step_time
        })
        all_training_metrics.append(step_metrics)
        
        # 6. è¾“å‡ºç»“æœ
        print(f"âœ“ æ­¥éª¤ {step + 1} å®Œæˆ:")
        print(f"  Episodes: {len(episodes)} (æœ‰æ•ˆ: {valid_count}, å¡«å……: {len(episodes) - valid_count})")
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        print(f"  æˆåŠŸç‡: {success_rate:.2%}")
        print(f"  æŸå¤±: {loss:.6f}")
        print(f"  è€—æ—¶: {step_time:.2f}ç§’")
        
        # 7. è‡ªé€‚åº”CFGè°ƒæ•´ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼ŒåŸºäºå½“å‰æ­¥éª¤æˆåŠŸç‡ï¼‰
        if adaptive_cfg_enabled and len(episodes) > 0:
            try:
                # è®¡ç®—å½“å‰æ­¥éª¤çš„æ€»ä½“æˆåŠŸç‡
                current_successes = [ep.get('success', False) for ep in episodes]
                current_success_rate = np.mean(current_successes)
                current_cfg = get_config_value(config, 'collection_cfg_scale')
                
                # ğŸ”¥ å®Œå…¨é…ç½®åŒ–çš„CFGè°ƒæ•´é€»è¾‘
                adaptive_cfg_config = get_config_value(config, 'adaptive_cfg_config', {}, ['features'])
                
                min_cfg = adaptive_cfg_config.get('min_cfg', 1.0)
                max_cfg = adaptive_cfg_config.get('max_cfg', 3.0)
                cfg_step = adaptive_cfg_config.get('cfg_step', 0.2)
                low_threshold = adaptive_cfg_config.get('low_success_threshold', 0.2)
                high_threshold = adaptive_cfg_config.get('high_success_threshold', 0.9)
                
                if current_success_rate < low_threshold:  # æˆåŠŸç‡å¾ˆä½
                    new_cfg = max(min_cfg, current_cfg - cfg_step)
                    print(f"ğŸ”§ è‡ªé€‚åº”CFG: æˆåŠŸç‡è¿‡ä½({current_success_rate:.3f} < {low_threshold})ï¼Œé™ä½CFG {current_cfg:.2f} â†’ {new_cfg:.2f}")
                    # ç»Ÿä¸€å†™å›é…ç½®
                    set_config_value(config, 'collection_cfg_scale', new_cfg)
                    env_runner.config.collection_cfg_scale = new_cfg
                elif current_success_rate > high_threshold:  # æˆåŠŸç‡å¾ˆé«˜
                    new_cfg = min(max_cfg, current_cfg + cfg_step)
                    print(f"ğŸ”§ è‡ªé€‚åº”CFG: æˆåŠŸç‡è¿‡é«˜({current_success_rate:.3f} > {high_threshold})ï¼Œæå‡CFG {current_cfg:.2f} â†’ {new_cfg:.2f}")
                    # ç»Ÿä¸€å†™å›é…ç½®
                    set_config_value(config, 'collection_cfg_scale', new_cfg)
                    env_runner.config.collection_cfg_scale = new_cfg
                else:
                    print(f"âœ… è‡ªé€‚åº”CFG: æˆåŠŸç‡é€‚ä¸­({current_success_rate:.3f})ï¼Œä¿æŒCFG={current_cfg:.2f}")
                
                step_metrics['adaptive_cfg_scale'] = get_config_value(config, 'collection_cfg_scale')
                step_metrics['current_success_rate'] = current_success_rate
                
            except Exception as e:
                print(f"âš ï¸ è‡ªé€‚åº”CFGè°ƒæ•´å¤±è´¥: {e}")
        
        # 8. CFGè¯„ä¼°ï¼ˆæ¯10æ­¥è¿›è¡Œä¸€æ¬¡ï¼‰
        if (step + 1) % 10 == 0:
            try:
                # ä»é…ç½®æ–‡ä»¶è¯»å–eval_episodesï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™é»˜è®¤ä¸º2
                cfg_sweep_config = get_config_value(config, 'cfg_sweep_config', {}, ['features'])
                eval_episodes = cfg_sweep_config.get('eval_episodes', 2)
                best_cfg, cfg_results = evaluate_with_cfg_sweep(policy, env_runner, task_name, config, eval_episodes=eval_episodes)
                step_metrics['best_cfg_scale'] = best_cfg
                step_metrics['cfg_sweep_results'] = cfg_results
                print(f"ğŸ¯ æ¨èCFGå¼ºåº¦: {best_cfg}")
                # æ³¨æ„ï¼šå¦‚æœå¯ç”¨äº†è‡ªé€‚åº”CFGï¼Œè¿™é‡Œä¸å¼ºåˆ¶è¦†ç›–
                if not adaptive_cfg_enabled:
                    env_runner.config.collection_cfg_scale = best_cfg
            except Exception as e:
                print(f"âš ï¸ CFGè¯„ä¼°å¤±è´¥: {e}")
        
        # 9. å®šæœŸä¿å­˜ rollout_stats
        if enable_rollout_stats_tracking and (step + 1) % 5 == 0:  # æ¯5æ­¥ä¿å­˜ä¸€æ¬¡
            save_rollout_stats(rollout_stats, rollout_stats_path)
            print(f"ğŸ’¾ å·²ä¿å­˜ rollout_stats ({len(rollout_stats)} ä¸ªåˆå§‹çŠ¶æ€)")
        
        # 10. ä¿å­˜æ£€æŸ¥ç‚¹
        if (step + 1) % config['training'].get('save_freq', 10) == 0:
            # è½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼Œä¾¿äºéƒ¨ç½²ä¸å ç”¨å°ï¼‰
            weights_path = output_dir / f"weights_step_{step + 1}.pt"
            torch.save({
                'step': step + 1,
                'policy_state_dict': policy.state_dict(),
                'config': config,
                'training_metrics': all_training_metrics,
            }, weights_path)
            print(f"âœ“ è½»é‡æƒé‡å·²ä¿å­˜: {weights_path}")

            # å¯é€‰ï¼šæŒ‰è¾ƒä½é¢‘ç‡ä¿å­˜å«ä¼˜åŒ–å™¨çš„å®Œæ•´æ£€æŸ¥ç‚¹ï¼Œä¾¿äºæ¢å¤è®­ç»ƒ
            save_opt_every = config.get('training', {}).get('save_optimizer_freq', None)
            if save_opt_every and ((step + 1) % int(save_opt_every) == 0):
                checkpoint_path = output_dir / f"checkpoint_step_{step + 1}.pt"
                torch.save({
                    'step': step + 1,
                    'policy_state_dict': policy.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'config': config,
                    'training_metrics': all_training_metrics,
                }, checkpoint_path)
                print(f"âœ“ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results_path = output_dir / "final_training_results.json"
    # å°† OmegaConf è½¬ä¸ºåŸç”Ÿ dict ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if OMEGACONF_AVAILABLE and isinstance(config, DictConfig):
        serializable_config = OmegaConf.to_container(config, resolve=True)
    else:
        serializable_config = config
    with open(final_results_path, 'w') as f:
        json.dump({
            'config': serializable_config,
            'training_metrics': all_training_metrics,
            'total_steps': len(all_training_metrics)
        }, f, indent=2)
    
    # æœ€ç»ˆè½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼‰
    final_weights_path = output_dir / "final_weights.pt"
    torch.save({
        'step': len(all_training_metrics),
        'policy_state_dict': policy.state_dict(),
        'config': config,
        'training_metrics': all_training_metrics,
    }, final_weights_path)
    print(f"âœ“ æœ€ç»ˆè½»é‡æƒé‡å·²ä¿å­˜: {final_weights_path}")

    # å¯é€‰ï¼šä¿å­˜æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆå«ä¼˜åŒ–å™¨ï¼‰ä¾¿äºæ¢å¤è®­ç»ƒ
    if config.get('training', {}).get('save_optimizer_final', False):
        final_checkpoint_path = output_dir / "final_checkpoint.pt"
        torch.save({
            'step': len(all_training_metrics),
            'policy_state_dict': policy.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'config': config,
            'training_metrics': all_training_metrics,
        }, final_checkpoint_path)
        print(f"âœ“ æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")

    # æœ€ç»ˆä¿å­˜ rollout_stats
    if enable_rollout_stats_tracking:
        save_rollout_stats(rollout_stats, rollout_stats_path)
        print(f"ğŸ’¾ æœ€ç»ˆä¿å­˜ rollout_stats: {rollout_stats_path}")

    print(f"\nğŸ‰ RIPT-VLAé£æ ¼è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœå·²ä¿å­˜: {final_results_path}")
    print(f"âœ¨ ä½¿ç”¨äº†ç®€åŒ–çš„ç›´æ¥æ¶æ„ï¼Œå‡å°‘äº†æŠ½è±¡å±‚å¤æ‚åº¦")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ")
    parser.add_argument(
        "--config_path", 
        type=str, 
        required=True,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config_path)
        
        # æ˜¾ç¤ºé…ç½®
        print("\n====== ä½¿ç”¨é…ç½® ======")
        if OMEGACONF_AVAILABLE:
            print(OmegaConf.to_yaml(config))
        else:
            print(yaml.dump(config, default_flow_style=False, allow_unicode=True))
        print("====================\n")
        
        # å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒ
        main_training_loop_ript_vla_style(config)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()