#!/usr/bin/env python3
"""
Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–ç‰ˆæœ¬
åŸºäºRIPT-VLAçš„ç›´æ¥æ¶æ„æ¨¡å¼ï¼Œå»é™¤å¤šä½™çš„æŠ½è±¡å±‚

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
1. ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†rolloutæ”¶é›†å’Œä¼˜åŒ–
2. ç®€åŒ–çš„ç»„ä»¶æ¶æ„ï¼Œå‡å°‘ä¸­é—´å±‚
3. ç›´æ¥ä½¿ç”¨SubprocVectorEnvè¿›è¡Œå¹¶è¡Œ
4. æ¨¡ä»¿RIPT-VLAçš„æˆåŠŸæ¨¡å¼


python 11_train_ript_vla_style.py --config_path pi0/ript/config/stage11_parallel_test.yaml 
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from datetime import datetime
import argparse
from typing import List, Dict, Any, Optional
import yaml
import traceback
import time
from tqdm import tqdm
from collections import deque
import hashlib

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Šå’ŒEGLé”™è¯¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["EGL_LOG_LEVEL"] = "fatal"  # æŠ‘åˆ¶EGLé”™è¯¯è¾“å‡º

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(f"=== Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ ===")
print(f"è„šæœ¬ä½ç½®: {current_file}")
print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print()

# å¯¼å…¥é…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ OmegaConfé…ç½®ç®¡ç†å·²å¯ç”¨")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€YAMLåŠ è½½")

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    print("æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
    
    # PI0ç­–ç•¥
    from pi0.modeling_pi0 import PI0Policy
    print("âœ“ PI0ç­–ç•¥æ¨¡å—")
    
    # RIPTç»„ä»¶ - åªå¯¼å…¥å¿…éœ€çš„
    from pi0.ript.reward_function import BinarySuccessReward
    from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter
    print("âœ“ RIPTæ ¸å¿ƒç»„ä»¶")
    
    # # å¯¼å…¥ç®€åŒ–çš„ç¯å¢ƒrunner
    # try:
    #     from pi0.ript.env.pi0_libero_runner_ript_vla import PI0LiberoRunner
    #     RIPT_VLA_RUNNER_AVAILABLE = True
    #     print("âœ“ RIPT-VLA Runner")
    # except ImportError as e:
    #     print(f"âš ï¸ RIPT-VLA runnerå¯¼å…¥å¤±è´¥: {e}")
    #     RIPT_VLA_RUNNER_AVAILABLE = False
    # é»˜è®¤å…³é—­RIPT-VLA Runnerï¼ˆè‹¥ä¸Šæ–¹å¯¼å…¥è¢«æ³¨é‡Šæˆ–å¤±è´¥æ—¶ä¿æŒä¸ºFalseï¼‰
    RIPT_VLA_RUNNER_AVAILABLE = False
        
    # å¤‡ç”¨å¯¼å…¥åŸæœ‰runner
    try:
        from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
        ORIGINAL_RUNNER_AVAILABLE = True
        print("âœ“ åŸæœ‰LIBEROEnvRunner")
    except ImportError as e:
        print("âš ï¸ åŸæœ‰runnerä¹Ÿä¸å¯ç”¨")
        ORIGINAL_RUNNER_AVAILABLE = False
    
    print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥å®Œæˆ")
    
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def get_config_value(config, key, default=None, sources=['algo', 'features']):
    """
    ç»Ÿä¸€é…ç½®è¯»å–å‡½æ•°ï¼Œæ”¯æŒå¤šçº§å›é€€
    
    Args:
        config: é…ç½®å­—å…¸æˆ–å¯¹è±¡
        key: é…ç½®é”®å
        default: é»˜è®¤å€¼
        sources: æœç´¢çš„é…ç½®èŠ‚ç‚¹åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    
    Returns:
        é…ç½®å€¼
    """
    # å¤„ç†OmegaConfå’Œå­—å…¸ä¸¤ç§æƒ…å†µ
    if hasattr(config, 'get'):
        # å­—å…¸å¼è®¿é—®
        for source in sources:
            if source in config and config[source] is not None and key in config[source]:
                return config[source][key]
        # æ ¹èŠ‚ç‚¹è®¿é—®
        return config.get(key, default)
    else:
        # å±æ€§å¼è®¿é—®ï¼ˆé’ˆå¯¹ä¸€äº›æ—§çš„configå¯¹è±¡ï¼‰
        for source in sources:
            if hasattr(config, source):
                source_obj = getattr(config, source)
                if source_obj is not None and hasattr(source_obj, key):
                    return getattr(source_obj, key)
        # æ ¹èŠ‚ç‚¹è®¿é—®
        return getattr(config, key, default)

def set_config_value(config, key, value, target_source='algo'):
    """
    ç»Ÿä¸€é…ç½®å†™å›å‡½æ•°
    
    Args:
        config: é…ç½®å­—å…¸æˆ–å¯¹è±¡
        key: é…ç½®é”®å
        value: è¦è®¾ç½®çš„å€¼
        target_source: ç›®æ ‡é…ç½®èŠ‚ç‚¹
    """
    if hasattr(config, 'get'):
        # å­—å…¸å¼è®¿é—®
        if target_source not in config:
            config[target_source] = {}
        config[target_source][key] = value
    else:
        # å±æ€§å¼è®¿é—®
        if not hasattr(config, target_source):
            setattr(config, target_source, type('Config', (), {})())
        target_obj = getattr(config, target_source)
        setattr(target_obj, key, value)

def load_config(config_path: str):
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨OmegaConfï¼Œä¾¿äºå±æ€§è®¿é—®ï¼‰"""
    print(f"æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    if OMEGACONF_AVAILABLE:
        config = OmegaConf.load(config_path)
    else:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    
    # ç®€å•çš„ç±»å‹è½¬æ¢
    try:
        lr = config["algo"]["lr"]
        if isinstance(lr, str):
            if OMEGACONF_AVAILABLE:
                config.algo.lr = float(lr)
            else:
                config["algo"]["lr"] = float(lr)
    except Exception:
        pass
    
    print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    return config

def create_policy_and_optimizer(config: Dict[str, Any]):
    """åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨ï¼ˆRIPT-VLAé£æ ¼ï¼‰"""
    print("æ­£åœ¨åŠ è½½PI0ç­–ç•¥...")
    
    policy_path = config['policy_path']
    policy = PI0Policy.from_pretrained(policy_path)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶å¯ç”¨CFGï¼ˆè§£å†³åŸå§‹checkpointå…¼å®¹æ€§é—®é¢˜ï¼‰
    print("ğŸ”§ å¼ºåˆ¶å¯ç”¨CFGåŠŸèƒ½...")
    policy.model.cfg_enabled = True
    if hasattr(policy, 'config'):
        policy.config.cfg_enabled = True
    print("âœ… CFGå·²å¯ç”¨ï¼Œè®­ç»ƒå’Œæ¨ç†éƒ½å°†ä½¿ç”¨CFGåˆ†æ”¯")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    policy = policy.to(device)
    print(f"âœ“ ç­–ç•¥åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    # ğŸ”¥ ä¿®å¤ï¼šåªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå†»ç»“PaliGemmaå‰ç¼€ï¼ˆæå‡ç¨³å®šæ€§ï¼‰
    print("ğŸ”§ é…ç½®è®­ç»ƒå‚æ•°èŒƒå›´...")
    
    # 1. å†»ç»“PaliGemmaå‰ç¼€
    for p in policy.model.paligemma_with_expert.parameters():
        p.requires_grad = False
    
    # 2. åªæ”¶é›†éœ€è¦è®­ç»ƒçš„å‚æ•°
    trainable_params = []
    trainable_params += list(policy.model.action_in_proj.parameters())
    trainable_params += list(policy.model.action_time_mlp_in.parameters())
    trainable_params += list(policy.model.action_time_mlp_out.parameters())
    trainable_params += list(policy.model.action_out_proj.parameters())
    trainable_params += list(policy.model.state_proj.parameters())
    
    # 3. CFG embeddingå‚æ•°
    if hasattr(policy.model, "cfg_emb"):
        trainable_params += list(policy.model.cfg_emb.parameters())
        print("âœ… CFG embeddingå‚æ•°å·²åŠ å…¥è®­ç»ƒ")
    
    # 4. åˆ›å»ºä¼˜åŒ–å™¨
    print("æ­£åœ¨åˆ›å»ºä¼˜åŒ–å™¨...")
    lr = config['algo'].get('lr', 1e-5)
    optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)
    
    total_params = sum(p.numel() for p in trainable_params)
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼Œå­¦ä¹ ç‡: {lr}")
    print(f"ğŸ¯ åªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå‚æ•°æ•°é‡: {total_params:,}")
    
    return policy, optimizer, device

def compute_init_state_hash_from_obs(episode_data: Dict[str, Any]) -> str:
    """
    ä»åŸå§‹è§‚æµ‹è®¡ç®—åˆå§‹çŠ¶æ€å“ˆå¸Œï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
    
    Args:
        episode_data: episodeæ•°æ®ï¼ŒåŒ…å«observations
    
    Returns:
        åˆå§‹çŠ¶æ€çš„SHA256å“ˆå¸Œå­—ç¬¦ä¸²
    """
    try:
        # æå–ç¬¬ä¸€ä¸ªè§‚æµ‹ä½œä¸ºåˆå§‹çŠ¶æ€
        if 'observations' in episode_data and len(episode_data['observations']) > 0:
            first_obs = episode_data['observations'][0]
            
            # æå–å…³é”®çŠ¶æ€ä¿¡æ¯ç”¨äºå“ˆå¸Œ
            if isinstance(first_obs, dict):
                # æå– end-effector ä½ç½®å’Œå§¿æ€
                state_components = []
                if "robot0_eef_pos" in first_obs:
                    state_components.append(np.array(first_obs["robot0_eef_pos"], dtype=np.float32))
                if "robot0_eef_quat" in first_obs:
                    state_components.append(np.array(first_obs["robot0_eef_quat"], dtype=np.float32))
                if "robot0_gripper_qpos" in first_obs:
                    state_components.append(np.array(first_obs["robot0_gripper_qpos"], dtype=np.float32))
                
                if state_components:
                    # åˆå¹¶æ‰€æœ‰çŠ¶æ€ç»„ä»¶
                    combined_state = np.concatenate(state_components).astype(np.float32)
                    # è®¡ç®—å“ˆå¸Œ
                    return hashlib.sha256(combined_state.tobytes()).hexdigest()
        
        # å¦‚æœæ— æ³•æå–çŠ¶æ€ï¼Œè¿”å›é»˜è®¤å“ˆå¸Œ
        return "default_hash"
        
    except Exception as e:
        print(f"âš ï¸ ä»è§‚æµ‹è®¡ç®—åˆå§‹çŠ¶æ€å“ˆå¸Œå¤±è´¥: {e}")
        return "error_hash"

def compute_init_state_hash(batch_states: torch.Tensor, batch_pad_mask: torch.Tensor, batch_idx: int) -> str:
    """
    è®¡ç®—åˆå§‹çŠ¶æ€çš„å“ˆå¸Œå€¼ï¼ˆå®Œå…¨å¯¹æ ‡ ript-vla_ori çš„ compute_hash_from_stateï¼‰
    
    Args:
        batch_states: (B, T, state_dim) æ‰¹é‡çŠ¶æ€åºåˆ—
        batch_pad_mask: (B, T) æ‰¹é‡æœ‰æ•ˆæ©ç ï¼ŒTrueè¡¨ç¤ºæœ‰æ•ˆ
        batch_idx: åœ¨æ‰¹æ¬¡ä¸­çš„ç´¢å¼•
    
    Returns:
        åˆå§‹çŠ¶æ€çš„SHA256å“ˆå¸Œå­—ç¬¦ä¸²
    """
    try:
        # ğŸ”¥ å®Œå…¨å¯¹æ ‡ RIPT: state_data = state['states'][bidx][0]
        if batch_idx < batch_states.shape[0] and batch_states.shape[1] > 0:
            # å–æŒ‡å®šbatchç´¢å¼•çš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çŠ¶æ€
            state_data = batch_states[batch_idx, 0]  # (state_dim,)
            
            # å–å¯¹åº”çš„æ©ç 
            if batch_pad_mask is not None and batch_idx < batch_pad_mask.shape[0]:
                state_mask = batch_pad_mask[batch_idx, 0]  # ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„æ©ç 
                
                # ğŸ”¥ å®Œå…¨å¯¹æ ‡ RIPT: state_data[state_mask].cpu().numpy().tobytes()
                if state_mask.item():  # å¦‚æœç¬¬ä¸€ä¸ªæ—¶é—´æ­¥æ˜¯æœ‰æ•ˆçš„
                    state_bytes = state_data.cpu().numpy().astype(np.float32).tobytes()
                    return hashlib.sha256(state_bytes).hexdigest()
            else:
                # å¦‚æœæ²¡æœ‰æ©ç ï¼Œç›´æ¥ä½¿ç”¨çŠ¶æ€æ•°æ®
                state_bytes = state_data.cpu().numpy().astype(np.float32).tobytes()
                return hashlib.sha256(state_bytes).hexdigest()
        
        return "default_hash"
        
    except Exception as e:
        print(f"âš ï¸ è®¡ç®—åˆå§‹çŠ¶æ€å“ˆå¸Œå¤±è´¥: {e}")
        return "error_hash"

def load_rollout_stats(stats_path: str) -> Dict[str, List[int]]:
    """åŠ è½½ rollout ç»Ÿè®¡ä¿¡æ¯"""
    if Path(stats_path).exists():
        try:
            with open(stats_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ rollout_stats å¤±è´¥: {e}")
    return {}

def save_rollout_stats(stats: Dict[str, List[int]], stats_path: str):
    """ä¿å­˜ rollout ç»Ÿè®¡ä¿¡æ¯"""
    try:
        Path(stats_path).parent.mkdir(parents=True, exist_ok=True)
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ rollout_stats å¤±è´¥: {e}")

def should_skip_init_state(init_hash: str, rollout_stats: Dict[str, List[int]], 
                          rloo_batch_size: int, rollout_skip_cnt: Dict[str, int],
                          rollout_skip_threshold: int = 10) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æŸä¸ªåˆå§‹çŠ¶æ€ï¼ˆå¯¹æ ‡ ript-vla_ori çš„è·³è¿‡é€»è¾‘ï¼‰
    
    Args:
        init_hash: åˆå§‹çŠ¶æ€å“ˆå¸Œ
        rollout_stats: {init_hash: [success_list]} å†å²è®°å½•
        rloo_batch_size: RLOOæ‰¹æ¬¡å¤§å°
        rollout_skip_cnt: è·³è¿‡è®¡æ•°å™¨
        rollout_skip_threshold: è·³è¿‡é˜ˆå€¼
    
    Returns:
        æ˜¯å¦åº”è¯¥è·³è¿‡
    """
    if init_hash in rollout_stats:
        recent_successes = rollout_stats[init_hash][-rloo_batch_size:]
        
        # æ£€æŸ¥æ˜¯å¦è¿ç»­å…¨æˆåŠŸæˆ–å…¨å¤±è´¥
        if len(recent_successes) >= rloo_batch_size:
            if all(s == 1 for s in recent_successes):
                print(f"ğŸ”„ è·³è¿‡æ ·æœ¬: init_hash={init_hash[:8]}... (è¿ç»­ {rloo_batch_size} æ¬¡å…¨æˆåŠŸ)")
                return True
            elif all(s == 0 for s in recent_successes):
                print(f"ğŸ”„ è·³è¿‡æ ·æœ¬: init_hash={init_hash[:8]}... (è¿ç»­ {rloo_batch_size} æ¬¡å…¨å¤±è´¥)")
                return True
    
    return False

def update_rollout_stats(init_hash: str, success: bool, rollout_stats: Dict[str, List[int]],
                        rollout_skip_cnt: Dict[str, int], max_history_length: int = 100):
    """æ›´æ–° rollout ç»Ÿè®¡ä¿¡æ¯"""
    if init_hash not in rollout_stats:
        rollout_stats[init_hash] = []
        rollout_skip_cnt[init_hash] = 0
    
    # æ·»åŠ æ–°ç»“æœ
    rollout_stats[init_hash].append(1 if success else 0)
    
    # é™åˆ¶å†å²é•¿åº¦
    if len(rollout_stats[init_hash]) > max_history_length:
        rollout_stats[init_hash] = rollout_stats[init_hash][-max_history_length:]

def update_rollout_stats_with_correct_hash(episodes: List[Dict], batch_states: torch.Tensor, 
                                         batch_pad_mask: torch.Tensor, owner_indices: List[int],
                                         rollout_stats: Dict[str, List[int]], 
                                         rollout_skip_cnt: Dict[str, int]):
    """
    ä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œæ›´æ–° rollout ç»Ÿè®¡ä¿¡æ¯ï¼ˆåœ¨ CFG adapter å¤„ç†åè°ƒç”¨ï¼‰
    
    Args:
        episodes: åŸå§‹ episodes
        batch_states: CFG adapter å¤„ç†åçš„çŠ¶æ€ (B, state_dim)ï¼Œæ³¨æ„è¿™é‡Œæ˜¯å•ä¸ªæ—¶é—´æ­¥
        batch_pad_mask: å¯¹åº”çš„æ©ç ï¼Œå¦‚æœå¯ç”¨çš„è¯
        owner_indices: çª—å£åˆ° episode çš„æ˜ å°„
        rollout_stats: rollout ç»Ÿè®¡å­—å…¸
        rollout_skip_cnt: è·³è¿‡è®¡æ•°å­—å…¸
    """
    if batch_states is None or len(episodes) == 0:
        return
    
    try:
        # ä¸ºæ¯ä¸ª episode è®¡ç®—æ­£ç¡®çš„å“ˆå¸Œ
        episode_hash_map = {}  # episode_idx -> correct_hash
        
        # æ ¹æ® owner_indices æ˜ å°„ï¼Œä¸ºæ¯ä¸ª episode æ‰¾åˆ°å¯¹åº”çš„çŠ¶æ€
        for window_idx, episode_idx in enumerate(owner_indices):
            if window_idx < batch_states.shape[0] and episode_idx < len(episodes):
                # ä½¿ç”¨çª—å£å¯¹åº”çš„çŠ¶æ€è®¡ç®—å“ˆå¸Œ
                state_data = batch_states[window_idx]  # (state_dim,)
                
                # å¯¹äºå•ä¸ªçŠ¶æ€ï¼ˆä¸æ˜¯åºåˆ—ï¼‰ï¼Œç›´æ¥è®¡ç®—å“ˆå¸Œ
                state_bytes = state_data.cpu().numpy().astype(np.float32).tobytes()
                correct_hash = hashlib.sha256(state_bytes).hexdigest()
                
                episode_hash_map[episode_idx] = correct_hash
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        for episode_idx, episode in enumerate(episodes):
            if episode_idx in episode_hash_map:
                correct_hash = episode_hash_map[episode_idx]
                success = episode.get('success', False)
                
                # å¦‚æœä¹‹å‰æœ‰ä¸´æ—¶å“ˆå¸Œï¼Œéœ€è¦è¿ç§»ç»Ÿè®¡
                temp_hash = episode.get('temp_init_hash')
                if temp_hash and temp_hash != correct_hash and temp_hash in rollout_stats:
                    # è¿ç§»ç»Ÿè®¡æ•°æ®åˆ°æ­£ç¡®çš„å“ˆå¸Œ
                    if correct_hash not in rollout_stats:
                        rollout_stats[correct_hash] = rollout_stats[temp_hash].copy()
                        rollout_skip_cnt[correct_hash] = rollout_skip_cnt.get(temp_hash, 0)
                    else:
                        # åˆå¹¶ç»Ÿè®¡æ•°æ®
                        rollout_stats[correct_hash].extend(rollout_stats[temp_hash])
                    
                    # æ¸…ç†ä¸´æ—¶å“ˆå¸Œ
                    del rollout_stats[temp_hash]
                    if temp_hash in rollout_skip_cnt:
                        del rollout_skip_cnt[temp_hash]
                
                # æ›´æ–°æ­£ç¡®å“ˆå¸Œçš„ç»Ÿè®¡
                update_rollout_stats(correct_hash, success, rollout_stats, rollout_skip_cnt)
                
                # è®°å½•æ­£ç¡®çš„å“ˆå¸Œåˆ° episodeï¼ˆç”¨äºè°ƒè¯•ï¼‰
                episode['correct_init_hash'] = correct_hash
                
    except Exception as e:
        print(f"âš ï¸ æ›´æ–°æ­£ç¡®å“ˆå¸Œç»Ÿè®¡æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def create_environment_runner(config: Dict[str, Any], policy):
    """åˆ›å»ºç¯å¢ƒrunnerï¼ˆRIPT-VLAé£æ ¼é€‰æ‹©ï¼‰"""
    use_ript_vla = config.get('features', {}).get('use_ript_vla_runner', False)
    
    print(f"ğŸ” Runneré€‰æ‹©: use_ript_vla_runner = {use_ript_vla}")
    
    if use_ript_vla and RIPT_VLA_RUNNER_AVAILABLE:
        print("ğŸš€ ä½¿ç”¨RIPT-VLAé£æ ¼ç¯å¢ƒrunner")
        
        runner = PI0LiberoRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            rank=0
        )
        
    elif ORIGINAL_RUNNER_AVAILABLE:
        print("ğŸ”„ ä½¿ç”¨åŸæœ‰ç¯å¢ƒrunner")
        
        # ç¡®ä¿norm_stats_pathå­˜åœ¨
        norm_stats_path = config.get('norm_stats_path')
        if not norm_stats_path:
            norm_stats_path = f"{config['policy_path']}/norm_stats.json"
        
        runner = LIBEROEnvRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            norm_stats_path=norm_stats_path,
            config=config,
            rank=0,
            world_size=1
        )
    else:
        raise RuntimeError("âŒ æ²¡æœ‰å¯ç”¨çš„ç¯å¢ƒrunnerï¼")
    
    print("âœ“ ç¯å¢ƒrunneråˆ›å»ºæˆåŠŸ")
    return runner

def _dynamic_filter_rollouts(episodes: List[Dict], dynamic_sampling_config: Dict, 
                             recent_success_rates: deque) -> List[Dict]:
    """
    å‡çº§ç‰ˆåŠ¨æ€é‡‡æ ·ï¼šåŒºé—´ç­–ç•¥ + å¹³æ»‘çª—å£æœºåˆ¶
    
    Args:
        episodes: æ”¶é›†çš„episodes
        dynamic_sampling_config: åŒ…å« p_min, p_max, smooth_window çš„é…ç½®
        recent_success_rates: æœ€è¿‘æˆåŠŸç‡çš„æ»‘åŠ¨çª—å£
    
    Returns:
        è¿‡æ»¤åçš„episodesï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
    """
    if not dynamic_sampling_config.get('enabled', False) or not episodes:
        return episodes
    
    # è®¡ç®—å½“å‰æ‰¹æ¬¡æˆåŠŸç‡
    successes = [bool(ep.get('success', False)) for ep in episodes]
    current_success_rate = np.mean(successes) if successes else 0.0
    
    p_min = dynamic_sampling_config.get('p_min', 0.1)
    p_max = dynamic_sampling_config.get('p_max', 0.9)
    
    # ç¬¬ä¸€å±‚è¿‡æ»¤ï¼šå½“å‰æ‰¹æ¬¡åŒºé—´æ£€æŸ¥
    if current_success_rate < p_min or current_success_rate > p_max:
        print(f"âš ï¸ åŠ¨æ€é‡‡æ ·ç¬¬ä¸€å±‚æ‹’ç»: success_rate={current_success_rate:.3f} ä¸åœ¨ [{p_min}, {p_max}] åŒºé—´å†…")
        return []
    
    # ç¬¬äºŒå±‚è¿‡æ»¤ï¼šå¹³æ»‘çª—å£æ£€æŸ¥ï¼ˆé™ä½æŠ–åŠ¨ï¼‰
    recent_success_rates.append(current_success_rate)
    if len(recent_success_rates) >= 2:  # è‡³å°‘æœ‰2ä¸ªæ ·æœ¬æ‰è¿›è¡Œçª—å£æ£€æŸ¥
        window_avg = np.mean(recent_success_rates)
        if window_avg < p_min or window_avg > p_max:
            print(f"âš ï¸ åŠ¨æ€é‡‡æ ·ç¬¬äºŒå±‚æ‹’ç»: çª—å£å¹³å‡={window_avg:.3f} ä¸åœ¨åŒºé—´å†… (çª—å£å¤§å°={len(recent_success_rates)})")
            return []
    
    print(f"âœ… åŠ¨æ€é‡‡æ ·é€šè¿‡: å½“å‰={current_success_rate:.3f}, çª—å£å¹³å‡={np.mean(recent_success_rates):.3f}")
    return episodes


def collect_rollouts_ript_vla_style(env_runner, task_name, num_rollouts, 
                                     dynamic_sampling_config: Dict = None, 
                                     recent_success_rates: deque = None,
                                     rollout_goal_per_step: int = None,
                                     rollout_stats: Dict[str, List[int]] = None,
                                     rollout_skip_cnt: Dict[str, int] = None,
                                     rloo_batch_size: int = None):
    """
    RIPT-VLAé£æ ¼çš„rolloutæ”¶é›† + æ–‡ä»¶è®¡æ•°å™¨æ—©åœ + per-initå“ˆå¸Œè·³è¿‡
    ç›´æ¥è°ƒç”¨runnerï¼Œæ— ä¸­é—´å±‚
    """
    print(f"æ­£åœ¨æ”¶é›† {num_rollouts} ä¸ªrollouts...")
    
    # å¦‚æœè®¾ç½®äº†å…¨å±€æ ·æœ¬ç›®æ ‡ï¼Œæ˜¾ç¤ºå½“å‰è¿›åº¦
    if rollout_goal_per_step and hasattr(env_runner, 'file_counter') and env_runner.file_counter:
        current_count = env_runner.file_counter.get()
        print(f"ğŸ“Š å½“å‰å…¨å±€æ ·æœ¬æ•°: {current_count}/{rollout_goal_per_step}")
    
    try:
        # è·å–ä»»åŠ¡çš„åˆå§‹çŠ¶æ€
        task_id = 0  # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
        if hasattr(env_runner, 'benchmark'):
            all_init_states = env_runner.benchmark.get_task_init_states(task_id)
        else:
            all_init_states = None
        
        # ç›´æ¥è°ƒç”¨ç¯å¢ƒrunnerçš„æ–¹æ³•
        rollout_generator = env_runner.run_policy_in_env(
            env_name=task_name,
            all_init_states=all_init_states
        )
        
        # æ”¶é›†rolloutsï¼Œæ”¯æŒæ–‡ä»¶è®¡æ•°å™¨æ—©åœ
        collected_rollouts = []
        rollout_count = 0
        
        for success, total_reward, episode_data in rollout_generator:
            episode = {
                'success': success,
                'total_reward': total_reward,
                **episode_data
            }
            
            # ğŸ”¥ æ–°å¢ï¼šper-init å“ˆå¸Œè·³è¿‡æ£€æŸ¥
            enable_init_skip = (rollout_stats is not None and rollout_skip_cnt is not None and 
                               rloo_batch_size is not None and rloo_batch_size > 0)
            
            # ğŸ”¥ ä¿®æ”¹ï¼šæš‚æ—¶ä½¿ç”¨è§‚æµ‹å“ˆå¸Œï¼Œåç»­åœ¨è®­ç»ƒå¾ªç¯ä¸­ç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œ
            if enable_init_skip:
                # ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è®¡ç®—å“ˆå¸Œï¼ˆåŸºäºè§‚æµ‹ï¼‰
                init_hash = compute_init_state_hash_from_obs(episode)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡è¿™ä¸ªåˆå§‹çŠ¶æ€
                if should_skip_init_state(init_hash, rollout_stats, rloo_batch_size, 
                                        rollout_skip_cnt, rollout_skip_threshold=10):
                    rollout_skip_cnt[init_hash] = rollout_skip_cnt.get(init_hash, 0) + 1
                    
                    # å¦‚æœè·³è¿‡æ¬¡æ•°è¿‡å¤šï¼Œæ¸…ç†è¯¥å“ˆå¸Œè®°å½•
                    if rollout_skip_cnt[init_hash] > 10:
                        if init_hash in rollout_stats:
                            del rollout_stats[init_hash]
                        del rollout_skip_cnt[init_hash]
                        print(f"ğŸ§¹ æ¸…ç†è¿‡åº¦è·³è¿‡çš„ init_hash: {init_hash[:8]}...")
                    
                    continue  # è·³è¿‡è¿™ä¸ª episode
                
                # æš‚æ—¶è®°å½•å“ˆå¸Œï¼Œåç»­ä¼šç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œæ›´æ–°
                episode['temp_init_hash'] = init_hash
            
            collected_rollouts.append(episode)
            rollout_count += 1
            
            # ğŸ”¥ æ–°å¢ï¼šæ–‡ä»¶è®¡æ•°å™¨æ›´æ–°å’Œæ—©åœæ£€æŸ¥
            if hasattr(env_runner, 'file_counter') and env_runner.file_counter:
                env_runner.file_counter.update(1)
                current_global_count = env_runner.file_counter.get()
                
                # æ—©åœæ£€æŸ¥ï¼šè¾¾åˆ°å…¨å±€ç›®æ ‡æ ·æœ¬æ•°
                if rollout_goal_per_step and current_global_count >= rollout_goal_per_step:
                    print(f"ğŸ¯ è¾¾åˆ°å…¨å±€æ ·æœ¬ç›®æ ‡ ({current_global_count}/{rollout_goal_per_step})ï¼Œæå‰ç»“æŸæ”¶é›†")
                    break
            
            # åŸæœ‰çš„æ•°é‡é™åˆ¶
            if rollout_count >= num_rollouts:
                break
        
        # å‡çº§ç‰ˆåŠ¨æ€é‡‡æ ·è¿‡æ»¤
        if dynamic_sampling_config and recent_success_rates is not None:
            filtered = _dynamic_filter_rollouts(collected_rollouts, dynamic_sampling_config, recent_success_rates)
        else:
            filtered = collected_rollouts
            
        if not filtered:
            print("âš ï¸ æœ¬æ‰¹æ¬¡è¢«åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼Œè¿”å›ç©ºé›†")
            return filtered, []
        
        # ğŸ”¥ æ–°å¢ï¼špadding + valid_mask æœºåˆ¶ï¼ˆå¯¹æ ‡ ript-vla_oriï¼‰
        target_rollouts = num_rollouts  # ç›®æ ‡æ•°é‡
        valid_mask = [True] * len(filtered)
        
        if len(filtered) < target_rollouts:
            num_pad = target_rollouts - len(filtered)
            print(f"ğŸ“¦ Padding: éœ€è¦ {target_rollouts} ä¸ªrolloutsï¼Œå®é™…æ”¶é›† {len(filtered)} ä¸ªï¼Œå¡«å…… {num_pad} ä¸ª")
            
            if len(filtered) > 0:
                # ç”¨æœ€åä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¡«å……
                last_episode = filtered[-1].copy()
                for _ in range(num_pad):
                    padded_episode = last_episode.copy()
                    padded_episode['is_padded'] = True  # æ ‡è®°ä¸ºå¡«å……æ ·æœ¬
                    filtered.append(padded_episode)
                    valid_mask.append(False)  # æ ‡è®°ä¸ºæ— æ•ˆ
            else:
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ç”¨äºå¡«å……")
                return filtered, []
        
        print(f"âœ“ æˆåŠŸæ”¶é›†äº† {len(filtered)} ä¸ªrollouts (è¿‡æ»¤+å¡«å……åï¼Œ{sum(valid_mask)} ä¸ªæœ‰æ•ˆ)")
        return filtered, valid_mask
        
    except Exception as e:
        print(f"âŒ Rolloutæ”¶é›†å¤±è´¥: {e}")
        traceback.print_exc()
        return [], []

def compute_advantages_rloo(episodes: List[Dict], rloo_batch_size: int = None) -> torch.Tensor:
    """
    æ­£å®—çš„RLOO (Reward Ranked Leave-One-Out) ä¼˜åŠ¿è®¡ç®—
    
    Args:
        episodes: æ”¶é›†çš„episodesåˆ—è¡¨
        rloo_batch_size: RLOOæ‰¹æ¬¡å¤§å°ï¼Œç”¨äºLeave-One-Outè®¡ç®—
    
    Returns:
        torch.Tensor: è®¡ç®—å¾—åˆ°çš„ä¼˜åŠ¿å€¼
    """
    if not episodes:
        return torch.tensor([])
    
    # æå–å¥–åŠ±
    rewards = []
    for ep in episodes:
        reward = ep.get('total_reward', 0.0)
        rewards.append(float(reward))
    
    rlhf_reward = torch.tensor(rewards, dtype=torch.float32)
    num_rollouts = len(episodes)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨çœŸæ­£çš„RLOOæ‰¹æ¬¡å¤§å°è€Œä¸æ˜¯æ€»æ•°
    if rloo_batch_size is None or rloo_batch_size <= 1:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæˆ–batch sizeè¿‡å°ï¼Œé€€åŒ–ä¸ºç®€å•æ–¹æ³•
        print("âš ï¸ RLOO batch sizeæœªæŒ‡å®šæˆ–è¿‡å°ï¼Œä½¿ç”¨ç®€å•ä¼˜åŠ¿è®¡ç®—")
        advantage = rlhf_reward - rlhf_reward.mean()
    else:
        # ğŸš€ æ­£å®—RLOOè®¡ç®—
        try:
            # ç¡®ä¿å¯ä»¥æ•´é™¤ï¼Œå¦‚æœä¸èƒ½æ•´é™¤åˆ™è£å‰ªåˆ°æœ€å¤§å¯æ•´é™¤æ•°é‡
            effective_rollouts = (num_rollouts // rloo_batch_size) * rloo_batch_size
            if effective_rollouts != num_rollouts:
                print(f"ğŸ”§ RLOOè°ƒæ•´ï¼š{num_rollouts} â†’ {effective_rollouts} rollouts (batch_size={rloo_batch_size})")
                rlhf_reward = rlhf_reward[:effective_rollouts]
                num_rollouts = effective_rollouts
            
            num_batches = num_rollouts // rloo_batch_size
            rlhf_reward_reshaped = rlhf_reward.reshape(num_batches, rloo_batch_size)
            
            # æ ‡å‡†RLOOï¼šæ¯ä¸ªæ ·æœ¬çš„baseline = åŒæ‰¹æ¬¡å…¶ä»–æ ·æœ¬çš„å¹³å‡å€¼
            # baseline[i,j] = (sum(batch[i]) - reward[i,j]) / (batch_size - 1)
            batch_sums = rlhf_reward_reshaped.sum(dim=1, keepdim=True)  # (num_batches, 1)
            baseline = (batch_sums - rlhf_reward_reshaped) / (rloo_batch_size - 1)  # (num_batches, rloo_batch_size)
            
            # ä¼˜åŠ¿ = è‡ªå·±çš„å¥–åŠ± - å…¶ä»–äººçš„å¹³å‡å¥–åŠ±
            advantage = rlhf_reward_reshaped - baseline  # (num_batches, rloo_batch_size)
            advantage = advantage.flatten()  # å±•å¹³ä¸ºä¸€ç»´
            
            # NaNå’ŒInfæ£€æŸ¥
            if torch.isnan(advantage).any() or torch.isinf(advantage).any():
                print("âš ï¸ RLOOè®¡ç®—äº§ç”ŸNaN/Infï¼Œä½¿ç”¨å®‰å…¨æ›¿æ¢")
                advantage = torch.nan_to_num(advantage, nan=0.0, posinf=1.0, neginf=-1.0)
            
            print(f"ğŸ¯ æ­£å®—RLOOä¼˜åŠ¿è®¡ç®—å®Œæˆ:")
            print(f"   æ‰¹æ¬¡é…ç½®: {num_rollouts} rollouts â†’ {num_batches} batches Ã— {rloo_batch_size}")
            print(f"   ä¼˜åŠ¿ç»Ÿè®¡: mean={advantage.mean():.4f}, std={advantage.std():.4f}")
            print(f"   æ­£ä¼˜åŠ¿æ¯”ä¾‹: {(advantage > 0).float().mean():.2%}")
            
        except Exception as e:
            print(f"âŒ RLOOè®¡ç®—å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•")
            advantage = rlhf_reward - rlhf_reward.mean()
    
    return advantage

def update_policy_ript_vla_style(policy, optimizer, cfg_adapter, episodes, advantages, device):
    """
    RIPT-VLAé£æ ¼çš„ç­–ç•¥æ›´æ–°
    ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†ï¼Œæ— å¤æ‚ç»„ä»¶
    """
    if not episodes or len(advantages) == 0:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œç­–ç•¥æ›´æ–°")
        return 0.0
    
    print(f"æ­£åœ¨æ›´æ–°ç­–ç•¥ï¼ˆ{len(episodes)} ä¸ªepisodesï¼‰...")
    
    try:
        # è®¡ç®—åŠ æƒæŸå¤±
        advantages = advantages.to(device)
        loss = cfg_adapter.compute_weighted_loss(episodes, advantages, device)
        
        # æ¢¯åº¦æ›´æ–°
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
        
        optimizer.step()
        
        loss_value = loss.item()
        print(f"âœ“ ç­–ç•¥æ›´æ–°å®Œæˆï¼ŒæŸå¤±: {loss_value:.6f}")
        
        return loss_value
        
    except Exception as e:
        print(f"âŒ ç­–ç•¥æ›´æ–°å¤±è´¥: {e}")
        traceback.print_exc()
        return 0.0

def evaluate_with_cfg_sweep(policy, env_runner, task_name, config, eval_episodes=3):
    """ğŸ”¥ æ–°å¢ï¼šè¯„ä¼°ä¸åŒCFGå¼ºåº¦çš„æ•ˆæœ"""
    cfg_scales = [1.0, 1.5, 3.0, 5.0]
    best_cfg = 1.0
    best_success_rate = 0.0
    
    results = {}
    print(f"\nğŸ” å¼€å§‹CFGå¼ºåº¦æ‰«æè¯„ä¼°...")
    
    for cfg_scale in cfg_scales:
        print(f"ğŸ“Š æµ‹è¯•CFG={cfg_scale}...")
        # ä¸´æ—¶è®¾ç½®CFGå¼ºåº¦
        original_cfg = get_config_value(config, 'collection_cfg_scale', 1.5)
        set_config_value(config, 'collection_cfg_scale', cfg_scale)
        env_runner.config.collection_cfg_scale = cfg_scale
        
        # è¿è¡Œè¯„ä¼°episodes
        success_count = 0
        for ep_idx in range(eval_episodes):
            try:
                # ä½¿ç”¨ç°æœ‰çš„rolloutæ”¶é›†å‡½æ•° - æ­£ç¡®è§£åŒ…å’Œä¼ å‚
                episodes, valid_mask = collect_rollouts_ript_vla_style(
                    env_runner, task_name, 1,
                    dynamic_sampling_config=None,  # è¯„ä¼°æ—¶å…³é—­åŠ¨æ€é‡‡æ ·
                    recent_success_rates=None,
                    rollout_goal_per_step=None,
                    rollout_stats=None,
                    rollout_skip_cnt=None,
                    rloo_batch_size=1
                )
                if episodes and len(episodes) > 0:
                    if episodes[0].get('success', False):
                        success_count += 1
            except Exception as e:
                print(f"   è¯„ä¼°episode {ep_idx} å¤±è´¥: {e}")
                continue
        
        success_rate = success_count / eval_episodes
        results[cfg_scale] = success_rate
        
        if success_rate > best_success_rate:
            best_success_rate = success_rate
            best_cfg = cfg_scale
        
        # æ¢å¤åŸè®¾ç½®
        set_config_value(config, 'collection_cfg_scale', original_cfg)
        env_runner.config.collection_cfg_scale = original_cfg
        
        print(f"   CFG={cfg_scale}: æˆåŠŸç‡={success_rate:.2%} ({success_count}/{eval_episodes})")
    
    print(f"ğŸ¯ æœ€ä½³CFGå¼ºåº¦: {best_cfg} (æˆåŠŸç‡: {best_success_rate:.2%})")
    return best_cfg, results

def main_training_loop_ript_vla_style(config: Dict[str, Any]):
    """
    ä¸»è®­ç»ƒå¾ªç¯ï¼ˆRIPT-VLAé£æ ¼ï¼‰+ åŠ¨æ€é‡‡æ · + æ–‡ä»¶è®¡æ•°å™¨
    ç›´æ¥åœ¨ä¸»å‡½æ•°ä¸­å¤„ç†æ‰€æœ‰é€»è¾‘ï¼Œå‡å°‘æŠ½è±¡å±‚
    """
    print("ğŸš€ å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒå¾ªç¯")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(config['output_dir'])
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_name = config.get('exp_name', 'ript_vla_style_train')
    output_dir = output_dir / f"{exp_name}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # ğŸ”¥ æ–°å¢ï¼šè¯»å–å¢å¼ºé…ç½®
    features_config = config.get('features', {})
    dynamic_sampling_config = features_config.get('dynamic_sampling', {})
    enable_file_counter = features_config.get('enable_file_counter', False)
    adaptive_cfg_enabled = features_config.get('adaptive_cfg', False)
    
    # ğŸ”¥ ä¿®æ­£ï¼šæŒ‰ ript æ ‡å‡†è®¡ç®—å…¨å±€é˜ˆå€¼
    demo_batch_size = config['algo']['rloo_batch_size']  # å¯¹åº” ript çš„ demo_batch_size
    world_size = 1  # å½“å‰å•æœºï¼Œåç»­å¯ä»åˆ†å¸ƒå¼ç¯å¢ƒè¯»å–
    early_stop_percentage = features_config.get('early_stop_percentage', 0.8)  # æ–°å¢é…ç½®é¡¹
    
    # ğŸ”¥ æ–°å¢ï¼šç»Ÿä¸€é…ç½®è¯»å– - rollout_statsç›¸å…³
    enable_rollout_stats_tracking = get_config_value(config, 'enable_rollout_stats_tracking', False, ['features', 'algo'])
    rollout_stats_path = get_config_value(config, 'rollout_stats_path', str(output_dir / "rollout_stats.json"), ['features', 'algo'])
    
    if enable_file_counter:
        total_demo_batch_size = demo_batch_size * world_size
        rollout_goal_per_step = int(total_demo_batch_size * early_stop_percentage)
        print(f"ğŸ¯ å…¨å±€é˜ˆå€¼è®¡ç®—: {demo_batch_size} Ã— {world_size} Ã— {early_stop_percentage} = {rollout_goal_per_step}")
    else:
        rollout_goal_per_step = None
    
    print(f"\nğŸ”§ å¢å¼ºåŠŸèƒ½é…ç½®:")
    print(f"  åŠ¨æ€é‡‡æ ·: {'âœ…' if dynamic_sampling_config.get('enabled', False) else 'âŒ'}")
    if dynamic_sampling_config.get('enabled', False):
        print(f"    åŒºé—´: [{dynamic_sampling_config.get('p_min', 0.1)}, {dynamic_sampling_config.get('p_max', 0.9)}]")
        print(f"    å¹³æ»‘çª—å£: {dynamic_sampling_config.get('smooth_window', 3)}")
    print(f"  æ–‡ä»¶è®¡æ•°å™¨: {'âœ…' if enable_file_counter else 'âŒ'}")
    if rollout_goal_per_step:
        print(f"    æ¯æ­¥å…¨å±€ç›®æ ‡: {rollout_goal_per_step}")
    print(f"  è‡ªé€‚åº”CFG: {'âœ…' if adaptive_cfg_enabled else 'âŒ'}")
    
    # ğŸ”¥ æ–°å¢ï¼šåˆå§‹åŒ–å¹³æ»‘çª—å£
    smooth_window_size = dynamic_sampling_config.get('smooth_window', 3)
    recent_success_rates = deque(maxlen=smooth_window_size)
    
    # ğŸ”¥ æ–°å¢ï¼šper-init å“ˆå¸Œè·³è¿‡æœºåˆ¶åˆå§‹åŒ–
    
    rollout_stats = {}
    rollout_skip_cnt = {}
    
    if enable_rollout_stats_tracking:
        rollout_stats = load_rollout_stats(rollout_stats_path)
        rollout_skip_cnt = {k: 0 for k in rollout_stats.keys()}
        print(f"ğŸ“Š å·²åŠ è½½ {len(rollout_stats)} ä¸ªåˆå§‹çŠ¶æ€çš„å†å²è®°å½•")
        print(f"ğŸ’¾ rollout_stats è·¯å¾„: {rollout_stats_path}")
    
    # åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨
    policy, optimizer, device = create_policy_and_optimizer(config)
    
    # åˆ›å»ºCFGé€‚é…å™¨ï¼ˆå¿…éœ€ï¼Œç”¨äºæŸå¤±è®¡ç®—ï¼‰
    # ğŸ”¥ æ–°å¢ï¼šçª—å£åŒ–é…ç½®æ”¯æŒ
    dataset_config = config.get('dataset', {})
    windowing_mode = dataset_config.get('windowing_mode', 'last')
    window_stride = dataset_config.get('window_stride', 10)
    max_windows_per_episode = dataset_config.get('max_windows_per_episode', 1)
    
    print(f"\nğŸ”§ CFGçª—å£åŒ–é…ç½®:")
    print(f"  æ¨¡å¼: {windowing_mode}")
    print(f"  æ­¥é•¿: {window_stride}")
    print(f"  æ¯episodeæœ€å¤§çª—å£æ•°: {max_windows_per_episode}")
    
    cfg_adapter = PI0_CFG_Adapter(
        policy=policy,
        norm_stats_path=f"{config['policy_path']}/norm_stats.json",
        windowing_mode=windowing_mode,
        window_stride=window_stride,
        max_windows_per_episode=max_windows_per_episode
    )
    
    # åˆ›å»ºç¯å¢ƒrunner
    env_runner = create_environment_runner(config, policy)
    
    # ğŸ”¥ æ–°å¢ï¼šæ–‡ä»¶è®¡æ•°å™¨åˆå§‹åŒ–
    if enable_file_counter:
        file_counter = env_runner.setup_file_counter(counter_name="rollout", work_dir=str(output_dir))
        if file_counter:
            print(f"âœ… æ–‡ä»¶è®¡æ•°å™¨å·²å¯ç”¨: {output_dir}/rollout_counter")
        else:
            print(f"âš ï¸ æ–‡ä»¶è®¡æ•°å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ™®é€šæ¨¡å¼")
            enable_file_counter = False
    
    # è®­ç»ƒé…ç½®
    num_train_steps = config['training']['num_train_steps']
    # ä¸2_test_pi0_on_libero.pyå¯¹é½ï¼šä½¿ç”¨libero_goalåŸºå‡†é»˜è®¤task_id=1
    # è‹¥YAMLä¸­æ˜ç¡®ç»™äº†task_names_to_useï¼Œåˆ™ä»ç„¶ä½¿ç”¨ç¬¬ä¸€ä¸ªåç§°åšæ˜¾ç¤ºï¼Œä¸å½±å“ç¯å¢ƒå†…éƒ¨task_idé€‰æ‹©
    task_name = config['task'].get('task_names_to_use', ['libero_goal_default'])[0]
    rloo_batch_size = config['algo']['rloo_batch_size']
    
    print(f"\nå¼€å§‹è®­ç»ƒå¾ªç¯:")
    print(f"  è®­ç»ƒæ­¥æ•°: {num_train_steps}")
    print(f"  ä»»åŠ¡: {task_name}")
    print(f"  æ‰¹æ¬¡å¤§å°: {rloo_batch_size}")
    print()
    
    all_training_metrics = []
    
    # ğŸ”¥ ä¸»è®­ç»ƒå¾ªç¯ - RIPT-VLAé£æ ¼
    for step in range(num_train_steps):
        step_start_time = time.time()
        
        print(f"=== è®­ç»ƒæ­¥éª¤ {step + 1}/{num_train_steps} ===")
        
        # 1. æ”¶é›†rolloutsï¼ˆç›´æ¥è°ƒç”¨ï¼Œæ— ä¸­é—´å±‚ï¼‰+ æ–°åŠŸèƒ½é›†æˆ
        episodes, valid_mask = collect_rollouts_ript_vla_style(
            env_runner, task_name, rloo_batch_size,
            dynamic_sampling_config=dynamic_sampling_config,
            recent_success_rates=recent_success_rates,
            rollout_goal_per_step=rollout_goal_per_step,
            rollout_stats=rollout_stats if enable_rollout_stats_tracking else None,
            rollout_skip_cnt=rollout_skip_cnt if enable_rollout_stats_tracking else None,
            rloo_batch_size=rloo_batch_size
        )
        
        if not episodes:
            print("âš ï¸ æœªæ”¶é›†åˆ°æœ‰æ•ˆepisodesï¼Œè·³è¿‡æ­¤æ­¥")
            continue
        
        # è®°å½•æœ‰æ•ˆæ ·æœ¬æ•°é‡
        valid_count = sum(valid_mask) if valid_mask else len(episodes)
        step_metrics = {
            'step': step + 1,
            'num_episodes': len(episodes),
            'valid_episodes': valid_count,
            'padding_episodes': len(episodes) - valid_count
        }
        
        # 2. ğŸ”¥ æ–°å¢ï¼šä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œæ›´æ–°ç»Ÿè®¡ï¼ˆåœ¨ CFG adapter å¤„ç†åï¼‰
        if enable_rollout_stats_tracking:
            try:
                # é€šè¿‡ CFG adapter å¤„ç† episodes ä»¥è·å¾—æ­£ç¡®çš„çŠ¶æ€è¡¨ç¤º
                batch, owner_indices = cfg_adapter.process_episodes(episodes, device)
                batch_states = batch.get('state')  # (B, state_dim)
                
                # ä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€å“ˆå¸Œæ›´æ–°ç»Ÿè®¡
                update_rollout_stats_with_correct_hash(
                    episodes, batch_states, None, owner_indices, 
                    rollout_stats, rollout_skip_cnt
                )
                
                print(f"ğŸ”„ å·²ç”¨æ­£ç¡®çŠ¶æ€å“ˆå¸Œæ›´æ–° {len(episodes)} ä¸ªepisodesçš„ç»Ÿè®¡")
                
            except Exception as e:
                print(f"âš ï¸ æ­£ç¡®å“ˆå¸Œæ›´æ–°å¤±è´¥: {e}")
        
        # 3. è®¡ç®—ä¼˜åŠ¿ï¼ˆæ­£å®—RLOOæ–¹æ³•ï¼‰
        advantages = compute_advantages_rloo(episodes, rloo_batch_size=rloo_batch_size)
        
        # 4. æ›´æ–°ç­–ç•¥ï¼ˆç›´æ¥æ›´æ–°ï¼Œæ— å¤æ‚ç»„ä»¶ï¼‰
        loss = update_policy_ript_vla_style(
            policy, optimizer, cfg_adapter, episodes, advantages, device
        )
        
        # 5. è®°å½•æŒ‡æ ‡
        avg_reward = np.mean([ep['total_reward'] for ep in episodes])
        success_rate = np.mean([ep['success'] for ep in episodes])
        step_time = time.time() - step_start_time
        
        # æ›´æ–°step_metrics
        step_metrics.update({
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'loss': loss,
            'step_time': step_time
        })
        all_training_metrics.append(step_metrics)
        
        # 6. è¾“å‡ºç»“æœ
        print(f"âœ“ æ­¥éª¤ {step + 1} å®Œæˆ:")
        print(f"  Episodes: {len(episodes)} (æœ‰æ•ˆ: {valid_count}, å¡«å……: {len(episodes) - valid_count})")
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        print(f"  æˆåŠŸç‡: {success_rate:.2%}")
        print(f"  æŸå¤±: {loss:.6f}")
        print(f"  è€—æ—¶: {step_time:.2f}ç§’")
        
        # 7. è‡ªé€‚åº”CFGè°ƒæ•´ï¼ˆåŸºäºæˆåŠŸç‡çª—å£ï¼‰
        if adaptive_cfg_enabled and len(recent_success_rates) >= 2:
            try:
                current_avg = np.mean(recent_success_rates)
                p_min = dynamic_sampling_config.get('p_min', 0.1)
                p_max = dynamic_sampling_config.get('p_max', 0.9)
                current_cfg = get_config_value(config, 'collection_cfg_scale', 1.5)
                
                # è‡ªé€‚åº”è°ƒæ•´é€»è¾‘
                if current_avg < p_min:
                    # æˆåŠŸç‡è¿‡ä½ï¼Œé™ä½CFGå¼ºåº¦ï¼ˆå‡å°‘è¿‡åº¦å¼•å¯¼ï¼‰
                    new_cfg = max(1.0, current_cfg - 0.2)
                    print(f"ğŸ”§ è‡ªé€‚åº”CFG: æˆåŠŸç‡è¿‡ä½({current_avg:.3f} < {p_min})ï¼Œé™ä½CFG {current_cfg:.1f} â†’ {new_cfg:.1f}")
                    # ç»Ÿä¸€å†™å›é…ç½®
                    set_config_value(config, 'collection_cfg_scale', new_cfg)
                    env_runner.config.collection_cfg_scale = new_cfg
                elif current_avg > p_max:
                    # æˆåŠŸç‡è¿‡é«˜ï¼Œå¢åŠ CFGå¼ºåº¦ï¼ˆå¢å¼ºå¼•å¯¼ï¼‰
                    new_cfg = min(5.0, current_cfg + 0.2)
                    print(f"ğŸ”§ è‡ªé€‚åº”CFG: æˆåŠŸç‡è¿‡é«˜({current_avg:.3f} > {p_max})ï¼Œæå‡CFG {current_cfg:.1f} â†’ {new_cfg:.1f}")
                    # ç»Ÿä¸€å†™å›é…ç½®
                    set_config_value(config, 'collection_cfg_scale', new_cfg)
                    env_runner.config.collection_cfg_scale = new_cfg
                else:
                    print(f"âœ… è‡ªé€‚åº”CFG: æˆåŠŸç‡é€‚ä¸­({current_avg:.3f})ï¼Œä¿æŒCFG={current_cfg:.1f}")
                
                step_metrics['adaptive_cfg_scale'] = get_config_value(config, 'collection_cfg_scale', 1.5)
                step_metrics['success_rate_window_avg'] = current_avg
                
            except Exception as e:
                print(f"âš ï¸ è‡ªé€‚åº”CFGè°ƒæ•´å¤±è´¥: {e}")
        
        # 8. CFGè¯„ä¼°ï¼ˆæ¯10æ­¥è¿›è¡Œä¸€æ¬¡ï¼‰
        if (step + 1) % 10 == 0:
            try:
                best_cfg, cfg_results = evaluate_with_cfg_sweep(policy, env_runner, task_name, config, eval_episodes=2)
                step_metrics['best_cfg_scale'] = best_cfg
                step_metrics['cfg_sweep_results'] = cfg_results
                print(f"ğŸ¯ æ¨èCFGå¼ºåº¦: {best_cfg}")
                # æ³¨æ„ï¼šå¦‚æœå¯ç”¨äº†è‡ªé€‚åº”CFGï¼Œè¿™é‡Œä¸å¼ºåˆ¶è¦†ç›–
                if not adaptive_cfg_enabled:
                    env_runner.config.collection_cfg_scale = best_cfg
            except Exception as e:
                print(f"âš ï¸ CFGè¯„ä¼°å¤±è´¥: {e}")
        
        # 9. å®šæœŸä¿å­˜ rollout_stats
        if enable_rollout_stats_tracking and (step + 1) % 5 == 0:  # æ¯5æ­¥ä¿å­˜ä¸€æ¬¡
            save_rollout_stats(rollout_stats, rollout_stats_path)
            print(f"ğŸ’¾ å·²ä¿å­˜ rollout_stats ({len(rollout_stats)} ä¸ªåˆå§‹çŠ¶æ€)")
        
        # 10. ä¿å­˜æ£€æŸ¥ç‚¹
        if (step + 1) % config['training'].get('save_freq', 10) == 0:
            # è½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼Œä¾¿äºéƒ¨ç½²ä¸å ç”¨å°ï¼‰
            weights_path = output_dir / f"weights_step_{step + 1}.pt"
            torch.save({
                'step': step + 1,
                'policy_state_dict': policy.state_dict(),
                'config': config,
                'training_metrics': all_training_metrics,
            }, weights_path)
            print(f"âœ“ è½»é‡æƒé‡å·²ä¿å­˜: {weights_path}")

            # å¯é€‰ï¼šæŒ‰è¾ƒä½é¢‘ç‡ä¿å­˜å«ä¼˜åŒ–å™¨çš„å®Œæ•´æ£€æŸ¥ç‚¹ï¼Œä¾¿äºæ¢å¤è®­ç»ƒ
            save_opt_every = config.get('training', {}).get('save_optimizer_freq', None)
            if save_opt_every and ((step + 1) % int(save_opt_every) == 0):
                checkpoint_path = output_dir / f"checkpoint_step_{step + 1}.pt"
                torch.save({
                    'step': step + 1,
                    'policy_state_dict': policy.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'config': config,
                    'training_metrics': all_training_metrics,
                }, checkpoint_path)
                print(f"âœ“ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results_path = output_dir / "final_training_results.json"
    # å°† OmegaConf è½¬ä¸ºåŸç”Ÿ dict ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if OMEGACONF_AVAILABLE and isinstance(config, DictConfig):
        serializable_config = OmegaConf.to_container(config, resolve=True)
    else:
        serializable_config = config
    with open(final_results_path, 'w') as f:
        json.dump({
            'config': serializable_config,
            'training_metrics': all_training_metrics,
            'total_steps': len(all_training_metrics)
        }, f, indent=2)
    
    # æœ€ç»ˆè½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼‰
    final_weights_path = output_dir / "final_weights.pt"
    torch.save({
        'step': len(all_training_metrics),
        'policy_state_dict': policy.state_dict(),
        'config': config,
        'training_metrics': all_training_metrics,
    }, final_weights_path)
    print(f"âœ“ æœ€ç»ˆè½»é‡æƒé‡å·²ä¿å­˜: {final_weights_path}")

    # å¯é€‰ï¼šä¿å­˜æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆå«ä¼˜åŒ–å™¨ï¼‰ä¾¿äºæ¢å¤è®­ç»ƒ
    if config.get('training', {}).get('save_optimizer_final', False):
        final_checkpoint_path = output_dir / "final_checkpoint.pt"
        torch.save({
            'step': len(all_training_metrics),
            'policy_state_dict': policy.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'config': config,
            'training_metrics': all_training_metrics,
        }, final_checkpoint_path)
        print(f"âœ“ æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")

    # æœ€ç»ˆä¿å­˜ rollout_stats
    if enable_rollout_stats_tracking:
        save_rollout_stats(rollout_stats, rollout_stats_path)
        print(f"ğŸ’¾ æœ€ç»ˆä¿å­˜ rollout_stats: {rollout_stats_path}")
    
    print(f"\nğŸ‰ RIPT-VLAé£æ ¼è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœå·²ä¿å­˜: {final_results_path}")
    print(f"âœ¨ ä½¿ç”¨äº†ç®€åŒ–çš„ç›´æ¥æ¶æ„ï¼Œå‡å°‘äº†æŠ½è±¡å±‚å¤æ‚åº¦")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ")
    parser.add_argument(
        "--config_path", 
        type=str, 
        required=True,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config_path)
        
        # æ˜¾ç¤ºé…ç½®
        print("\n====== ä½¿ç”¨é…ç½® ======")
        if OMEGACONF_AVAILABLE:
            print(OmegaConf.to_yaml(config))
        else:
            print(yaml.dump(config, default_flow_style=False, allow_unicode=True))
        print("====================\n")
        
        # å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒ
        main_training_loop_ript_vla_style(config)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()