#!/usr/bin/env python3
"""
Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–ç‰ˆæœ¬
åŸºäºRIPT-VLAçš„ç›´æ¥æ¶æ„æ¨¡å¼ï¼Œå»é™¤å¤šä½™çš„æŠ½è±¡å±‚

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
1. ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†rolloutæ”¶é›†å’Œä¼˜åŒ–
2. ç®€åŒ–çš„ç»„ä»¶æ¶æ„ï¼Œå‡å°‘ä¸­é—´å±‚
3. ç›´æ¥ä½¿ç”¨SubprocVectorEnvè¿›è¡Œå¹¶è¡Œ
4. æ¨¡ä»¿RIPT-VLAçš„æˆåŠŸæ¨¡å¼


python 11_train_ript_vla_style.py --config_path pi0/ript/config/stage11_parallel_test.yaml 
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from datetime import datetime
import argparse
from typing import List, Dict, Any, Optional
import yaml
import traceback
import time
from tqdm import tqdm

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Šå’ŒEGLé”™è¯¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["EGL_LOG_LEVEL"] = "fatal"  # æŠ‘åˆ¶EGLé”™è¯¯è¾“å‡º

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(f"=== Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ ===")
print(f"è„šæœ¬ä½ç½®: {current_file}")
print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print()

# å¯¼å…¥é…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ OmegaConfé…ç½®ç®¡ç†å·²å¯ç”¨")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€YAMLåŠ è½½")

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    print("æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
    
    # PI0ç­–ç•¥
    from pi0.modeling_pi0 import PI0Policy
    print("âœ“ PI0ç­–ç•¥æ¨¡å—")
    
    # RIPTç»„ä»¶ - åªå¯¼å…¥å¿…éœ€çš„
    from pi0.ript.reward_function import BinarySuccessReward
    from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter
    print("âœ“ RIPTæ ¸å¿ƒç»„ä»¶")
    
    # # å¯¼å…¥ç®€åŒ–çš„ç¯å¢ƒrunner
    # try:
    #     from pi0.ript.env.pi0_libero_runner_ript_vla import PI0LiberoRunner
    #     RIPT_VLA_RUNNER_AVAILABLE = True
    #     print("âœ“ RIPT-VLA Runner")
    # except ImportError as e:
    #     print(f"âš ï¸ RIPT-VLA runnerå¯¼å…¥å¤±è´¥: {e}")
    #     RIPT_VLA_RUNNER_AVAILABLE = False
    # é»˜è®¤å…³é—­RIPT-VLA Runnerï¼ˆè‹¥ä¸Šæ–¹å¯¼å…¥è¢«æ³¨é‡Šæˆ–å¤±è´¥æ—¶ä¿æŒä¸ºFalseï¼‰
    RIPT_VLA_RUNNER_AVAILABLE = False
        
    # å¤‡ç”¨å¯¼å…¥åŸæœ‰runner
    try:
        from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
        ORIGINAL_RUNNER_AVAILABLE = True
        print("âœ“ åŸæœ‰LIBEROEnvRunner")
    except ImportError as e:
        print("âš ï¸ åŸæœ‰runnerä¹Ÿä¸å¯ç”¨")
        ORIGINAL_RUNNER_AVAILABLE = False
    
    print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥å®Œæˆ")
    
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def load_config(config_path: str):
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨OmegaConfï¼Œä¾¿äºå±æ€§è®¿é—®ï¼‰"""
    print(f"æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    if OMEGACONF_AVAILABLE:
        config = OmegaConf.load(config_path)
    else:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    
    # ç®€å•çš„ç±»å‹è½¬æ¢
    try:
        lr = config["algo"]["lr"]
        if isinstance(lr, str):
            if OMEGACONF_AVAILABLE:
                config.algo.lr = float(lr)
            else:
                config["algo"]["lr"] = float(lr)
    except Exception:
        pass
    
    print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    return config

def create_policy_and_optimizer(config: Dict[str, Any]):
    """åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨ï¼ˆRIPT-VLAé£æ ¼ï¼‰"""
    print("æ­£åœ¨åŠ è½½PI0ç­–ç•¥...")
    
    policy_path = config['policy_path']
    policy = PI0Policy.from_pretrained(policy_path)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    policy = policy.to(device)
    print(f"âœ“ ç­–ç•¥åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    # ç›´æ¥åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåƒRIPT-VLAä¸€æ ·ï¼‰
    print("æ­£åœ¨åˆ›å»ºä¼˜åŒ–å™¨...")
    lr = config['algo'].get('lr', 1e-5)
    optimizer = torch.optim.AdamW(
        policy.parameters(),
        lr=lr,
        weight_decay=0.01
    )
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼Œå­¦ä¹ ç‡: {lr}")
    
    return policy, optimizer, device

def create_environment_runner(config: Dict[str, Any], policy):
    """åˆ›å»ºç¯å¢ƒrunnerï¼ˆRIPT-VLAé£æ ¼é€‰æ‹©ï¼‰"""
    use_ript_vla = config.get('features', {}).get('use_ript_vla_runner', False)
    
    print(f"ğŸ” Runneré€‰æ‹©: use_ript_vla_runner = {use_ript_vla}")
    
    if use_ript_vla and RIPT_VLA_RUNNER_AVAILABLE:
        print("ğŸš€ ä½¿ç”¨RIPT-VLAé£æ ¼ç¯å¢ƒrunner")
        
        runner = PI0LiberoRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            rank=0
        )
        
    elif ORIGINAL_RUNNER_AVAILABLE:
        print("ğŸ”„ ä½¿ç”¨åŸæœ‰ç¯å¢ƒrunner")
        
        # ç¡®ä¿norm_stats_pathå­˜åœ¨
        norm_stats_path = config.get('norm_stats_path')
        if not norm_stats_path:
            norm_stats_path = f"{config['policy_path']}/norm_stats.json"
        
        runner = LIBEROEnvRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            norm_stats_path=norm_stats_path,
            config=config,
            rank=0,
            world_size=1
        )
    else:
        raise RuntimeError("âŒ æ²¡æœ‰å¯ç”¨çš„ç¯å¢ƒrunnerï¼")
    
    print("âœ“ ç¯å¢ƒrunneråˆ›å»ºæˆåŠŸ")
    return runner

def _dynamic_filter_rollouts(episodes: List[Dict], enable_dynamic_sampling: bool) -> List[Dict]:
    """æŒ‰RIPT-VLAæ€è·¯çš„æœ€å°åŠ¨æ€é‡‡æ ·ï¼šä¸¢å¼ƒå…¨0æˆ–å…¨1æˆåŠŸç‡çš„æ‰¹æ¬¡"""
    if not enable_dynamic_sampling or not episodes:
        return episodes
    successes = [bool(ep.get('success', False)) for ep in episodes]
    if len(successes) > 0 and (all(successes) or not any(successes)):
        print(f"âš ï¸ åŠ¨æ€é‡‡æ ·ä¸¢å¼ƒæœ¬æ‰¹æ¬¡ (uniform successes: {successes})")
        return []
    return episodes


def collect_rollouts_ript_vla_style(env_runner, task_name, num_rollouts, enable_dynamic_sampling: bool = False):
    """
    RIPT-VLAé£æ ¼çš„rolloutæ”¶é›†
    ç›´æ¥è°ƒç”¨runnerï¼Œæ— ä¸­é—´å±‚
    """
    print(f"æ­£åœ¨æ”¶é›† {num_rollouts} ä¸ªrollouts...")
    
    try:
        # è·å–ä»»åŠ¡çš„åˆå§‹çŠ¶æ€
        task_id = 0  # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
        if hasattr(env_runner, 'benchmark'):
            all_init_states = env_runner.benchmark.get_task_init_states(task_id)
        else:
            all_init_states = None
        
        # ç›´æ¥è°ƒç”¨ç¯å¢ƒrunnerçš„æ–¹æ³•
        rollout_generator = env_runner.run_policy_in_env(
            env_name=task_name,
            all_init_states=all_init_states
        )
        
        # æ”¶é›†æ‰€æœ‰rollouts
        collected_rollouts = []
        rollout_count = 0
        
        for success, total_reward, episode_data in rollout_generator:
            episode = {
                'success': success,
                'total_reward': total_reward,
                **episode_data
            }
            collected_rollouts.append(episode)
            rollout_count += 1
            
            if rollout_count >= num_rollouts:
                break
        
        # æœ€å°åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼šä¸¢å¼ƒå…¨0æˆ–å…¨1æ‰¹æ¬¡
        filtered = _dynamic_filter_rollouts(collected_rollouts, enable_dynamic_sampling)
        if not filtered:
            print("âš ï¸ æœ¬æ‰¹æ¬¡è¢«åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼Œè¿”å›ç©ºé›†")
        else:
            print(f"âœ“ æˆåŠŸæ”¶é›†äº† {len(filtered)} ä¸ªrollouts (è¿‡æ»¤å)")
        return filtered
        
    except Exception as e:
        print(f"âŒ Rolloutæ”¶é›†å¤±è´¥: {e}")
        traceback.print_exc()
        return []

def compute_advantages_simple(episodes: List[Dict]) -> torch.Tensor:
    """
    CFGåŸç‰ˆä¼˜åŠ¿è®¡ç®—é€»è¾‘ - ä½¿ç”¨total_rewardå’ŒLeave-One-Out baseline
    å®Œå…¨æŒ‰ç…§åŸç‰ˆrl_optimizer_pi0_cfg.pyçš„é€»è¾‘å®ç°
    """
    if not episodes:
        return torch.tensor([])
    
    # ä½¿ç”¨åŸç‰ˆçš„total_rewardï¼Œä¸åšä»»ä½•ä¿®æ”¹
    rewards = []
    for ep in episodes:
        reward = ep.get('total_reward', 0.0)
        rewards.append(float(reward))
    
    # åˆ›å»ºå¥–åŠ±å¼ é‡
    rlhf_reward = torch.tensor(rewards, dtype=torch.float32)
    
    # åŸç‰ˆLeave-One-Outä¼˜åŠ¿è®¡ç®—é€»è¾‘
    num_rollouts = len(episodes)
    rollouts_per_batch = len(episodes)  # ç®€åŒ–ï¼šæ‰¹æ¬¡å¤§å°ç­‰äºæ€»æ•°
    
    if num_rollouts % rollouts_per_batch != 0 or rollouts_per_batch <= 1:
        # ç®€åŒ–çš„ä¼˜åŠ¿: reward - mean(reward)
        advantage = rlhf_reward - rlhf_reward.mean()
    else:
        num_batches = num_rollouts // rollouts_per_batch
        rlhf_reward_reshaped = rlhf_reward.reshape(num_batches, rollouts_per_batch)
        
        if rollouts_per_batch <= 1:
            advantage = rlhf_reward - rlhf_reward.mean()
        else:
            baseline = (rlhf_reward_reshaped.sum(1, keepdim=True) - rlhf_reward_reshaped) / (rollouts_per_batch - 1)
            advantage = rlhf_reward_reshaped - baseline
            advantage = advantage.flatten()
    
    print(f"ğŸ”§ CFGåŸç‰ˆä¼˜åŠ¿è®¡ç®—ç»“æœ: {advantage}")
    return advantage

def update_policy_ript_vla_style(policy, optimizer, cfg_adapter, episodes, advantages, device):
    """
    RIPT-VLAé£æ ¼çš„ç­–ç•¥æ›´æ–°
    ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†ï¼Œæ— å¤æ‚ç»„ä»¶
    """
    if not episodes or len(advantages) == 0:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œç­–ç•¥æ›´æ–°")
        return 0.0
    
    print(f"æ­£åœ¨æ›´æ–°ç­–ç•¥ï¼ˆ{len(episodes)} ä¸ªepisodesï¼‰...")
    
    try:
        # è®¡ç®—åŠ æƒæŸå¤±
        advantages = advantages.to(device)
        loss = cfg_adapter.compute_weighted_loss(episodes, advantages, device)
        
        # æ¢¯åº¦æ›´æ–°
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
        
        optimizer.step()
        
        loss_value = loss.item()
        print(f"âœ“ ç­–ç•¥æ›´æ–°å®Œæˆï¼ŒæŸå¤±: {loss_value:.6f}")
        
        return loss_value
        
    except Exception as e:
        print(f"âŒ ç­–ç•¥æ›´æ–°å¤±è´¥: {e}")
        traceback.print_exc()
        return 0.0

def main_training_loop_ript_vla_style(config: Dict[str, Any]):
    """
    ä¸»è®­ç»ƒå¾ªç¯ï¼ˆRIPT-VLAé£æ ¼ï¼‰
    ç›´æ¥åœ¨ä¸»å‡½æ•°ä¸­å¤„ç†æ‰€æœ‰é€»è¾‘ï¼Œå‡å°‘æŠ½è±¡å±‚
    """
    print("ğŸš€ å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒå¾ªç¯")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(config['output_dir'])
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_name = config.get('exp_name', 'ript_vla_style_train')
    output_dir = output_dir / f"{exp_name}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨
    policy, optimizer, device = create_policy_and_optimizer(config)
    
    # åˆ›å»ºCFGé€‚é…å™¨ï¼ˆå¿…éœ€ï¼Œç”¨äºæŸå¤±è®¡ç®—ï¼‰
    cfg_adapter = PI0_CFG_Adapter(
        policy=policy,
        norm_stats_path=f"{config['policy_path']}/norm_stats.json"
    )
    
    # åˆ›å»ºç¯å¢ƒrunner
    env_runner = create_environment_runner(config, policy)
    
    # è®­ç»ƒé…ç½®
    num_train_steps = config['training']['num_train_steps']
    # ä¸2_test_pi0_on_libero.pyå¯¹é½ï¼šä½¿ç”¨libero_goalåŸºå‡†é»˜è®¤task_id=1
    # è‹¥YAMLä¸­æ˜ç¡®ç»™äº†task_names_to_useï¼Œåˆ™ä»ç„¶ä½¿ç”¨ç¬¬ä¸€ä¸ªåç§°åšæ˜¾ç¤ºï¼Œä¸å½±å“ç¯å¢ƒå†…éƒ¨task_idé€‰æ‹©
    task_name = config['task'].get('task_names_to_use', ['libero_goal_default'])[0]
    rloo_batch_size = config['algo']['rloo_batch_size']
    
    print(f"\nå¼€å§‹è®­ç»ƒå¾ªç¯:")
    print(f"  è®­ç»ƒæ­¥æ•°: {num_train_steps}")
    print(f"  ä»»åŠ¡: {task_name}")
    print(f"  æ‰¹æ¬¡å¤§å°: {rloo_batch_size}")
    print()
    
    all_training_metrics = []
    
    # ğŸ”¥ ä¸»è®­ç»ƒå¾ªç¯ - RIPT-VLAé£æ ¼
    for step in range(num_train_steps):
        step_start_time = time.time()
        
        print(f"=== è®­ç»ƒæ­¥éª¤ {step + 1}/{num_train_steps} ===")
        
        # 1. æ”¶é›†rolloutsï¼ˆç›´æ¥è°ƒç”¨ï¼Œæ— ä¸­é—´å±‚ï¼‰
        episodes = collect_rollouts_ript_vla_style(
            env_runner, task_name, rloo_batch_size,
            enable_dynamic_sampling=config['algo'].get('enable_dynamic_sampling', False)
        )
        
        if not episodes:
            print("âš ï¸ æœªæ”¶é›†åˆ°æœ‰æ•ˆepisodesï¼Œè·³è¿‡æ­¤æ­¥")
            continue
        
        # 2. è®¡ç®—ä¼˜åŠ¿ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        advantages = compute_advantages_simple(episodes)
        
        # 3. æ›´æ–°ç­–ç•¥ï¼ˆç›´æ¥æ›´æ–°ï¼Œæ— å¤æ‚ç»„ä»¶ï¼‰
        loss = update_policy_ript_vla_style(
            policy, optimizer, cfg_adapter, episodes, advantages, device
        )
        
        # 4. è®°å½•æŒ‡æ ‡
        avg_reward = np.mean([ep['total_reward'] for ep in episodes])
        success_rate = np.mean([ep['success'] for ep in episodes])
        step_time = time.time() - step_start_time
        
        step_metrics = {
            'step': step + 1,
            'num_episodes': len(episodes),
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'loss': loss,
            'step_time': step_time
        }
        all_training_metrics.append(step_metrics)
        
        # 5. è¾“å‡ºç»“æœ
        print(f"âœ“ æ­¥éª¤ {step + 1} å®Œæˆ:")
        print(f"  Episodes: {len(episodes)}")
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        print(f"  æˆåŠŸç‡: {success_rate:.2%}")
        print(f"  æŸå¤±: {loss:.6f}")
        print(f"  è€—æ—¶: {step_time:.2f}ç§’")
        
        # 6. ä¿å­˜æ£€æŸ¥ç‚¹
        if (step + 1) % config['training'].get('save_freq', 10) == 0:
            # è½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼Œä¾¿äºéƒ¨ç½²ä¸å ç”¨å°ï¼‰
            weights_path = output_dir / f"weights_step_{step + 1}.pt"
            torch.save({
                'step': step + 1,
                'policy_state_dict': policy.state_dict(),
                'config': config,
                'training_metrics': all_training_metrics,
            }, weights_path)
            print(f"âœ“ è½»é‡æƒé‡å·²ä¿å­˜: {weights_path}")

            # å¯é€‰ï¼šæŒ‰è¾ƒä½é¢‘ç‡ä¿å­˜å«ä¼˜åŒ–å™¨çš„å®Œæ•´æ£€æŸ¥ç‚¹ï¼Œä¾¿äºæ¢å¤è®­ç»ƒ
            save_opt_every = config.get('training', {}).get('save_optimizer_freq', None)
            if save_opt_every and ((step + 1) % int(save_opt_every) == 0):
                checkpoint_path = output_dir / f"checkpoint_step_{step + 1}.pt"
                torch.save({
                    'step': step + 1,
                    'policy_state_dict': policy.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'config': config,
                    'training_metrics': all_training_metrics,
                }, checkpoint_path)
                print(f"âœ“ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results_path = output_dir / "final_training_results.json"
    # å°† OmegaConf è½¬ä¸ºåŸç”Ÿ dict ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if OMEGACONF_AVAILABLE and isinstance(config, DictConfig):
        serializable_config = OmegaConf.to_container(config, resolve=True)
    else:
        serializable_config = config
    with open(final_results_path, 'w') as f:
        json.dump({
            'config': serializable_config,
            'training_metrics': all_training_metrics,
            'total_steps': len(all_training_metrics)
        }, f, indent=2)
    
    # æœ€ç»ˆè½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼‰
    final_weights_path = output_dir / "final_weights.pt"
    torch.save({
        'step': len(all_training_metrics),
        'policy_state_dict': policy.state_dict(),
        'config': config,
        'training_metrics': all_training_metrics,
    }, final_weights_path)
    print(f"âœ“ æœ€ç»ˆè½»é‡æƒé‡å·²ä¿å­˜: {final_weights_path}")

    # å¯é€‰ï¼šä¿å­˜æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆå«ä¼˜åŒ–å™¨ï¼‰ä¾¿äºæ¢å¤è®­ç»ƒ
    if config.get('training', {}).get('save_optimizer_final', False):
        final_checkpoint_path = output_dir / "final_checkpoint.pt"
        torch.save({
            'step': len(all_training_metrics),
            'policy_state_dict': policy.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'config': config,
            'training_metrics': all_training_metrics,
        }, final_checkpoint_path)
        print(f"âœ“ æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")

    print(f"\nğŸ‰ RIPT-VLAé£æ ¼è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœå·²ä¿å­˜: {final_results_path}")
    print(f"âœ¨ ä½¿ç”¨äº†ç®€åŒ–çš„ç›´æ¥æ¶æ„ï¼Œå‡å°‘äº†æŠ½è±¡å±‚å¤æ‚åº¦")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ")
    parser.add_argument(
        "--config_path", 
        type=str, 
        required=True,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config_path)
        
        # æ˜¾ç¤ºé…ç½®
        print("\n====== ä½¿ç”¨é…ç½® ======")
        if OMEGACONF_AVAILABLE:
            print(OmegaConf.to_yaml(config))
        else:
            print(yaml.dump(config, default_flow_style=False, allow_unicode=True))
        print("====================\n")
        
        # å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒ
        main_training_loop_ript_vla_style(config)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()