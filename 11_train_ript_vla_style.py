#!/usr/bin/env python3
"""
Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–ç‰ˆæœ¬
åŸºäºRIPT-VLAçš„ç›´æ¥æ¶æ„æ¨¡å¼ï¼Œå»é™¤å¤šä½™çš„æŠ½è±¡å±‚

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
1. ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†rolloutæ”¶é›†å’Œä¼˜åŒ–
2. ç®€åŒ–çš„ç»„ä»¶æ¶æ„ï¼Œå‡å°‘ä¸­é—´å±‚
3. ç›´æ¥ä½¿ç”¨SubprocVectorEnvè¿›è¡Œå¹¶è¡Œ
4. æ¨¡ä»¿RIPT-VLAçš„æˆåŠŸæ¨¡å¼


python 11_train_ript_vla_style.py --config_path pi0/ript/config/stage11_parallel_test.yaml 
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from datetime import datetime
import argparse
from typing import List, Dict, Any, Optional
import yaml
import traceback
import time
from tqdm import tqdm

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Šå’ŒEGLé”™è¯¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["EGL_LOG_LEVEL"] = "fatal"  # æŠ‘åˆ¶EGLé”™è¯¯è¾“å‡º

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(f"=== Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ ===")
print(f"è„šæœ¬ä½ç½®: {current_file}")
print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print()

# å¯¼å…¥é…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ OmegaConfé…ç½®ç®¡ç†å·²å¯ç”¨")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€YAMLåŠ è½½")

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    print("æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
    
    # PI0ç­–ç•¥
    from pi0.modeling_pi0 import PI0Policy
    print("âœ“ PI0ç­–ç•¥æ¨¡å—")
    
    # RIPTç»„ä»¶ - åªå¯¼å…¥å¿…éœ€çš„
    from pi0.ript.reward_function import BinarySuccessReward
    from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter
    print("âœ“ RIPTæ ¸å¿ƒç»„ä»¶")
    
    # # å¯¼å…¥ç®€åŒ–çš„ç¯å¢ƒrunner
    # try:
    #     from pi0.ript.env.pi0_libero_runner_ript_vla import PI0LiberoRunner
    #     RIPT_VLA_RUNNER_AVAILABLE = True
    #     print("âœ“ RIPT-VLA Runner")
    # except ImportError as e:
    #     print(f"âš ï¸ RIPT-VLA runnerå¯¼å…¥å¤±è´¥: {e}")
    #     RIPT_VLA_RUNNER_AVAILABLE = False
    # é»˜è®¤å…³é—­RIPT-VLA Runnerï¼ˆè‹¥ä¸Šæ–¹å¯¼å…¥è¢«æ³¨é‡Šæˆ–å¤±è´¥æ—¶ä¿æŒä¸ºFalseï¼‰
    RIPT_VLA_RUNNER_AVAILABLE = False
        
    # å¤‡ç”¨å¯¼å…¥åŸæœ‰runner
    try:
        from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
        ORIGINAL_RUNNER_AVAILABLE = True
        print("âœ“ åŸæœ‰LIBEROEnvRunner")
    except ImportError as e:
        print("âš ï¸ åŸæœ‰runnerä¹Ÿä¸å¯ç”¨")
        ORIGINAL_RUNNER_AVAILABLE = False
    
    print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥å®Œæˆ")
    
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def load_config(config_path: str):
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨OmegaConfï¼Œä¾¿äºå±æ€§è®¿é—®ï¼‰"""
    print(f"æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    if OMEGACONF_AVAILABLE:
        config = OmegaConf.load(config_path)
    else:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    
    # ç®€å•çš„ç±»å‹è½¬æ¢
    try:
        lr = config["algo"]["lr"]
        if isinstance(lr, str):
            if OMEGACONF_AVAILABLE:
                config.algo.lr = float(lr)
            else:
                config["algo"]["lr"] = float(lr)
    except Exception:
        pass
    
    print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    return config

def create_policy_and_optimizer(config: Dict[str, Any]):
    """åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨ï¼ˆRIPT-VLAé£æ ¼ï¼‰"""
    print("æ­£åœ¨åŠ è½½PI0ç­–ç•¥...")
    
    policy_path = config['policy_path']
    policy = PI0Policy.from_pretrained(policy_path)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶å¯ç”¨CFGï¼ˆè§£å†³åŸå§‹checkpointå…¼å®¹æ€§é—®é¢˜ï¼‰
    print("ğŸ”§ å¼ºåˆ¶å¯ç”¨CFGåŠŸèƒ½...")
    policy.model.cfg_enabled = True
    if hasattr(policy, 'config'):
        policy.config.cfg_enabled = True
    print("âœ… CFGå·²å¯ç”¨ï¼Œè®­ç»ƒå’Œæ¨ç†éƒ½å°†ä½¿ç”¨CFGåˆ†æ”¯")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    policy = policy.to(device)
    print(f"âœ“ ç­–ç•¥åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    # ğŸ”¥ ä¿®å¤ï¼šåªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå†»ç»“PaliGemmaå‰ç¼€ï¼ˆæå‡ç¨³å®šæ€§ï¼‰
    print("ğŸ”§ é…ç½®è®­ç»ƒå‚æ•°èŒƒå›´...")
    
    # 1. å†»ç»“PaliGemmaå‰ç¼€
    for p in policy.model.paligemma_with_expert.parameters():
        p.requires_grad = False
    
    # 2. åªæ”¶é›†éœ€è¦è®­ç»ƒçš„å‚æ•°
    trainable_params = []
    trainable_params += list(policy.model.action_in_proj.parameters())
    trainable_params += list(policy.model.action_time_mlp_in.parameters())
    trainable_params += list(policy.model.action_time_mlp_out.parameters())
    trainable_params += list(policy.model.action_out_proj.parameters())
    trainable_params += list(policy.model.state_proj.parameters())
    
    # 3. CFG embeddingå‚æ•°
    if hasattr(policy.model, "cfg_emb"):
        trainable_params += list(policy.model.cfg_emb.parameters())
        print("âœ… CFG embeddingå‚æ•°å·²åŠ å…¥è®­ç»ƒ")
    
    # 4. åˆ›å»ºä¼˜åŒ–å™¨
    print("æ­£åœ¨åˆ›å»ºä¼˜åŒ–å™¨...")
    lr = config['algo'].get('lr', 1e-5)
    optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)
    
    total_params = sum(p.numel() for p in trainable_params)
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼Œå­¦ä¹ ç‡: {lr}")
    print(f"ğŸ¯ åªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå‚æ•°æ•°é‡: {total_params:,}")
    
    return policy, optimizer, device

def create_environment_runner(config: Dict[str, Any], policy):
    """åˆ›å»ºç¯å¢ƒrunnerï¼ˆRIPT-VLAé£æ ¼é€‰æ‹©ï¼‰"""
    use_ript_vla = config.get('features', {}).get('use_ript_vla_runner', False)
    
    print(f"ğŸ” Runneré€‰æ‹©: use_ript_vla_runner = {use_ript_vla}")
    
    if use_ript_vla and RIPT_VLA_RUNNER_AVAILABLE:
        print("ğŸš€ ä½¿ç”¨RIPT-VLAé£æ ¼ç¯å¢ƒrunner")
        
        runner = PI0LiberoRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            rank=0
        )
        
    elif ORIGINAL_RUNNER_AVAILABLE:
        print("ğŸ”„ ä½¿ç”¨åŸæœ‰ç¯å¢ƒrunner")
        
        # ç¡®ä¿norm_stats_pathå­˜åœ¨
        norm_stats_path = config.get('norm_stats_path')
        if not norm_stats_path:
            norm_stats_path = f"{config['policy_path']}/norm_stats.json"
        
        runner = LIBEROEnvRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            norm_stats_path=norm_stats_path,
            config=config,
            rank=0,
            world_size=1
        )
    else:
        raise RuntimeError("âŒ æ²¡æœ‰å¯ç”¨çš„ç¯å¢ƒrunnerï¼")
    
    print("âœ“ ç¯å¢ƒrunneråˆ›å»ºæˆåŠŸ")
    return runner

def _dynamic_filter_rollouts(episodes: List[Dict], enable_dynamic_sampling: bool) -> List[Dict]:
    """æŒ‰RIPT-VLAæ€è·¯çš„æœ€å°åŠ¨æ€é‡‡æ ·ï¼šä¸¢å¼ƒå…¨0æˆ–å…¨1æˆåŠŸç‡çš„æ‰¹æ¬¡"""
    if not enable_dynamic_sampling or not episodes:
        return episodes
    successes = [bool(ep.get('success', False)) for ep in episodes]
    if len(successes) > 0 and (all(successes) or not any(successes)):
        print(f"âš ï¸ åŠ¨æ€é‡‡æ ·ä¸¢å¼ƒæœ¬æ‰¹æ¬¡ (uniform successes: {successes})")
        return []
    return episodes


def collect_rollouts_ript_vla_style(env_runner, task_name, num_rollouts, enable_dynamic_sampling: bool = False):
    """
    RIPT-VLAé£æ ¼çš„rolloutæ”¶é›†
    ç›´æ¥è°ƒç”¨runnerï¼Œæ— ä¸­é—´å±‚
    """
    print(f"æ­£åœ¨æ”¶é›† {num_rollouts} ä¸ªrollouts...")
    
    try:
        # è·å–ä»»åŠ¡çš„åˆå§‹çŠ¶æ€
        task_id = 0  # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
        if hasattr(env_runner, 'benchmark'):
            all_init_states = env_runner.benchmark.get_task_init_states(task_id)
        else:
            all_init_states = None
        
        # ç›´æ¥è°ƒç”¨ç¯å¢ƒrunnerçš„æ–¹æ³•
        rollout_generator = env_runner.run_policy_in_env(
            env_name=task_name,
            all_init_states=all_init_states
        )
        
        # æ”¶é›†æ‰€æœ‰rollouts
        collected_rollouts = []
        rollout_count = 0
        
        for success, total_reward, episode_data in rollout_generator:
            episode = {
                'success': success,
                'total_reward': total_reward,
                **episode_data
            }
            collected_rollouts.append(episode)
            rollout_count += 1
            
            if rollout_count >= num_rollouts:
                break
        
        # æœ€å°åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼šä¸¢å¼ƒå…¨0æˆ–å…¨1æ‰¹æ¬¡
        filtered = _dynamic_filter_rollouts(collected_rollouts, enable_dynamic_sampling)
        if not filtered:
            print("âš ï¸ æœ¬æ‰¹æ¬¡è¢«åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼Œè¿”å›ç©ºé›†")
        else:
            print(f"âœ“ æˆåŠŸæ”¶é›†äº† {len(filtered)} ä¸ªrollouts (è¿‡æ»¤å)")
        return filtered
        
    except Exception as e:
        print(f"âŒ Rolloutæ”¶é›†å¤±è´¥: {e}")
        traceback.print_exc()
        return []

def compute_advantages_rloo(episodes: List[Dict], rloo_batch_size: int = None) -> torch.Tensor:
    """
    æ­£å®—çš„RLOO (Reward Ranked Leave-One-Out) ä¼˜åŠ¿è®¡ç®—
    
    Args:
        episodes: æ”¶é›†çš„episodesåˆ—è¡¨
        rloo_batch_size: RLOOæ‰¹æ¬¡å¤§å°ï¼Œç”¨äºLeave-One-Outè®¡ç®—
    
    Returns:
        torch.Tensor: è®¡ç®—å¾—åˆ°çš„ä¼˜åŠ¿å€¼
    """
    if not episodes:
        return torch.tensor([])
    
    # æå–å¥–åŠ±
    rewards = []
    for ep in episodes:
        reward = ep.get('total_reward', 0.0)
        rewards.append(float(reward))
    
    rlhf_reward = torch.tensor(rewards, dtype=torch.float32)
    num_rollouts = len(episodes)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨çœŸæ­£çš„RLOOæ‰¹æ¬¡å¤§å°è€Œä¸æ˜¯æ€»æ•°
    if rloo_batch_size is None or rloo_batch_size <= 1:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæˆ–batch sizeè¿‡å°ï¼Œé€€åŒ–ä¸ºç®€å•æ–¹æ³•
        print("âš ï¸ RLOO batch sizeæœªæŒ‡å®šæˆ–è¿‡å°ï¼Œä½¿ç”¨ç®€å•ä¼˜åŠ¿è®¡ç®—")
        advantage = rlhf_reward - rlhf_reward.mean()
    else:
        # ğŸš€ æ­£å®—RLOOè®¡ç®—
        try:
            # ç¡®ä¿å¯ä»¥æ•´é™¤ï¼Œå¦‚æœä¸èƒ½æ•´é™¤åˆ™è£å‰ªåˆ°æœ€å¤§å¯æ•´é™¤æ•°é‡
            effective_rollouts = (num_rollouts // rloo_batch_size) * rloo_batch_size
            if effective_rollouts != num_rollouts:
                print(f"ğŸ”§ RLOOè°ƒæ•´ï¼š{num_rollouts} â†’ {effective_rollouts} rollouts (batch_size={rloo_batch_size})")
                rlhf_reward = rlhf_reward[:effective_rollouts]
                num_rollouts = effective_rollouts
            
            num_batches = num_rollouts // rloo_batch_size
            rlhf_reward_reshaped = rlhf_reward.reshape(num_batches, rloo_batch_size)
            
            # æ ‡å‡†RLOOï¼šæ¯ä¸ªæ ·æœ¬çš„baseline = åŒæ‰¹æ¬¡å…¶ä»–æ ·æœ¬çš„å¹³å‡å€¼
            # baseline[i,j] = (sum(batch[i]) - reward[i,j]) / (batch_size - 1)
            batch_sums = rlhf_reward_reshaped.sum(dim=1, keepdim=True)  # (num_batches, 1)
            baseline = (batch_sums - rlhf_reward_reshaped) / (rloo_batch_size - 1)  # (num_batches, rloo_batch_size)
            
            # ä¼˜åŠ¿ = è‡ªå·±çš„å¥–åŠ± - å…¶ä»–äººçš„å¹³å‡å¥–åŠ±
            advantage = rlhf_reward_reshaped - baseline  # (num_batches, rloo_batch_size)
            advantage = advantage.flatten()  # å±•å¹³ä¸ºä¸€ç»´
            
            # NaNå’ŒInfæ£€æŸ¥
            if torch.isnan(advantage).any() or torch.isinf(advantage).any():
                print("âš ï¸ RLOOè®¡ç®—äº§ç”ŸNaN/Infï¼Œä½¿ç”¨å®‰å…¨æ›¿æ¢")
                advantage = torch.nan_to_num(advantage, nan=0.0, posinf=1.0, neginf=-1.0)
            
            print(f"ğŸ¯ æ­£å®—RLOOä¼˜åŠ¿è®¡ç®—å®Œæˆ:")
            print(f"   æ‰¹æ¬¡é…ç½®: {num_rollouts} rollouts â†’ {num_batches} batches Ã— {rloo_batch_size}")
            print(f"   ä¼˜åŠ¿ç»Ÿè®¡: mean={advantage.mean():.4f}, std={advantage.std():.4f}")
            print(f"   æ­£ä¼˜åŠ¿æ¯”ä¾‹: {(advantage > 0).float().mean():.2%}")
            
        except Exception as e:
            print(f"âŒ RLOOè®¡ç®—å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•")
            advantage = rlhf_reward - rlhf_reward.mean()
    
    return advantage

def update_policy_ript_vla_style(policy, optimizer, cfg_adapter, episodes, advantages, device):
    """
    RIPT-VLAé£æ ¼çš„ç­–ç•¥æ›´æ–°
    ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†ï¼Œæ— å¤æ‚ç»„ä»¶
    """
    if not episodes or len(advantages) == 0:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œç­–ç•¥æ›´æ–°")
        return 0.0
    
    print(f"æ­£åœ¨æ›´æ–°ç­–ç•¥ï¼ˆ{len(episodes)} ä¸ªepisodesï¼‰...")
    
    try:
        # è®¡ç®—åŠ æƒæŸå¤±
        advantages = advantages.to(device)
        loss = cfg_adapter.compute_weighted_loss(episodes, advantages, device)
        
        # æ¢¯åº¦æ›´æ–°
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
        
        optimizer.step()
        
        loss_value = loss.item()
        print(f"âœ“ ç­–ç•¥æ›´æ–°å®Œæˆï¼ŒæŸå¤±: {loss_value:.6f}")
        
        return loss_value
        
    except Exception as e:
        print(f"âŒ ç­–ç•¥æ›´æ–°å¤±è´¥: {e}")
        traceback.print_exc()
        return 0.0

def evaluate_with_cfg_sweep(policy, env_runner, task_name, eval_episodes=3):
    """ğŸ”¥ æ–°å¢ï¼šè¯„ä¼°ä¸åŒCFGå¼ºåº¦çš„æ•ˆæœ"""
    cfg_scales = [1.0, 1.5, 3.0, 5.0]
    best_cfg = 1.0
    best_success_rate = 0.0
    
    results = {}
    print(f"\nğŸ” å¼€å§‹CFGå¼ºåº¦æ‰«æè¯„ä¼°...")
    
    for cfg_scale in cfg_scales:
        print(f"ğŸ“Š æµ‹è¯•CFG={cfg_scale}...")
        # ä¸´æ—¶è®¾ç½®CFGå¼ºåº¦
        original_cfg = getattr(env_runner.config, 'collection_cfg_scale', 1.5)
        env_runner.config.collection_cfg_scale = cfg_scale
        
        # è¿è¡Œè¯„ä¼°episodes
        success_count = 0
        for ep_idx in range(eval_episodes):
            try:
                # ä½¿ç”¨ç°æœ‰çš„rolloutæ”¶é›†å‡½æ•°
                episodes = collect_rollouts_ript_vla_style(
                    env_runner, task_name, 1, enable_dynamic_sampling=False
                )
                if episodes and len(episodes) > 0:
                    if episodes[0].get('success', False):
                        success_count += 1
            except Exception as e:
                print(f"   è¯„ä¼°episode {ep_idx} å¤±è´¥: {e}")
                continue
        
        success_rate = success_count / eval_episodes
        results[cfg_scale] = success_rate
        
        if success_rate > best_success_rate:
            best_success_rate = success_rate
            best_cfg = cfg_scale
        
        # æ¢å¤åŸè®¾ç½®
        env_runner.config.collection_cfg_scale = original_cfg
        
        print(f"   CFG={cfg_scale}: æˆåŠŸç‡={success_rate:.2%} ({success_count}/{eval_episodes})")
    
    print(f"ğŸ¯ æœ€ä½³CFGå¼ºåº¦: {best_cfg} (æˆåŠŸç‡: {best_success_rate:.2%})")
    return best_cfg, results

def main_training_loop_ript_vla_style(config: Dict[str, Any]):
    """
    ä¸»è®­ç»ƒå¾ªç¯ï¼ˆRIPT-VLAé£æ ¼ï¼‰
    ç›´æ¥åœ¨ä¸»å‡½æ•°ä¸­å¤„ç†æ‰€æœ‰é€»è¾‘ï¼Œå‡å°‘æŠ½è±¡å±‚
    """
    print("ğŸš€ å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒå¾ªç¯")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(config['output_dir'])
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_name = config.get('exp_name', 'ript_vla_style_train')
    output_dir = output_dir / f"{exp_name}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨
    policy, optimizer, device = create_policy_and_optimizer(config)
    
    # åˆ›å»ºCFGé€‚é…å™¨ï¼ˆå¿…éœ€ï¼Œç”¨äºæŸå¤±è®¡ç®—ï¼‰
    # ğŸ”¥ æ–°å¢ï¼šçª—å£åŒ–é…ç½®æ”¯æŒ
    dataset_config = config.get('dataset', {})
    windowing_mode = dataset_config.get('windowing_mode', 'last')
    window_stride = dataset_config.get('window_stride', 10)
    max_windows_per_episode = dataset_config.get('max_windows_per_episode', 1)
    
    print(f"\nğŸ”§ CFGçª—å£åŒ–é…ç½®:")
    print(f"  æ¨¡å¼: {windowing_mode}")
    print(f"  æ­¥é•¿: {window_stride}")
    print(f"  æ¯episodeæœ€å¤§çª—å£æ•°: {max_windows_per_episode}")
    
    cfg_adapter = PI0_CFG_Adapter(
        policy=policy,
        norm_stats_path=f"{config['policy_path']}/norm_stats.json",
        windowing_mode=windowing_mode,
        window_stride=window_stride,
        max_windows_per_episode=max_windows_per_episode
    )
    
    # åˆ›å»ºç¯å¢ƒrunner
    env_runner = create_environment_runner(config, policy)
    
    # è®­ç»ƒé…ç½®
    num_train_steps = config['training']['num_train_steps']
    # ä¸2_test_pi0_on_libero.pyå¯¹é½ï¼šä½¿ç”¨libero_goalåŸºå‡†é»˜è®¤task_id=1
    # è‹¥YAMLä¸­æ˜ç¡®ç»™äº†task_names_to_useï¼Œåˆ™ä»ç„¶ä½¿ç”¨ç¬¬ä¸€ä¸ªåç§°åšæ˜¾ç¤ºï¼Œä¸å½±å“ç¯å¢ƒå†…éƒ¨task_idé€‰æ‹©
    task_name = config['task'].get('task_names_to_use', ['libero_goal_default'])[0]
    rloo_batch_size = config['algo']['rloo_batch_size']
    
    print(f"\nå¼€å§‹è®­ç»ƒå¾ªç¯:")
    print(f"  è®­ç»ƒæ­¥æ•°: {num_train_steps}")
    print(f"  ä»»åŠ¡: {task_name}")
    print(f"  æ‰¹æ¬¡å¤§å°: {rloo_batch_size}")
    print()
    
    all_training_metrics = []
    
    # ğŸ”¥ ä¸»è®­ç»ƒå¾ªç¯ - RIPT-VLAé£æ ¼
    for step in range(num_train_steps):
        step_start_time = time.time()
        
        print(f"=== è®­ç»ƒæ­¥éª¤ {step + 1}/{num_train_steps} ===")
        
        # 1. æ”¶é›†rolloutsï¼ˆç›´æ¥è°ƒç”¨ï¼Œæ— ä¸­é—´å±‚ï¼‰
        episodes = collect_rollouts_ript_vla_style(
            env_runner, task_name, rloo_batch_size,
            enable_dynamic_sampling=config['algo'].get('enable_dynamic_sampling', False)
        )
        
        if not episodes:
            print("âš ï¸ æœªæ”¶é›†åˆ°æœ‰æ•ˆepisodesï¼Œè·³è¿‡æ­¤æ­¥")
            continue
        
        # 2. è®¡ç®—ä¼˜åŠ¿ï¼ˆæ­£å®—RLOOæ–¹æ³•ï¼‰
        advantages = compute_advantages_rloo(episodes, rloo_batch_size=rloo_batch_size)
        
        # 3. æ›´æ–°ç­–ç•¥ï¼ˆç›´æ¥æ›´æ–°ï¼Œæ— å¤æ‚ç»„ä»¶ï¼‰
        loss = update_policy_ript_vla_style(
            policy, optimizer, cfg_adapter, episodes, advantages, device
        )
        
        # 4. è®°å½•æŒ‡æ ‡
        avg_reward = np.mean([ep['total_reward'] for ep in episodes])
        success_rate = np.mean([ep['success'] for ep in episodes])
        step_time = time.time() - step_start_time
        
        step_metrics = {
            'step': step + 1,
            'num_episodes': len(episodes),
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'loss': loss,
            'step_time': step_time
        }
        all_training_metrics.append(step_metrics)
        
        # 5. è¾“å‡ºç»“æœ
        print(f"âœ“ æ­¥éª¤ {step + 1} å®Œæˆ:")
        print(f"  Episodes: {len(episodes)}")
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        print(f"  æˆåŠŸç‡: {success_rate:.2%}")
        print(f"  æŸå¤±: {loss:.6f}")
        print(f"  è€—æ—¶: {step_time:.2f}ç§’")
        
        # 6. CFGè¯„ä¼°ï¼ˆæ¯10æ­¥è¿›è¡Œä¸€æ¬¡ï¼‰
        if (step + 1) % 10 == 0:
            try:
                best_cfg, cfg_results = evaluate_with_cfg_sweep(policy, env_runner, task_name, eval_episodes=2)
                step_metrics['best_cfg_scale'] = best_cfg
                step_metrics['cfg_sweep_results'] = cfg_results
                print(f"ğŸ¯ æ¨èCFGå¼ºåº¦: {best_cfg}")
                # å¯é€‰ï¼šåŠ¨æ€è°ƒæ•´æ”¶é›†æ—¶ä½¿ç”¨çš„CFGå¼ºåº¦
                env_runner.config.collection_cfg_scale = best_cfg
            except Exception as e:
                print(f"âš ï¸ CFGè¯„ä¼°å¤±è´¥: {e}")
        
        # 7. ä¿å­˜æ£€æŸ¥ç‚¹
        if (step + 1) % config['training'].get('save_freq', 10) == 0:
            # è½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼Œä¾¿äºéƒ¨ç½²ä¸å ç”¨å°ï¼‰
            weights_path = output_dir / f"weights_step_{step + 1}.pt"
            torch.save({
                'step': step + 1,
                'policy_state_dict': policy.state_dict(),
                'config': config,
                'training_metrics': all_training_metrics,
            }, weights_path)
            print(f"âœ“ è½»é‡æƒé‡å·²ä¿å­˜: {weights_path}")

            # å¯é€‰ï¼šæŒ‰è¾ƒä½é¢‘ç‡ä¿å­˜å«ä¼˜åŒ–å™¨çš„å®Œæ•´æ£€æŸ¥ç‚¹ï¼Œä¾¿äºæ¢å¤è®­ç»ƒ
            save_opt_every = config.get('training', {}).get('save_optimizer_freq', None)
            if save_opt_every and ((step + 1) % int(save_opt_every) == 0):
                checkpoint_path = output_dir / f"checkpoint_step_{step + 1}.pt"
                torch.save({
                    'step': step + 1,
                    'policy_state_dict': policy.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'config': config,
                    'training_metrics': all_training_metrics,
                }, checkpoint_path)
                print(f"âœ“ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results_path = output_dir / "final_training_results.json"
    # å°† OmegaConf è½¬ä¸ºåŸç”Ÿ dict ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if OMEGACONF_AVAILABLE and isinstance(config, DictConfig):
        serializable_config = OmegaConf.to_container(config, resolve=True)
    else:
        serializable_config = config
    with open(final_results_path, 'w') as f:
        json.dump({
            'config': serializable_config,
            'training_metrics': all_training_metrics,
            'total_steps': len(all_training_metrics)
        }, f, indent=2)
    
    # æœ€ç»ˆè½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼‰
    final_weights_path = output_dir / "final_weights.pt"
    torch.save({
        'step': len(all_training_metrics),
        'policy_state_dict': policy.state_dict(),
        'config': config,
        'training_metrics': all_training_metrics,
    }, final_weights_path)
    print(f"âœ“ æœ€ç»ˆè½»é‡æƒé‡å·²ä¿å­˜: {final_weights_path}")

    # å¯é€‰ï¼šä¿å­˜æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆå«ä¼˜åŒ–å™¨ï¼‰ä¾¿äºæ¢å¤è®­ç»ƒ
    if config.get('training', {}).get('save_optimizer_final', False):
        final_checkpoint_path = output_dir / "final_checkpoint.pt"
        torch.save({
            'step': len(all_training_metrics),
            'policy_state_dict': policy.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'config': config,
            'training_metrics': all_training_metrics,
        }, final_checkpoint_path)
        print(f"âœ“ æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")

    print(f"\nğŸ‰ RIPT-VLAé£æ ¼è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœå·²ä¿å­˜: {final_results_path}")
    print(f"âœ¨ ä½¿ç”¨äº†ç®€åŒ–çš„ç›´æ¥æ¶æ„ï¼Œå‡å°‘äº†æŠ½è±¡å±‚å¤æ‚åº¦")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ")
    parser.add_argument(
        "--config_path", 
        type=str, 
        required=True,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config_path)
        
        # æ˜¾ç¤ºé…ç½®
        print("\n====== ä½¿ç”¨é…ç½® ======")
        if OMEGACONF_AVAILABLE:
            print(OmegaConf.to_yaml(config))
        else:
            print(yaml.dump(config, default_flow_style=False, allow_unicode=True))
        print("====================\n")
        
        # å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒ
        main_training_loop_ript_vla_style(config)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()