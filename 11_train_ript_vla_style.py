#!/usr/bin/env python3
"""
Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–ç‰ˆæœ¬
åŸºäºRIPT-VLAçš„ç›´æ¥æ¶æ„æ¨¡å¼ï¼Œå»é™¤å¤šä½™çš„æŠ½è±¡å±‚

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
1. ç›´æ¥åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†rolloutæ”¶é›†å’Œä¼˜åŒ–
2. ç®€åŒ–çš„ç»„ä»¶æ¶æ„ï¼Œå‡å°‘ä¸­é—´å±‚
3. ç›´æ¥ä½¿ç”¨SubprocVectorEnvè¿›è¡Œå¹¶è¡Œ
4. æ¨¡ä»¿RIPT-VLAçš„æˆåŠŸæ¨¡å¼


python 11_train_ript_vla_style.py --config_path pi0/ript/config/stage11_parallel_test.yaml 
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from datetime import datetime
import argparse
from typing import List, Dict, Any, Optional
import yaml
import traceback
import time
from tqdm import tqdm

# ğŸ”¥ æ—©æœŸè®¾ç½®multiprocessingï¼Œé¿å…å­è¿›ç¨‹é‡å¤è®¾ç½®
import multiprocessing as mp
if mp.get_start_method(allow_none=True) != "spawn":
    mp.set_start_method("spawn", force=True)

# ğŸ”¥ æ·»åŠ RIPTå¯¹é½çš„æ•°æ®é›†å·¥å…·
from pi0.ript.utils.libero_utils_ript_aligned import (
    build_dataset_ript_aligned,
    collate_fn_ript_aligned
)

import hashlib

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Šå’ŒEGLé”™è¯¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["EGL_LOG_LEVEL"] = "fatal"  # æŠ‘åˆ¶EGLé”™è¯¯è¾“å‡º

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(f"=== Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ ===")
print(f"è„šæœ¬ä½ç½®: {current_file}")
print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print()

class RolloutStatsTracker:
    """
    æ¯ä¸ªåˆå§‹çŠ¶æ€çš„rolloutç»Ÿè®¡è·Ÿè¸ªå™¨
    å®ç°per-initè·³è¿‡æœºåˆ¶ï¼Œä¸RIPTåŸç‰ˆå¯¹é½
    """
    def __init__(self, rollout_skip_threshold: int = 3, stats_path: Optional[str] = None):
        self.rollout_stats = {}  # {(task_id, init_hash): [success_history]}
        self.rollout_skip_cnt = {}  # {(task_id, init_hash): skip_count}
        self.rollout_skip_threshold = rollout_skip_threshold
        self.stats_path = stats_path
        
        # åŠ è½½å·²æœ‰ç»Ÿè®¡
        if stats_path and Path(stats_path).exists():
            self._load_stats()
        
        print(f"ğŸ”§ RolloutStatsTrackeråˆå§‹åŒ–:")
        print(f"  è·³è¿‡é˜ˆå€¼: {rollout_skip_threshold}")
        print(f"  ç»Ÿè®¡è·¯å¾„: {stats_path}")
        print(f"  å·²æœ‰ç»Ÿè®¡: {len(self.rollout_stats)} ä¸ªinit")
    
    def _compute_init_hash(self, task_id: int, init_state_data: Any) -> str:
        """è®¡ç®—åˆå§‹çŠ¶æ€çš„ç¨³å®šå“ˆå¸Œå€¼ï¼ˆfloat64 + contiguousï¼‰"""
        if isinstance(init_state_data, torch.Tensor):
            # è½¬æ¢ä¸ºfloat64 contiguous numpyæ•°ç»„
            numpy_data = np.ascontiguousarray(init_state_data.cpu().numpy(), dtype=np.float64)
        elif isinstance(init_state_data, np.ndarray):
            # ç¡®ä¿æ˜¯float64 contiguous
            numpy_data = np.ascontiguousarray(init_state_data, dtype=np.float64)
        else:
            # å›é€€åˆ°å­—ç¬¦ä¸²
            data_bytes = str(init_state_data).encode()
            return hashlib.sha256(data_bytes).hexdigest()[:16]
        
        # ä½¿ç”¨ç¨³å®šçš„byteså“ˆå¸Œ
        data_bytes = numpy_data.tobytes()
        return hashlib.sha256(data_bytes).hexdigest()[:16]  # çŸ­å“ˆå¸Œ
    
    def should_skip_init(self, task_id: int, init_hash: str, rloo_batch_size: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡è¿™ä¸ªåˆå§‹çŠ¶æ€
        RIPTåŸç‰ˆé€»è¾‘ï¼šæœ€è¿‘K=rloo_batch_sizeæ¬¡å…¨æˆåŠŸåˆ™è·³è¿‡
        """
        key = (task_id, init_hash)
        
        if key not in self.rollout_stats:
            return False
        
        history = self.rollout_stats[key]
        if len(history) < rloo_batch_size:
            return False
        
        # æ£€æŸ¥æœ€è¿‘Kæ¬¡æ˜¯å¦å…¨æˆåŠŸ
        recent_k = history[-rloo_batch_size:]
        all_successful = all(s == 1 for s in recent_k)
        
        if all_successful:
            print(f"ğŸš« è·³è¿‡init ({task_id}, {init_hash}): æœ€è¿‘{rloo_batch_size}æ¬¡å…¨æˆåŠŸ")
            return True
        
        return False
    
    def update_stats(self, task_id: int, init_hash: str, successes: List[bool]):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        key = (task_id, init_hash)
        
        if key not in self.rollout_stats:
            self.rollout_stats[key] = []
            self.rollout_skip_cnt[key] = 0
        
        # æ·»åŠ æ–°çš„æˆåŠŸè®°å½•
        success_ints = [1 if s else 0 for s in successes]
        self.rollout_stats[key].extend(success_ints)
        
        # ä¿æŒå†å²è®°å½•é•¿åº¦åˆç†ï¼ˆæœ€å¤šä¿ç•™100æ¬¡ï¼‰
        if len(self.rollout_stats[key]) > 100:
            self.rollout_stats[key] = self.rollout_stats[key][-100:]
        
        print(f"ğŸ“Š æ›´æ–°ç»Ÿè®¡ ({task_id}, {init_hash}): +{len(successes)} æ¬¡ï¼Œ"
              f"æ€»è®¡ {len(self.rollout_stats[key])} æ¬¡ï¼Œ"
              f"æˆåŠŸç‡ {np.mean(self.rollout_stats[key]):.2%}")
    
    def increment_skip_count(self, task_id: int, init_hash: str):
        """å¢åŠ è·³è¿‡è®¡æ•°"""
        key = (task_id, init_hash)
        if key not in self.rollout_skip_cnt:
            self.rollout_skip_cnt[key] = 0
        
        self.rollout_skip_cnt[key] += 1
        
        # å¦‚æœè·³è¿‡æ¬¡æ•°è¿‡å¤šï¼Œç§»é™¤è¿™ä¸ªinitï¼ˆé¿å…æ°¸ä¹…è·³è¿‡ï¼‰
        if self.rollout_skip_cnt[key] > self.rollout_skip_threshold:
            print(f"ğŸ—‘ï¸ ç§»é™¤init ({task_id}, {init_hash}): è·³è¿‡æ¬¡æ•°è¶…è¿‡é˜ˆå€¼")
            if key in self.rollout_stats:
                del self.rollout_stats[key]
            del self.rollout_skip_cnt[key]
    
    def _load_stats(self):
        """åŠ è½½ç»Ÿè®¡æ•°æ®"""
        try:
            with open(self.stats_path, 'r') as f:
                data = json.load(f)
                self.rollout_stats = data.get('rollout_stats', {})
                self.rollout_skip_cnt = data.get('rollout_skip_cnt', {})
                
                # è½¬æ¢å­—ç¬¦ä¸²é”®ä¸ºå…ƒç»„
                new_stats = {}
                new_skip_cnt = {}
                for key, value in self.rollout_stats.items():
                    if isinstance(key, str) and ',' in key:
                        task_id, init_hash = key.strip('()').split(', ')
                        new_key = (int(task_id), init_hash.strip("'\""))
                        new_stats[new_key] = value
                    else:
                        new_stats[key] = value
                
                for key, value in self.rollout_skip_cnt.items():
                    if isinstance(key, str) and ',' in key:
                        task_id, init_hash = key.strip('()').split(', ')
                        new_key = (int(task_id), init_hash.strip("'\""))
                        new_skip_cnt[new_key] = value
                    else:
                        new_skip_cnt[key] = value
                
                self.rollout_stats = new_stats
                self.rollout_skip_cnt = new_skip_cnt
                
                print(f"âœ… åŠ è½½ç»Ÿè®¡æ•°æ®: {len(self.rollout_stats)} ä¸ªinit")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    
    def save_stats(self):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®"""
        if not self.stats_path:
            return
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(self.stats_path).parent.mkdir(parents=True, exist_ok=True)
            
            # è½¬æ¢å…ƒç»„é”®ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿JSONåºåˆ—åŒ–
            serializable_stats = {}
            serializable_skip_cnt = {}
            
            for key, value in self.rollout_stats.items():
                str_key = str(key)
                serializable_stats[str_key] = value
            
            for key, value in self.rollout_skip_cnt.items():
                str_key = str(key)
                serializable_skip_cnt[str_key] = value
            
            data = {
                'rollout_stats': serializable_stats,
                'rollout_skip_cnt': serializable_skip_cnt,
                'timestamp': datetime.now().isoformat()
            }
            
            with open(self.stats_path, 'w') as f:
                json.dump(data, f, indent=2)
                
            print(f"ğŸ’¾ ç»Ÿè®¡æ•°æ®å·²ä¿å­˜: {self.stats_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")

# å¯¼å…¥é…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ OmegaConfé…ç½®ç®¡ç†å·²å¯ç”¨")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€YAMLåŠ è½½")

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    print("æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
    
    # PI0ç­–ç•¥
    from pi0.modeling_pi0 import PI0Policy
    print("âœ“ PI0ç­–ç•¥æ¨¡å—")
    
    # RIPTç»„ä»¶ - åªå¯¼å…¥å¿…éœ€çš„
    from pi0.ript.reward_function import BinarySuccessReward
    from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter
    print("âœ“ RIPTæ ¸å¿ƒç»„ä»¶")

    # ğŸ”¥ ä½¿ç”¨RIPTå¯¹é½çš„æ•°æ®åŠ è½½å™¨ï¼ˆä¿®å¤MuJoCoçŠ¶æ€é—®é¢˜ï¼‰
    print("âœ“ ä½¿ç”¨RIPTå¯¹é½æ•°æ®åŠ è½½å™¨")
    
    # # å¯¼å…¥ç®€åŒ–çš„ç¯å¢ƒrunner
    # try:
    #     from pi0.ript.env.pi0_libero_runner_ript_vla import PI0LiberoRunner
    #     RIPT_VLA_RUNNER_AVAILABLE = True
    #     print("âœ“ RIPT-VLA Runner")
    # except ImportError as e:
    #     print(f"âš ï¸ RIPT-VLA runnerå¯¼å…¥å¤±è´¥: {e}")
    #     RIPT_VLA_RUNNER_AVAILABLE = False
    # é»˜è®¤å…³é—­RIPT-VLA Runnerï¼ˆè‹¥ä¸Šæ–¹å¯¼å…¥è¢«æ³¨é‡Šæˆ–å¤±è´¥æ—¶ä¿æŒä¸ºFalseï¼‰
    RIPT_VLA_RUNNER_AVAILABLE = False
        
    # å¤‡ç”¨å¯¼å…¥åŸæœ‰runner
    try:
        from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
        ORIGINAL_RUNNER_AVAILABLE = True
        print("âœ“ åŸæœ‰LIBEROEnvRunner")
    except ImportError as e:
        print("âš ï¸ åŸæœ‰runnerä¹Ÿä¸å¯ç”¨")
        ORIGINAL_RUNNER_AVAILABLE = False
    
    print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥å®Œæˆ")
    
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def load_config(config_path: str):
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨OmegaConfï¼Œä¾¿äºå±æ€§è®¿é—®ï¼‰"""
    print(f"æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    if OMEGACONF_AVAILABLE:
        config = OmegaConf.load(config_path)
    else:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    
    # ç®€å•çš„ç±»å‹è½¬æ¢
    try:
        lr = config["algo"]["lr"]
        if isinstance(lr, str):
            if OMEGACONF_AVAILABLE:
                config.algo.lr = float(lr)
            else:
                config["algo"]["lr"] = float(lr)
    except Exception:
        pass
    
    print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    return config

def create_policy_and_optimizer(config: Dict[str, Any]):
    """åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨ï¼ˆRIPT-VLAé£æ ¼ï¼‰"""
    print("æ­£åœ¨åŠ è½½PI0ç­–ç•¥...")
    
    policy_path = config['policy_path']
    policy = PI0Policy.from_pretrained(policy_path)
    
    # ğŸ”§ æ ¹æ®é…ç½®æ§åˆ¶CFGåŠŸèƒ½
    policy_config = config.get('policy', {})
    cfg_enabled = policy_config.get('cfg_enabled', True)  # é»˜è®¤å¯ç”¨ä»¥ä¿æŒå…¼å®¹æ€§

    print(f"ğŸ”§ é…ç½®CFGåŠŸèƒ½: {'å¯ç”¨' if cfg_enabled else 'ç¦ç”¨'}")
    policy.model.cfg_enabled = cfg_enabled
    if hasattr(policy, 'config'):
        policy.config.cfg_enabled = cfg_enabled

    if cfg_enabled:
        print("âœ… CFGå·²å¯ç”¨ï¼Œè®­ç»ƒå’Œæ¨ç†å°†ä½¿ç”¨CFGåˆ†æ”¯")
    else:
        print("âš ï¸ CFGå·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    policy = policy.to(device)
    print(f"âœ“ ç­–ç•¥åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    # ğŸ”¥ ä¿®å¤ï¼šåªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå†»ç»“PaliGemmaå‰ç¼€ï¼ˆæå‡ç¨³å®šæ€§ï¼‰
    print("ğŸ”§ é…ç½®è®­ç»ƒå‚æ•°èŒƒå›´...")
    
    # 1. å†»ç»“PaliGemmaå‰ç¼€
    for p in policy.model.paligemma_with_expert.parameters():
        p.requires_grad = False
    
    # 2. åªæ”¶é›†éœ€è¦è®­ç»ƒçš„å‚æ•°
    trainable_params = []
    trainable_params += list(policy.model.action_in_proj.parameters())
    trainable_params += list(policy.model.action_time_mlp_in.parameters())
    trainable_params += list(policy.model.action_time_mlp_out.parameters())
    trainable_params += list(policy.model.action_out_proj.parameters())
    trainable_params += list(policy.model.state_proj.parameters())
    
    # 3. CFG embeddingå‚æ•° (ä»…åœ¨CFGå¯ç”¨æ—¶è®­ç»ƒ)
    if hasattr(policy.model, "cfg_emb") and getattr(policy.model, 'cfg_enabled', True):
        trainable_params += list(policy.model.cfg_emb.parameters())
        print("âœ… CFG embeddingå‚æ•°å·²åŠ å…¥è®­ç»ƒ")
    elif hasattr(policy.model, "cfg_emb"):
        print("âš ï¸ CFGå·²ç¦ç”¨ï¼Œè·³è¿‡CFG embeddingå‚æ•°è®­ç»ƒ")
    
    # 4. åˆ›å»ºä¼˜åŒ–å™¨
    print("æ­£åœ¨åˆ›å»ºä¼˜åŒ–å™¨...")
    lr = config['algo'].get('lr', 1e-5)
    optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)
    
    total_params = sum(p.numel() for p in trainable_params)
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼Œå­¦ä¹ ç‡: {lr}")
    print(f"ğŸ¯ åªè®­ç»ƒä¸“å®¶å¤´éƒ¨ï¼Œå‚æ•°æ•°é‡: {total_params:,}")
    
    return policy, optimizer, device

def is_dynamic_sampling_enabled(config: Dict[str, Any]) -> bool:
    """ç»Ÿä¸€åŠ¨æ€é‡‡æ ·é…ç½®è¯»å–"""
    return bool(config.get('features', {}).get('dynamic_sampling', {}).get('enabled', False))

def create_environment_runner(config: Dict[str, Any], policy):
    """åˆ›å»ºç¯å¢ƒrunnerï¼ˆRIPT-VLAé£æ ¼é€‰æ‹©ï¼‰"""
    use_ript_vla = config.get('features', {}).get('use_ript_vla_runner', False)
    
    print(f"ğŸ” Runneré€‰æ‹©: use_ript_vla_runner = {use_ript_vla}")
    
    if use_ript_vla and RIPT_VLA_RUNNER_AVAILABLE:
        print("ğŸš€ ä½¿ç”¨RIPT-VLAé£æ ¼ç¯å¢ƒrunner")
        
        runner = PI0LiberoRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            rank=0
        )
        
    elif ORIGINAL_RUNNER_AVAILABLE:
        print("ğŸ”„ ä½¿ç”¨åŸæœ‰ç¯å¢ƒrunner")
        
        # ç¡®ä¿norm_stats_pathå­˜åœ¨
        norm_stats_path = config.get('norm_stats_path')
        if not norm_stats_path:
            norm_stats_path = f"{config['policy_path']}/norm_stats.json"
        
        runner = LIBEROEnvRunner(
            policy=policy,
            benchmark_name=config['task']['benchmark_name'],
            rollouts_per_env=config['algo']['rloo_batch_size'],
            num_parallel_envs=config['task']['num_parallel_envs'],
            max_episode_length=config['task']['max_episode_length'],
            task_names_to_use=config['task'].get('task_names_to_use', []),
            norm_stats_path=norm_stats_path,
            config=config,
            rank=0,
            world_size=1
        )
    else:
        raise RuntimeError("âŒ æ²¡æœ‰å¯ç”¨çš„ç¯å¢ƒrunnerï¼")
    
    print("âœ“ ç¯å¢ƒrunneråˆ›å»ºæˆåŠŸ")
    return runner

def _dynamic_filter_rollouts(episodes: List[Dict], enable_dynamic_sampling: bool) -> List[Dict]:
    """æŒ‰RIPT-VLAæ€è·¯çš„æœ€å°åŠ¨æ€é‡‡æ ·ï¼šä¸¢å¼ƒå…¨0æˆ–å…¨1æˆåŠŸç‡çš„æ‰¹æ¬¡"""
    if not enable_dynamic_sampling or not episodes:
        return episodes
    successes = [bool(ep.get('success', False)) for ep in episodes]
    if len(successes) > 0 and (all(successes) or not any(successes)):
        print(f"âš ï¸ åŠ¨æ€é‡‡æ ·ä¸¢å¼ƒæœ¬æ‰¹æ¬¡ (uniform successes: {successes})")
        return []
    return episodes


class DemoStateSampler:
    """RIPTå¯¹é½çš„DemoçŠ¶æ€é‡‡æ ·å™¨ - åŒdemoç”¨åŒä¸€çŠ¶æ€ï¼Œä¸åŒdemoè½®æ¢çŠ¶æ€"""
    
    def __init__(self):
        self.demo_to_state_cache = {}  # ç¼“å­˜ï¼šdemo_id -> é€‰å®šçš„çŠ¶æ€ç´¢å¼•
        self.next_state_idx = 0  # ä¸‹ä¸€ä¸ªæ–°demoä½¿ç”¨çš„çŠ¶æ€ç´¢å¼•
        
    def get_next_init_state(self, demo_initial_state):
        """
        ä»demoä¸­è·å–ä¸‹ä¸€ä¸ªåˆå§‹çŠ¶æ€ï¼ˆæŒ‰é¡ºåºè½®æ¢ï¼‰
        
        Args:
            demo_initial_state: LIBERO demoæ•°æ®
            
        Returns:
            tuple: (selected_state_numpy, init_state_hash, state_description)
        """
        if demo_initial_state is None:
            return None, None, "æ— demoæ•°æ®"
            
        if 'init_state' not in demo_initial_state or demo_initial_state['init_state'] is None:
            return demo_initial_state['initial_obs'], "obs_fallback", "ä½¿ç”¨è§‚æµ‹æ•°æ®ï¼ˆæ— MuJoCoçŠ¶æ€ï¼‰"
            
        # ä½¿ç”¨demoä¸­çš„MuJoCoçŠ¶æ€
        init_state_data = demo_initial_state['init_state']
        # ğŸ”¥ ä¿®å¤ï¼šcollateåæ˜¯[B,T,92]æ ¼å¼ï¼Œéœ€è¦å–å•æ¡[T,92]
        states = init_state_data['states'][0]  # [T, 92]
        pad_mask = init_state_data['pad_mask'][0]  # [T]
        
        # è·å–æ‰€æœ‰æœ‰æ•ˆçŠ¶æ€ç´¢å¼•
        valid_indices = torch.where(pad_mask)[0]
        if len(valid_indices) == 0:
            return demo_initial_state['initial_obs'], "obs_fallback", "demoçŠ¶æ€æ— æœ‰æ•ˆæ•°æ®ï¼Œå›é€€åˆ°è§‚æµ‹"
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒdemoç”¨åŒä¸€çŠ¶æ€ï¼Œä¸åŒdemoè½®æ¢çŠ¶æ€
        demo_id = demo_initial_state['task_id'][0].item()
        
        if demo_id not in self.demo_to_state_cache:
            # æ–°demoï¼šåˆ†é…ä¸‹ä¸€ä¸ªçŠ¶æ€ç´¢å¼•
            current_valid_idx = self.next_state_idx % len(valid_indices)
            self.demo_to_state_cache[demo_id] = current_valid_idx
            self.next_state_idx += 1
            print(f"  ğŸ¯ æ–°demo {demo_id}: åˆ†é…çŠ¶æ€ç´¢å¼• {current_valid_idx}")
        else:
            # å·²çŸ¥demoï¼šå¤ç”¨ä¹‹å‰åˆ†é…çš„çŠ¶æ€ç´¢å¼•
            current_valid_idx = self.demo_to_state_cache[demo_id]
            print(f"  ğŸ¯ å¤ç”¨demo {demo_id}: çŠ¶æ€ç´¢å¼• {current_valid_idx}")
        
        selected_state_idx = valid_indices[current_valid_idx]
        selected_state = states[selected_state_idx]
        
        # ğŸ”¥ ç¡®ä¿è¿”å›float64æ ¼å¼çš„contiguousæ•°ç»„
        selected_state_numpy = np.ascontiguousarray(selected_state.numpy(), dtype=np.float64)
        
        # ğŸ”¥ ä½¿ç”¨ç¨³å®šbyteså“ˆå¸Œæ›¿ä»£å­—ç¬¦ä¸²å“ˆå¸Œ
        state_bytes = np.ascontiguousarray(selected_state_numpy, dtype=np.float64).tobytes()
        state_hash = hashlib.sha256(state_bytes).hexdigest()[:16]  # å–å‰16ä½
        
        state_desc = f"Demo {demo_id} çŠ¶æ€ {current_valid_idx+1}/{len(valid_indices)} (ç´¢å¼•: {selected_state_idx.item()})"
        
        print(f"  ğŸ¯ è½®æ¢é€‰æ‹©: {state_desc}")
        
        return selected_state_numpy, state_hash, state_desc


# å…¨å±€çŠ¶æ€é‡‡æ ·å™¨å®ä¾‹
global_demo_sampler = DemoStateSampler()


def collect_rollouts_ript_vla_style(env_runner, task_name, num_rollouts, enable_dynamic_sampling: bool = False, stats_tracker: Optional[RolloutStatsTracker] = None, demo_initial_state=None):
    """
    RIPT-VLAé£æ ¼çš„rolloutæ”¶é›†ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒper-initè·³è¿‡å’Œdemoåˆå§‹çŠ¶æ€ï¼‰

    Args:
        demo_initial_state: æ¥è‡ªLIBEROæ•°æ®é›†çš„demoåˆå§‹çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
    """
    print(f"æ­£åœ¨æ”¶é›† {num_rollouts} ä¸ªrollouts...")

    try:
        # ğŸ”¥ å¤„ç†demoåˆå§‹çŠ¶æ€ï¼ˆRIPTå¯¹é½ + çŠ¶æ€è½®æ¢ï¼‰
        state_hash = None  # ç”¨äºç»Ÿè®¡è·Ÿè¸ª
        if demo_initial_state is not None:
            print(f"  ğŸ“‹ ä½¿ç”¨demoåˆå§‹çŠ¶æ€: ä»»åŠ¡ {demo_initial_state['task_name'][0]}")
            task_id = demo_initial_state['task_id'][0].item()

            # ğŸ”¥ ä½¿ç”¨çŠ¶æ€é‡‡æ ·å™¨è¿›è¡Œæœ‰åºè½®æ¢
            selected_state, _, state_desc = global_demo_sampler.get_next_init_state(demo_initial_state)
            if selected_state is not None:
                all_init_states = [selected_state]
                # ğŸ”¥ ä½¿ç”¨å†…å®¹å“ˆå¸Œæ›¿ä»£å­—ç¬¦ä¸²å“ˆå¸Œ
                if stats_tracker is not None:
                    state_hash = stats_tracker._compute_init_hash(task_id, selected_state)
                print(f"  âœ… {state_desc}")
            else:
                all_init_states = None
                print(f"  âš ï¸ çŠ¶æ€é‡‡æ ·å¤±è´¥ï¼Œå°†ä½¿ç”¨ç¯å¢ƒé»˜è®¤åˆå§‹åŒ–")
        else:
            # è·å–ä»»åŠ¡çš„åˆå§‹çŠ¶æ€å’Œtask_id
            task_id = 0  # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
            if hasattr(env_runner, 'benchmark'):
                all_init_states = env_runner.benchmark.get_task_init_states(task_id)
            else:
                all_init_states = None
        
        # ğŸ”¥ å¦‚æœæœ‰ç»Ÿè®¡è·Ÿè¸ªå™¨ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡è¿™ä¸ªä»»åŠ¡
        if stats_tracker and all_init_states is not None:
            # ğŸ”¥ RIPTå¯¹é½ï¼šä½¿ç”¨ç²¾ç¡®çš„çŠ¶æ€å“ˆå¸Œï¼Œè€Œä¸æ˜¯éšæœºé€‰æ‹©
            if state_hash is not None:
                # ä½¿ç”¨ä»demoé‡‡æ ·å™¨è·å–çš„ç²¾ç¡®çŠ¶æ€å“ˆå¸Œ
                init_hash = state_hash
            else:
                # å›é€€åˆ°ç¬¬ä¸€ä¸ªçŠ¶æ€ï¼ˆé¿å…éšæœºæ€§ï¼‰
                sample_init_state = all_init_states[0]
                init_hash = stats_tracker._compute_init_hash(task_id, sample_init_state)
            
            if stats_tracker.should_skip_init(task_id, init_hash, num_rollouts):
                stats_tracker.increment_skip_count(task_id, init_hash)
                print(f"ğŸš« è·³è¿‡æ­¤æ¬¡æ”¶é›†ï¼šinit ({task_id}, {init_hash}) æœ€è¿‘å…¨æˆåŠŸ")
                return []
        
        # ç›´æ¥è°ƒç”¨ç¯å¢ƒrunnerçš„æ–¹æ³•
        rollout_generator = env_runner.run_policy_in_env(
            env_name=task_name,
            all_init_states=all_init_states
        )
        
        # æ”¶é›†æ‰€æœ‰rollouts
        collected_rollouts = []
        rollout_count = 0
        
        for success, total_reward, episode_data in rollout_generator:
            episode = {
                'success': success,
                'total_reward': total_reward,
                **episode_data
            }
            
            # ğŸ”¥ æ·»åŠ init_hashä¿¡æ¯ï¼ˆç»Ÿä¸€ä½¿ç”¨å†…å®¹å“ˆå¸Œï¼‰
            if stats_tracker and state_hash is not None:
                episode['init_hash'] = state_hash
            elif stats_tracker and 'init_state' in episode_data:
                # ä»episode_dataä¸­è®¡ç®—å†…å®¹å“ˆå¸Œ
                episode['init_hash'] = stats_tracker._compute_init_hash(task_id, episode_data['init_state'])
            
            collected_rollouts.append(episode)
            rollout_count += 1
            
            if rollout_count >= num_rollouts:
                break
        
        # ğŸ”¥ æ›´æ–°ç»Ÿè®¡è·Ÿè¸ªå™¨
        if stats_tracker and collected_rollouts:
            # æå–æˆåŠŸç‡ä¿¡æ¯
            successes = [ep.get('success', False) for ep in collected_rollouts]
            
            # è·å–init_hashï¼ˆç»Ÿä¸€ä»episodeä¸­è¯»å–ï¼‰
            init_hash = None
            for ep in collected_rollouts:
                if 'init_hash' in ep:
                    init_hash = ep['init_hash']
                    break
            
            if init_hash:
                stats_tracker.update_stats(task_id, init_hash, successes)
        
        # æœ€å°åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼šä¸¢å¼ƒå…¨0æˆ–å…¨1æ‰¹æ¬¡
        filtered = _dynamic_filter_rollouts(collected_rollouts, enable_dynamic_sampling)
        if not filtered:
            print("âš ï¸ æœ¬æ‰¹æ¬¡è¢«åŠ¨æ€é‡‡æ ·è¿‡æ»¤ï¼Œè¿”å›ç©ºé›†")
        else:
            print(f"âœ“ æˆåŠŸæ”¶é›†äº† {len(filtered)} ä¸ªrollouts (è¿‡æ»¤å)")
        return filtered
        
    except Exception as e:
        print(f"âŒ Rolloutæ”¶é›†å¤±è´¥: {e}")
        traceback.print_exc()
        return []

def compute_advantages_rloo(episodes: List[Dict], rloo_batch_size: int = None) -> torch.Tensor:
    """
    æ­£å®—çš„RLOO (Reward Ranked Leave-One-Out) ä¼˜åŠ¿è®¡ç®—
    
    Args:
        episodes: æ”¶é›†çš„episodesåˆ—è¡¨
        rloo_batch_size: RLOOæ‰¹æ¬¡å¤§å°ï¼Œç”¨äºLeave-One-Outè®¡ç®—
    
    Returns:
        torch.Tensor: è®¡ç®—å¾—åˆ°çš„ä¼˜åŠ¿å€¼
    """
    if not episodes:
        return torch.tensor([])
    
    # æå–å¥–åŠ±
    rewards = []
    for ep in episodes:
        reward = ep.get('total_reward', 0.0)
        rewards.append(float(reward))
    
    rlhf_reward = torch.tensor(rewards, dtype=torch.float32)
    num_rollouts = len(episodes)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨çœŸæ­£çš„RLOOæ‰¹æ¬¡å¤§å°è€Œä¸æ˜¯æ€»æ•°
    if rloo_batch_size is None or rloo_batch_size <= 1:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæˆ–batch sizeè¿‡å°ï¼Œé€€åŒ–ä¸ºç®€å•æ–¹æ³•
        print("âš ï¸ RLOO batch sizeæœªæŒ‡å®šæˆ–è¿‡å°ï¼Œä½¿ç”¨ç®€å•ä¼˜åŠ¿è®¡ç®—")
        advantage = rlhf_reward - rlhf_reward.mean()
    else:
        # ğŸš€ æ­£å®—RLOOè®¡ç®—
        try:
            # ç¡®ä¿å¯ä»¥æ•´é™¤ï¼Œå¦‚æœä¸èƒ½æ•´é™¤åˆ™è£å‰ªåˆ°æœ€å¤§å¯æ•´é™¤æ•°é‡
            effective_rollouts = (num_rollouts // rloo_batch_size) * rloo_batch_size
            if effective_rollouts != num_rollouts:
                print(f"ğŸ”§ RLOOè°ƒæ•´ï¼š{num_rollouts} â†’ {effective_rollouts} rollouts (batch_size={rloo_batch_size})")
                rlhf_reward = rlhf_reward[:effective_rollouts]
                num_rollouts = effective_rollouts
            
            num_batches = num_rollouts // rloo_batch_size
            rlhf_reward_reshaped = rlhf_reward.reshape(num_batches, rloo_batch_size)
            
            # æ ‡å‡†RLOOï¼šæ¯ä¸ªæ ·æœ¬çš„baseline = åŒæ‰¹æ¬¡å…¶ä»–æ ·æœ¬çš„å¹³å‡å€¼
            # baseline[i,j] = (sum(batch[i]) - reward[i,j]) / (batch_size - 1)
            batch_sums = rlhf_reward_reshaped.sum(dim=1, keepdim=True)  # (num_batches, 1)
            baseline = (batch_sums - rlhf_reward_reshaped) / (rloo_batch_size - 1)  # (num_batches, rloo_batch_size)
            
            # ä¼˜åŠ¿ = è‡ªå·±çš„å¥–åŠ± - å…¶ä»–äººçš„å¹³å‡å¥–åŠ±
            advantage = rlhf_reward_reshaped - baseline  # (num_batches, rloo_batch_size)
            advantage = advantage.flatten()  # å±•å¹³ä¸ºä¸€ç»´
            
            # NaNå’ŒInfæ£€æŸ¥
            if torch.isnan(advantage).any() or torch.isinf(advantage).any():
                print("âš ï¸ RLOOè®¡ç®—äº§ç”ŸNaN/Infï¼Œä½¿ç”¨å®‰å…¨æ›¿æ¢")
                advantage = torch.nan_to_num(advantage, nan=0.0, posinf=1.0, neginf=-1.0)
            
            print(f"ğŸ¯ æ­£å®—RLOOä¼˜åŠ¿è®¡ç®—å®Œæˆ:")
            print(f"   æ‰¹æ¬¡é…ç½®: {num_rollouts} rollouts â†’ {num_batches} batches Ã— {rloo_batch_size}")
            print(f"   ä¼˜åŠ¿ç»Ÿè®¡: mean={advantage.mean():.4f}, std={advantage.std():.4f}")
            print(f"   æ­£ä¼˜åŠ¿æ¯”ä¾‹: {(advantage > 0).float().mean():.2%}")
            
        except Exception as e:
            print(f"âŒ RLOOè®¡ç®—å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•")
            advantage = rlhf_reward - rlhf_reward.mean()
    
    return advantage

def update_policy_ript_vla_style(policy, optimizer, cfg_adapter, episodes, advantages, device, config=None):
    """
    RIPT-VLAé£æ ¼çš„ç­–ç•¥æ›´æ–°ï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
    """
    if not episodes or len(advantages) == 0:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œç­–ç•¥æ›´æ–°")
        return 0.0
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢¯åº¦ç´¯ç§¯
    gradient_accumulation_steps = 1
    if config:
        gradient_accumulation_steps = config.get('algo', {}).get('gradient_accumulation_steps', 1)
    
    if gradient_accumulation_steps > 1:
        return update_policy_with_gradient_accumulation(policy, optimizer, cfg_adapter, episodes, advantages, device, gradient_accumulation_steps, config)
    else:
        return update_policy_simple(policy, optimizer, cfg_adapter, episodes, advantages, device, config)

def update_policy_with_gradient_accumulation(policy, optimizer, cfg_adapter, episodes, advantages, device, gradient_accumulation_steps, config=None):
    """
    æ¢¯åº¦ç´¯ç§¯ç‰ˆæœ¬çš„ç­–ç•¥æ›´æ–°ï¼ˆAMPå¢å¼º + çª—å£çº§å¾®æ‰¹å¤„ç†ï¼‰
    """
    total_episodes = len(episodes)

    print(f"ğŸ”§ çª—å£çº§å¾®æ‰¹æ¢¯åº¦ç´¯ç§¯:")
    print(f"   æ€»episodes: {total_episodes}")
    print(f"   ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")

    # ğŸ”¥ Phase 3: æ•°æ®åˆ©ç”¨ç‡ç›‘æ§
    if hasattr(cfg_adapter, 'use_so100_processing') and cfg_adapter.use_so100_processing:
        # ä¼°ç®—è®­ç»ƒæ ·æœ¬æ•°é‡ (åŸºäºå¹³å‡è½¨è¿¹é•¿åº¦)
        avg_episode_length = sum(len(ep.get('actions', [])) for ep in episodes) / len(episodes)
        estimated_samples = max(0, avg_episode_length - 50 + 1) * len(episodes)
        utilization_ratio = estimated_samples / len(episodes) if len(episodes) > 0 else 0
        print(f"ğŸ“Š SO100æ•°æ®åˆ©ç”¨ç‡: {len(episodes)} episodes â†’ ~{estimated_samples:.0f} samples ({utilization_ratio:.1f}x)")

    
    policy.train()
    
    # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨AMPçš„GradScalerï¼ˆæ–°ç‰ˆæœ¬APIï¼‰
    try:
        scaler = torch.amp.GradScaler('cuda')  # æ–°ç‰ˆæœ¬API
    except AttributeError:
        scaler = torch.cuda.amp.GradScaler()  # æ—§ç‰ˆæœ¬å…¼å®¹
    
    # ä½¿ç”¨æ ‡å‡†æ¢¯åº¦ç´¯ç§¯æ–¹æ³•
    return update_policy_with_gradient_accumulation_fallback(
        policy, optimizer, cfg_adapter, episodes, advantages, device, gradient_accumulation_steps, scaler, config=config
    )

def update_policy_with_gradient_accumulation_fallback(policy, optimizer, cfg_adapter, episodes, advantages, device, gradient_accumulation_steps, scaler, config=None):
    """
    å›é€€ç‰ˆæœ¬çš„æ¢¯åº¦ç´¯ç§¯ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    """
    total_episodes = len(episodes)
    mini_batch_size = max(1, total_episodes // gradient_accumulation_steps)
    
    # ğŸš€ ç»Ÿä¸€æ ·æœ¬æ± è®­ç»ƒï¼ˆæ— mini-batchåˆ†å‰²ï¼‰
    print(f"ğŸ”§ ç»Ÿä¸€æ ·æœ¬æ± è®­ç»ƒ:")
    print(f"   æ€»episodes: {total_episodes}")
    print(f"   ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    
    total_loss = 0.0
    
    # ğŸ”¥ ç›´æ¥ä½¿ç”¨æ‰€æœ‰episodesï¼Œä¸å†åˆ†å‰²mini-batch
    try:
        # ğŸš€ ä½¿ç”¨autocaståŒ…è£¹forwardè®¡ç®—
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            advantages = advantages.to(device)
            
            # ğŸš€ RIPTå¯¹é½ï¼šå¼ºåˆ¶ä½¿ç”¨SO100å¤„ç†ï¼Œä¸å…è®¸å›é€€
            if not (hasattr(cfg_adapter, 'use_so100_processing') and cfg_adapter.use_so100_processing):
                raise RuntimeError("âŒ RIPTå¯¹é½è¦æ±‚ï¼šå¿…é¡»å¯ç”¨use_so100_processingé…ç½®")
            
            print("ğŸš€ ä½¿ç”¨SO100ç»Ÿä¸€æ ·æœ¬æ± è®­ç»ƒï¼ˆRIPTå¯¹é½æ¨¡å¼ï¼‰...")
            # ä»configè¯»å–å‚æ•°ï¼Œæä¾›åˆç†é»˜è®¤å€¼
            batch_size_cfg = config.get('unified_pool_batch_size', 8) if config else 8
            shuffle_cfg = config.get('unified_pool_shuffle', True) if config else True
            print(f"  é…ç½®å‚æ•°: batch_size={batch_size_cfg}, shuffle={shuffle_cfg}")
            
            loss = cfg_adapter.compute_weighted_loss_unified(
                episodes=episodes,  # ğŸ”¥ ä½¿ç”¨æ‰€æœ‰episodes
                advantages=advantages,  # ğŸ”¥ ä½¿ç”¨æ‰€æœ‰advantages
                device=device,
                batch_size=batch_size_cfg,
                shuffle_samples=shuffle_cfg,
                scaler=scaler,
                optimizer=optimizer,
                gradient_accumulation_steps=gradient_accumulation_steps
            )
            
            total_loss = loss.item()
            print(f"âœ… SO100ç»Ÿä¸€æ ·æœ¬æ± è®­ç»ƒå®Œæˆï¼Œæ€»æŸå¤±: {total_loss}")
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        total_loss = 0.0
    
    print(f"âœ“ ç»Ÿä¸€æ ·æœ¬æ± è®­ç»ƒå®Œæˆï¼Œæ€»æŸå¤±: {total_loss:.6f}")
    return total_loss

def update_policy_simple(policy, optimizer, cfg_adapter, episodes, advantages, device, config=None):
    """ç®€å•ç‰ˆæœ¬çš„ç­–ç•¥æ›´æ–°ï¼ˆæ— æ¢¯åº¦ç´¯ç§¯ï¼‰"""
    print(f"æ­£åœ¨æ›´æ–°ç­–ç•¥ï¼ˆ{len(episodes)} ä¸ªepisodesï¼‰...")

    # ğŸ”¥ Phase 3: æ•°æ®åˆ©ç”¨ç‡ç›‘æ§
    if hasattr(cfg_adapter, 'use_so100_processing') and cfg_adapter.use_so100_processing:
        # ä¼°ç®—è®­ç»ƒæ ·æœ¬æ•°é‡ (åŸºäºå¹³å‡è½¨è¿¹é•¿åº¦)
        avg_episode_length = sum(len(ep.get('actions', [])) for ep in episodes) / len(episodes)
        estimated_samples = max(0, avg_episode_length - 50 + 1) * len(episodes)
        utilization_ratio = estimated_samples / len(episodes) if len(episodes) > 0 else 0
        print(f"ğŸ“Š SO100æ•°æ®åˆ©ç”¨ç‡: {len(episodes)} episodes â†’ ~{estimated_samples:.0f} samples ({utilization_ratio:.1f}x)")

    try:
        # è®¡ç®—åŠ æƒæŸå¤±
        advantages = advantages.to(device)

        # ğŸš€ RIPTå¯¹é½ï¼šå¼ºåˆ¶ä½¿ç”¨SO100å¤„ç†ï¼Œä¸å…è®¸å›é€€
        if not (hasattr(cfg_adapter, 'use_so100_processing') and cfg_adapter.use_so100_processing):
            raise RuntimeError("âŒ RIPTå¯¹é½è¦æ±‚ï¼šå¿…é¡»å¯ç”¨use_so100_processingé…ç½®")
            
        print("ğŸš€ ä½¿ç”¨SO100ç»Ÿä¸€æ ·æœ¬æ± è®­ç»ƒï¼ˆRIPTå¯¹é½æ¨¡å¼ï¼‰...")
        # ä»é…ç½®è¯»å–å¯è°ƒå‚æ•°ï¼Œæä¾›åˆç†é»˜è®¤å€¼
        batch_size_cfg = config.get('unified_pool_batch_size', 8) if config else 8
        shuffle_cfg = config.get('unified_pool_shuffle', True) if config else True
        
        print(f"  é…ç½®å‚æ•°: batch_size={batch_size_cfg}, shuffle={shuffle_cfg}")

        loss = cfg_adapter.compute_weighted_loss_unified(
            episodes=episodes,
            advantages=advantages,
            device=device,
            batch_size=batch_size_cfg,
            shuffle_samples=shuffle_cfg
        )

        # æ¢¯åº¦æ›´æ–°
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)

        optimizer.step()

        loss_value = loss.item()
        print(f"âœ“ ç­–ç•¥æ›´æ–°å®Œæˆï¼ŒæŸå¤±: {loss_value:.6f}")

        return loss_value
        
    except Exception as e:
        print(f"âŒ ç­–ç•¥æ›´æ–°å¤±è´¥: {e}")
        traceback.print_exc()
        return 0.0

def evaluate_with_cfg_sweep(policy, env_runner, task_name, eval_episodes=3):
    """ğŸ”¥ æ–°å¢ï¼šè¯„ä¼°ä¸åŒCFGå¼ºåº¦çš„æ•ˆæœ"""
    cfg_scales = [1.0, 1.5, 3.0, 5.0]
    best_cfg = 1.0
    best_success_rate = 0.0
    
    results = {}
    print(f"\nğŸ” å¼€å§‹CFGå¼ºåº¦æ‰«æè¯„ä¼°...")
    
    for cfg_scale in cfg_scales:
        print(f"ğŸ“Š æµ‹è¯•CFG={cfg_scale}...")
        # ğŸ”¥ ç»Ÿä¸€ä»config.algo.collection_cfg_scaleè¯»å–
        if hasattr(env_runner.config, 'algo') and hasattr(env_runner.config.algo, 'collection_cfg_scale'):
            original_cfg = env_runner.config.algo.collection_cfg_scale
        elif isinstance(env_runner.config, dict) and 'algo' in env_runner.config:
            original_cfg = env_runner.config['algo'].get('collection_cfg_scale', 1.5)
        else:
            print("âš ï¸ CFGæ‰«æï¼šæœªæ‰¾åˆ°algo.collection_cfg_scaleé…ç½®ï¼Œä½¿ç”¨1.5")
            original_cfg = 1.5
        
        # ğŸ”¥ åŒé‡å†™å…¥ï¼šå¯¹è±¡å¼+å­—å…¸å¼ç¡®ä¿å…¼å®¹æ€§
        if hasattr(env_runner.config, 'algo'):
            env_runner.config.algo.collection_cfg_scale = cfg_scale
        if isinstance(env_runner.config, dict) and 'algo' in env_runner.config:
            env_runner.config['algo']['collection_cfg_scale'] = cfg_scale
        
        # è¿è¡Œè¯„ä¼°episodes
        success_count = 0
        for ep_idx in range(eval_episodes):
            try:
                # ä½¿ç”¨ç°æœ‰çš„rolloutæ”¶é›†å‡½æ•°
                episodes = collect_rollouts_ript_vla_style(
                    env_runner, task_name, 1, enable_dynamic_sampling=False
                )
                if episodes and len(episodes) > 0:
                    if episodes[0].get('success', False):
                        success_count += 1
            except Exception as e:
                print(f"   è¯„ä¼°episode {ep_idx} å¤±è´¥: {e}")
                continue
        
        success_rate = success_count / eval_episodes
        results[cfg_scale] = success_rate
        
        if success_rate > best_success_rate:
            best_success_rate = success_rate
            best_cfg = cfg_scale
        
        # ğŸ”¥ åŒé‡æ¢å¤ï¼šå¯¹è±¡å¼+å­—å…¸å¼
        if hasattr(env_runner.config, 'algo'):
            env_runner.config.algo.collection_cfg_scale = original_cfg
        if isinstance(env_runner.config, dict) and 'algo' in env_runner.config:
            env_runner.config['algo']['collection_cfg_scale'] = original_cfg
        
        print(f"   CFG={cfg_scale}: æˆåŠŸç‡={success_rate:.2%} ({success_count}/{eval_episodes})")
    
    print(f"ğŸ¯ æœ€ä½³CFGå¼ºåº¦: {best_cfg} (æˆåŠŸç‡: {best_success_rate:.2%})")
    return best_cfg, results

def main_training_loop_ript_vla_style(config: Dict[str, Any]):
    """
    ä¸»è®­ç»ƒå¾ªç¯ï¼ˆRIPT-VLAé£æ ¼ï¼‰
    ç›´æ¥åœ¨ä¸»å‡½æ•°ä¸­å¤„ç†æ‰€æœ‰é€»è¾‘ï¼Œå‡å°‘æŠ½è±¡å±‚
    """
    print("ğŸš€ å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒå¾ªç¯")
    
    # ğŸ”¥ è®¾ç½®æ•°å€¼ä¼˜åŒ–å’Œæ˜¾å­˜ç®¡ç†
    print("ğŸ”§ è®¾ç½®æ•°å€¼ä¼˜åŒ–...")
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.set_float32_matmul_precision('high')
    torch.cuda.empty_cache()
    print("âœ… TF32å’Œæ˜¾å­˜ä¼˜åŒ–å·²å¯ç”¨")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(config['output_dir'])
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_name = config.get('exp_name', 'ript_vla_style_train')
    output_dir = output_dir / f"{exp_name}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨
    policy, optimizer, device = create_policy_and_optimizer(config)
    
    # åˆ›å»ºCFGé€‚é…å™¨ï¼ˆå¿…éœ€ï¼Œç”¨äºæŸå¤±è®¡ç®—ï¼‰
    # ğŸ”¥ Phase 3: æ•°æ®å¤„ç†é…ç½® (Legacy + SO100)
    dataset_config = config.get('dataset', {})
    data_processing_config = config.get('data_processing', {})
    policy_config = config.get('policy', {})

    # CFGçŠ¶æ€æ£€æŸ¥
    cfg_enabled = getattr(policy.model, 'cfg_enabled', True)

    # SO100å¤„ç†é…ç½® (Phase 3æ–°å¢) - ä¿®å¤ï¼šä»æ­£ç¡®çš„é…ç½®è·¯å¾„è¯»å–
    use_so100_processing = data_processing_config.get('use_so100_processing', False)

    # Legacyçª—å£åŒ–é…ç½® (å‘åå…¼å®¹) - ä¿®å¤ï¼šä»æ­£ç¡®çš„é…ç½®è·¯å¾„è¯»å–
    windowing_mode = data_processing_config.get('windowing_mode', 'last')
    window_stride = data_processing_config.get('window_stride', 10)
    max_windows_per_episode = data_processing_config.get('max_windows_per_episode', 1)

    print(f"\nğŸ”§ è®­ç»ƒé…ç½®:")
    print(f"  CFGæ¨¡å¼: {'å¯ç”¨' if cfg_enabled else 'ç¦ç”¨'}")
    print(f"  SO100å¤„ç†: {'å¯ç”¨' if use_so100_processing else 'ç¦ç”¨ (ä½¿ç”¨Legacyçª—å£åŒ–)'}")
    if not use_so100_processing:
        print(f"  çª—å£åŒ–æ¨¡å¼: {windowing_mode}")
        print(f"  çª—å£æ­¥é•¿: {window_stride}")
        print(f"  æ¯episodeæœ€å¤§çª—å£æ•°: {max_windows_per_episode}")
    else:
        print(f"  æ•°æ®åˆ©ç”¨ç‡: é¢„æœŸ50-150xæå‡")
        print(f"  æ ·æœ¬ç”Ÿæˆ: æ¯ä¸ªè½¨è¿¹ç”ŸæˆL-50+1ä¸ªè®­ç»ƒæ ·æœ¬")

    cfg_adapter = PI0_CFG_Adapter(
        policy=policy,
        norm_stats_path=f"{config['policy_path']}/norm_stats.json",
        use_so100_processing=use_so100_processing,  # ğŸ”¥ Phase 3: æ–°å¢SO100æ”¯æŒ
        windowing_mode=windowing_mode,
        window_stride=window_stride,
        max_windows_per_episode=max_windows_per_episode
    )
    
    # åˆ›å»ºç¯å¢ƒrunner
    env_runner = create_environment_runner(config, policy)
    
    # ğŸ”¥ åˆ›å»ºrolloutç»Ÿè®¡è·Ÿè¸ªå™¨ï¼ˆæŒ‰é…ç½®å¼€å…³ï¼‰
    enable_rollout_stats = config['algo'].get('enable_rollout_stats_tracking', False)
    if enable_rollout_stats:
        stats_path = config['algo'].get('rollout_stats_path', './output/stage11_ript_vla/rollout_stats.json')
        rollout_skip_threshold = config['algo'].get('rollout_skip_threshold', 3)
        stats_tracker = RolloutStatsTracker(
            rollout_skip_threshold=rollout_skip_threshold,
            stats_path=stats_path
        )
    else:
        stats_tracker = None
    
    # ğŸ”¥ è§£è€¦demo_batch_sizeä¸rloo_batch_size
    demo_batch_size = config['algo'].get('demo_batch_size', 6)  # æ”¹ä¸ºé»˜è®¤6ï¼Œä¸åŸç‰ˆRIPTä¸€è‡´
    rloo_batch_size = config['algo']['rloo_batch_size']
    num_train_steps = config['training']['num_train_steps']
    task_names = config['task'].get('task_names_to_use', ['LIBERO_SPATIAL_0'])

    # ğŸ”¥ åˆ›å»ºRIPTå¯¹é½çš„LIBERO demoæ•°æ®åŠ è½½å™¨
    use_libero_demos = config.get('use_libero_demos', True)
    if use_libero_demos:
        try:
            # ä»é…ç½®ä¸­è·å–æ•°æ®è·¯å¾„
            libero_data_prefix = config.get('libero_data_prefix', '/zhaohan/ZJH/openpi_pytorch/datasets')
            benchmark_name = config.get('benchmark_name', 'libero_spatial')  # ğŸ”¥ ä½¿ç”¨å°å†™æ ¼å¼

            # ğŸ”¥ ä½¿ç”¨RIPTå¯¹é½çš„æ•°æ®é›†ï¼ˆåŒ…å«MuJoCoçŠ¶æ€ï¼‰
            dataset = build_dataset_ript_aligned(
                data_prefix=libero_data_prefix,
                suite_name="libero",
                benchmark_name=benchmark_name,
                task_names_to_use=task_names if task_names != ['LIBERO_SPATIAL_0'] else None,
                load_state=True,  # ğŸ”¥ å…³é”®ï¼šåŠ è½½MuJoCoçŠ¶æ€
                seq_len=600,
                n_demos=50
            )

            # ğŸ”¥ ä½¿ç”¨RIPTå¯¹é½çš„collateå‡½æ•°
            from torch.utils.data import DataLoader
            demo_dataloader = DataLoader(
                dataset,
                batch_size=demo_batch_size,
                shuffle=True,
                collate_fn=collate_fn_ript_aligned,  # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨RIPTå¯¹é½çš„collate
                num_workers=0
            )

            demo_data_iter = iter(demo_dataloader)
            print(f"âœ… RIPTå¯¹é½demoæ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
            print(f"  æ•°æ®è·¯å¾„: {libero_data_prefix}")
            print(f"  åŸºå‡†: {benchmark_name}")
            print(f"  æ•°æ®é›†å¤§å°: {len(dataset)}")
            print(f"  ğŸ”¥ åŒ…å«MuJoCoçŠ¶æ€: True")
        except Exception as e:
            print(f"âš ï¸ RIPTå¯¹é½demoåŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
            print("  å°†ä½¿ç”¨ä¼ ç»Ÿçš„ç¯å¢ƒé‡ç½®æ–¹å¼")
            demo_dataloader = None
            demo_data_iter = None
    else:
        demo_dataloader = None
        demo_data_iter = None
    
    print(f"\nğŸ”§ æ‰¹æ¬¡é…ç½®:")
    print(f"  demo_batch_size: {demo_batch_size} (æ¯æ­¥æ”¶é›†çš„ç»„æ•°)")
    print(f"  rloo_batch_size: {rloo_batch_size} (æ¯ç»„å†…æ ·æœ¬æ•°)")
    print(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {demo_batch_size * rloo_batch_size}")
    
    print(f"\nå¼€å§‹è®­ç»ƒå¾ªç¯:")
    print(f"  è®­ç»ƒæ­¥æ•°: {num_train_steps}")
    print(f"  ä»»åŠ¡: {task_names}")
    print(f"  ä½¿ç”¨LIBERO demos: {'æ˜¯' if demo_dataloader else 'å¦'}")
    print()
    
    all_training_metrics = []
    
    # ğŸ”¥ æ˜¾å­˜ç›‘æ§å‡½æ•°
    def print_gpu_memory(step_name: str):
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            max_allocated = torch.cuda.max_memory_allocated() / 1024**3
            print(f"ğŸ“Š {step_name} - GPUæ˜¾å­˜: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, å³°å€¼: {max_allocated:.2f}GB")
    
    # ğŸ”¥ ä¸»è®­ç»ƒå¾ªç¯ - æŒ‰ç»„æ”¶é›†æ¨¡å¼
    for step in range(num_train_steps):
        step_start_time = time.time()
        torch.cuda.reset_peak_memory_stats()  # é‡ç½®å³°å€¼ç›‘æ§
        
        print(f"=== è®­ç»ƒæ­¥éª¤ {step + 1}/{num_train_steps} ===")
        print_gpu_memory("æ­¥éª¤å¼€å§‹")
        
        # 1. æŒ‰ç»„æ”¶é›†rolloutsï¼ˆè§£è€¦demo_batch_sizeä¸rloo_batch_sizeï¼‰
        all_groups = []  # ä¿ç•™åˆ†ç»„ç»“æ„ç”¨äºRLOOè®¡ç®—
        successful_groups = 0
        
        for group_idx in range(demo_batch_size):
            print(f"ğŸ”„ æ”¶é›†ç¬¬ {group_idx + 1}/{demo_batch_size} ç»„...")

            # ğŸ”¥ è·å–demoåˆå§‹çŠ¶æ€ï¼ˆRIPTå¯¹é½è½®æ¢æ¨¡å¼ï¼‰
            demo_batch = None
            if demo_data_iter is not None:
                try:
                    demo_batch = next(demo_data_iter)
                    print(f"  ğŸ“‹ ä½¿ç”¨LIBERO demo: ä»»åŠ¡{demo_batch['task_id'][0].item()}")
                except StopIteration:
                    # ğŸ”¥ RIPTå¯¹é½ï¼šé‡æ–°å¼€å§‹demoè¿­ä»£ï¼ˆç¡®ä¿æ•°æ®å¤šæ ·æ€§ï¼‰
                    demo_data_iter = iter(demo_dataloader)
                    demo_batch = next(demo_data_iter)
                    print(f"  ğŸ“‹ é‡æ–°å¼€å§‹demoè¿­ä»£: ä»»åŠ¡{demo_batch['task_id'][0].item()}")
                    print(f"  ğŸ”„ çŠ¶æ€é‡‡æ ·å™¨ç»§ç»­è½®æ¢ï¼ˆä¸é‡ç½®ï¼‰ï¼Œç¡®ä¿çŠ¶æ€å¤šæ ·æ€§")
                except Exception as e:
                    print(f"  âš ï¸ Demoè·å–å¤±è´¥: {e}")
                    demo_batch = None

            # æ”¶é›†ä¸€ç»„rolloutsï¼ˆä¼ é€’demoåˆå§‹çŠ¶æ€å’Œç»Ÿè®¡è·Ÿè¸ªå™¨ï¼‰
            group_episodes = collect_rollouts_ript_vla_style(
                env_runner, task_names[0] if not demo_batch else demo_batch['task_name'][0],
                rloo_batch_size,
                enable_dynamic_sampling=is_dynamic_sampling_enabled(config),
                stats_tracker=stats_tracker,
                demo_initial_state=demo_batch  # ğŸ”¥ æ–°å¢ï¼šä¼ é€’demoåˆå§‹çŠ¶æ€
            )
            
            if group_episodes:
                successes = [ep.get('success', False) for ep in group_episodes]
                
                # ğŸ”¥ æ£€æŸ¥ç»„çº§åŠ¨æ€é‡‡æ ·ï¼šåªåœ¨å¯ç”¨æ—¶æ‰ä¸¢å¼ƒå…¨0æˆ–å…¨1çš„ç»„
                dynamic_sampling_enabled = is_dynamic_sampling_enabled(config)
                
                if dynamic_sampling_enabled and len(successes) > 0 and (all(successes) or not any(successes)):
                    print(f"âš ï¸ ç»„ {group_idx + 1} è¢«åŠ¨æ€é‡‡æ ·ä¸¢å¼ƒ (uniform successes: {all(successes)})")
                else:
                    all_groups.append(group_episodes)
                    successful_groups += 1
                    
                    if dynamic_sampling_enabled:
                        print(f"âœ… ç»„ {group_idx + 1} æ”¶é›†æˆåŠŸï¼š{len(group_episodes)} episodesï¼Œ"
                              f"æˆåŠŸç‡ {np.mean(successes):.2%}")
                    else:
                        print(f"âœ… ç»„ {group_idx + 1} æ”¶é›†æˆåŠŸï¼š{len(group_episodes)} episodesï¼Œ"
                              f"æˆåŠŸç‡ {np.mean(successes):.2%} (åŠ¨æ€é‡‡æ ·å·²ç¦ç”¨)")
                    
                    # ğŸ”¥ æœ¬åœ°è®¡æ•°ï¼ˆå·²åˆ é™¤ä¸å­˜åœ¨çš„update_episode_counterï¼‰
            else:
                print(f"âŒ ç»„ {group_idx + 1} æ”¶é›†å¤±è´¥")
        
        # ğŸ”¥ å®šæœŸä¿å­˜ç»Ÿè®¡æ•°æ®
        if step % 5 == 0 and stats_tracker is not None:  # æ¯5æ­¥ä¿å­˜ä¸€æ¬¡
            stats_tracker.save_stats()
        
        # è®¡ç®—æ€»episodesæ•°ç”¨äºæ—¥å¿—
        total_episodes = sum(len(g) for g in all_groups)
        print(f"ğŸ“Š ç»„æ”¶é›†å®Œæˆ: {successful_groups}/{demo_batch_size} ç»„æˆåŠŸï¼Œæ€»episodes: {total_episodes}")
        print_gpu_memory("æ”¶é›†å®Œæˆ")
        
        if not all_groups:
            print("âš ï¸ æœªæ”¶é›†åˆ°æœ‰æ•ˆç»„ï¼Œè·³è¿‡æ­¤æ­¥")
            continue
        
        # 2. è®¡ç®—ä¼˜åŠ¿ï¼ˆæŒ‰init_hashåˆ†æ¡¶RLOOæ–¹æ³•ï¼‰
        per_group_advs = []
        all_collected_episodes = []
        
        for group_idx, g in enumerate(all_groups):
            if len(g) != rloo_batch_size:
                print(f"âš ï¸ è·³è¿‡ä¸å®Œæ•´ç»„ {group_idx}: {len(g)}/{rloo_batch_size} episodes")
                continue
            
            # ğŸ”¥ æŒ‰init_hashåˆ†æ¡¶ç¡®ä¿RLOOåœ¨åŒè´¨è½¨è¿¹å†…è®¡ç®—
            from collections import defaultdict
            hash_to_episodes = defaultdict(list)
            
            # æŒ‰init_hashåˆ†ç»„
            for ep in g:
                init_hash = ep.get('init_hash', 'unknown')
                hash_to_episodes[init_hash].append(ep)
            
            # å¯¹æ¯ä¸ªæ¡¶åˆ†åˆ«è®¡ç®—RLOO
            group_advantages = []
            group_episodes = []
            
            for init_hash, bucket_episodes in hash_to_episodes.items():
                if len(bucket_episodes) > 0:
                    # å¦‚æœæ¡¶å†…æ ·æœ¬æ•°å¤Ÿrloo_batch_sizeï¼Œç›´æ¥è®¡ç®—
                    if len(bucket_episodes) >= rloo_batch_size:
                        bucket_advs = compute_advantages_rloo(bucket_episodes[:rloo_batch_size], rloo_batch_size)
                        group_advantages.append(bucket_advs)
                        group_episodes.extend(bucket_episodes[:rloo_batch_size])
                    else:
                        # æ¡¶å†…æ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨ç°æœ‰æ ·æœ¬è®¡ç®—ï¼ˆé™çº§å¤„ç†ï¼‰
                        bucket_advs = compute_advantages_rloo(bucket_episodes, len(bucket_episodes))
                        group_advantages.append(bucket_advs)
                        group_episodes.extend(bucket_episodes)
                        
                    print(f"  ğŸ“¦ åˆ†æ¡¶ {init_hash[:8]}: {len(bucket_episodes)} episodes")
            
            if group_advantages:
                # æ‹¼æ¥åŒç»„å†…å„æ¡¶çš„ä¼˜åŠ¿
                group_concat_advs = torch.cat(group_advantages, dim=0)
                per_group_advs.append(group_concat_advs)
                all_collected_episodes.extend(group_episodes)
                print(f"âœ… ç»„ {group_idx} å®ŒæˆRLOO: {len(hash_to_episodes)} ä¸ªåˆ†æ¡¶")
            else:
                print(f"âš ï¸ ç»„ {group_idx} æ— æœ‰æ•ˆåˆ†æ¡¶")
        
        if not per_group_advs:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„RLOOåˆ†æ¡¶ï¼Œè·³è¿‡æ­¤æ­¥")
            continue
            
        advantages = torch.cat(per_group_advs, dim=0)
        print_gpu_memory("ä¼˜åŠ¿è®¡ç®—å®Œæˆ")
        
        # 3. æ›´æ–°ç­–ç•¥ï¼ˆå¸¦é…ç½®ä¼ é€’ä»¥æ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
        loss = update_policy_ript_vla_style(
            policy, optimizer, cfg_adapter, all_collected_episodes, advantages, device, config
        )
        print_gpu_memory("ç­–ç•¥æ›´æ–°å®Œæˆ")
        
        # 4. è®°å½•æŒ‡æ ‡
        avg_reward = np.mean([ep['total_reward'] for ep in all_collected_episodes])
        success_rate = np.mean([ep['success'] for ep in all_collected_episodes])
        step_time = time.time() - step_start_time
        
        step_metrics = {
            'step': step + 1,
            'demo_groups': successful_groups,
            'total_episodes': len(all_collected_episodes),
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'loss': loss,
            'step_time': step_time
        }
        all_training_metrics.append(step_metrics)
        
        # 5. è¾“å‡ºç»“æœ
        print(f"âœ“ æ­¥éª¤ {step + 1} å®Œæˆ:")
        print(f"  æˆåŠŸç»„æ•°: {successful_groups}/{demo_batch_size}")
        print(f"  æ€»Episodes: {len(all_collected_episodes)}")
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        print(f"  æˆåŠŸç‡: {success_rate:.2%}")
        print(f"  æŸå¤±: {loss:.6f}")
        print(f"  è€—æ—¶: {step_time:.2f}ç§’")
        print_gpu_memory("æ­¥éª¤ç»“æŸ")
        
        # 6. CFGè¯„ä¼°ï¼ˆæ¯10æ­¥è¿›è¡Œä¸€æ¬¡ï¼Œä»…åœ¨CFGå¯ç”¨æ—¶ï¼‰
        if (step + 1) % 10 == 0 and getattr(policy.model, 'cfg_enabled', True):
            try:
                best_cfg, cfg_results = evaluate_with_cfg_sweep(policy, env_runner, task_name, eval_episodes=2)
                step_metrics['best_cfg_scale'] = best_cfg
                step_metrics['cfg_sweep_results'] = cfg_results
                print(f"ğŸ¯ æ¨èCFGå¼ºåº¦: {best_cfg}")
                # å¯é€‰ï¼šåŠ¨æ€è°ƒæ•´æ”¶é›†æ—¶ä½¿ç”¨çš„CFGå¼ºåº¦
                env_runner.config.collection_cfg_scale = best_cfg
            except Exception as e:
                print(f"âš ï¸ CFGè¯„ä¼°å¤±è´¥: {e}")
        elif (step + 1) % 10 == 0:
            print("âš ï¸ CFGå·²ç¦ç”¨ï¼Œè·³è¿‡CFGå¼ºåº¦è¯„ä¼°")
        
        # 7. ä¿å­˜æ£€æŸ¥ç‚¹
        if (step + 1) % config['training'].get('save_freq', 10) == 0:
            # è½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼Œä¾¿äºéƒ¨ç½²ä¸å ç”¨å°ï¼‰
            weights_path = output_dir / f"weights_step_{step + 1}.pt"
            torch.save({
                'step': step + 1,
                'policy_state_dict': policy.state_dict(),
                'config': config,
                'training_metrics': all_training_metrics,
            }, weights_path)
            print(f"âœ“ è½»é‡æƒé‡å·²ä¿å­˜: {weights_path}")

            # å¯é€‰ï¼šæŒ‰è¾ƒä½é¢‘ç‡ä¿å­˜å«ä¼˜åŒ–å™¨çš„å®Œæ•´æ£€æŸ¥ç‚¹ï¼Œä¾¿äºæ¢å¤è®­ç»ƒ
            save_opt_every = config.get('training', {}).get('save_optimizer_freq', None)
            if save_opt_every and ((step + 1) % int(save_opt_every) == 0):
                checkpoint_path = output_dir / f"checkpoint_step_{step + 1}.pt"
                torch.save({
                    'step': step + 1,
                    'policy_state_dict': policy.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'config': config,
                    'training_metrics': all_training_metrics,
                }, checkpoint_path)
                print(f"âœ“ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # ğŸ”¥ ä¿å­˜æœ€ç»ˆç»Ÿè®¡æ•°æ®
    if stats_tracker is not None:
        stats_tracker.save_stats()
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {len(stats_tracker.rollout_stats)} ä¸ªä¸åŒçš„initçŠ¶æ€")
    else:
        print("ğŸ“Š ç»Ÿè®¡è·Ÿè¸ªå·²ç¦ç”¨")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results_path = output_dir / "final_training_results.json"
    # å°† OmegaConf è½¬ä¸ºåŸç”Ÿ dict ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if OMEGACONF_AVAILABLE and isinstance(config, DictConfig):
        serializable_config = OmegaConf.to_container(config, resolve=True)
    else:
        serializable_config = config
    with open(final_results_path, 'w') as f:
        json.dump({
            'config': serializable_config,
            'training_metrics': all_training_metrics,
            'total_steps': len(all_training_metrics)
        }, f, indent=2)
    
    # æœ€ç»ˆè½»é‡æƒé‡ï¼ˆä»…æ¨¡å‹ï¼‰
    final_weights_path = output_dir / "final_weights.pt"
    torch.save({
        'step': len(all_training_metrics),
        'policy_state_dict': policy.state_dict(),
        'config': config,
        'training_metrics': all_training_metrics,
    }, final_weights_path)
    print(f"âœ“ æœ€ç»ˆè½»é‡æƒé‡å·²ä¿å­˜: {final_weights_path}")

    # å¯é€‰ï¼šä¿å­˜æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆå«ä¼˜åŒ–å™¨ï¼‰ä¾¿äºæ¢å¤è®­ç»ƒ
    if config.get('training', {}).get('save_optimizer_final', False):
        final_checkpoint_path = output_dir / "final_checkpoint.pt"
        torch.save({
            'step': len(all_training_metrics),
            'policy_state_dict': policy.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'config': config,
            'training_metrics': all_training_metrics,
        }, final_checkpoint_path)
        print(f"âœ“ æœ€ç»ˆå®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")

    print(f"\nğŸ‰ RIPT-VLAé£æ ¼è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœå·²ä¿å­˜: {final_results_path}")
    print(f"âœ¨ ä½¿ç”¨äº†ç®€åŒ–çš„ç›´æ¥æ¶æ„ï¼Œå‡å°‘äº†æŠ½è±¡å±‚å¤æ‚åº¦")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Stage 11 RIPT-VLAé£æ ¼ç®€åŒ–è®­ç»ƒ")
    parser.add_argument(
        "--config_path", 
        type=str, 
        required=True,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config_path)
        
        # æ˜¾ç¤ºé…ç½®
        print("\n====== ä½¿ç”¨é…ç½® ======")
        if OMEGACONF_AVAILABLE:
            print(OmegaConf.to_yaml(config))
        else:
            print(yaml.dump(config, default_flow_style=False, allow_unicode=True))
        print("====================\n")
        
        # å¼€å§‹RIPT-VLAé£æ ¼çš„è®­ç»ƒ
        main_training_loop_ript_vla_style(config)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()