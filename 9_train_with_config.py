#!/usr/bin/env python3
"""
ç¬¬3é˜¶æ®µï¼šé…ç½®ç³»ç»Ÿå‡çº§ (9_train_with_config.py)
åŸºäº8_train_with_epochs.pyï¼Œå°†ç¡¬ç¼–ç é…ç½®æ›¿æ¢ä¸ºYAMLé…ç½®ç³»ç»Ÿ

æ ¸å¿ƒå‡çº§åŠŸèƒ½ï¼š
1. YAMLé…ç½®æ–‡ä»¶æ”¯æŒ (OmegaConf)
2. å‘½ä»¤è¡Œå‚æ•°ä¸é…ç½®æ–‡ä»¶ç»“åˆ
3. é…ç½®éªŒè¯å’Œé»˜è®¤å€¼å¤„ç†
4. å¤šç¯å¢ƒé…ç½®æ”¯æŒ (dev/prod)
5. å‘åå…¼å®¹æ€§ä¿æŒ

ä½¿ç”¨æ–¹æ³•:
    cd /zhaohan/ZJH/openpi_pytorch
    python 9_train_with_config.py --config_path pi0/ript/config/debug_train_pi0.yaml
    python 9_train_with_config.py --config_path pi0/ript/config/train_pi0_cfg_rl.yaml
    
    # è¦†ç›–é…ç½®å‚æ•°
    python 9_train_with_config.py --config_path pi0/ript/config/debug_train_pi0.yaml \
        --override training.num_train_steps=10 algo.lr=2e-5
"""

import os
import sys
import json
import numpy as np
import torch
from pathlib import Path
from datetime import datetime
import argparse
import hashlib
from typing import List, Dict, Any, Tuple, Optional, Union
import yaml

# ä¿®å¤tokenizerså¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥OmegaConfç”¨äºé…ç½®ç®¡ç†
try:
    from omegaconf import OmegaConf, DictConfig
    OMEGACONF_AVAILABLE = True
    print("âœ“ ä½¿ç”¨OmegaConfè¿›è¡Œé…ç½®ç®¡ç†")
except ImportError:
    OMEGACONF_AVAILABLE = False
    print("âš ï¸ OmegaConfæœªå®‰è£…ï¼Œå›é€€åˆ°åŸºç¡€YAMLè§£æ")

from pi0 import PI0Policy
from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner
from pi0.ript.reward_function import BinarySuccessReward
from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter

# å¯¼å…¥å¢å¼ºæ™ºèƒ½é‡‡æ ·ç³»ç»Ÿ
try:
    from pi0.ript.utils.enhanced_smart_sampling import EnhancedSmartSampler, enhanced_collect_smart_batches
    ENHANCED_SAMPLING_AVAILABLE = True
    print("âœ“ å¢å¼ºæ™ºèƒ½é‡‡æ ·ç³»ç»Ÿå·²å¯¼å…¥")
except ImportError:
    ENHANCED_SAMPLING_AVAILABLE = False
    print("âš ï¸ å¢å¼ºæ™ºèƒ½é‡‡æ ·ç³»ç»Ÿæœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨åŸºç¡€é‡‡æ ·")
    
    # åŸºç¡€é‡‡æ ·å®ç°ä½œä¸ºåå¤‡
    class EnhancedSmartSampler:
        def __init__(self, config):
            self.config = config
        
        def smart_sample_init_state(self, init_dataset):
            init_states = init_dataset.sample_batch(self.config.num_parallel_envs)
            state_hash = self.compute_state_hash(init_states)
            return init_states, state_hash
        
        def compute_state_hash(self, state_array):
            import hashlib
            return hashlib.sha256(np.ascontiguousarray(state_array).tobytes()).hexdigest()[:16]
    
    def enhanced_collect_smart_batches(env_runner, reward_function, init_dataset, sampler, config, iteration_idx):
        # ç®€åŒ–çš„é‡‡æ ·å®ç°
        print(f"ä½¿ç”¨åŸºç¡€é‡‡æ ·æ¨¡å¼ (è¿­ä»£ {iteration_idx + 1})")
        collected_batches = []
        
        for i in range(config.target_batches_per_iteration):
            init_states, _ = sampler.smart_sample_init_state(init_dataset)
            
            try:
                rollout_generator = env_runner.run_policy_in_env(
                    env_name=config.task_id,
                    all_init_states=init_states
                )
                
                rollout_batch = list(rollout_generator)
                if rollout_batch:
                    episodes = []
                    for success, total_reward, episode_data in rollout_batch:
                        episode = {
                            'success': success,
                            'total_reward': total_reward,
                            **episode_data
                        }
                        episodes.append(episode)
                    
                    if episodes:
                        collected_batches.append(episodes)
            except Exception as e:
                print(f"é‡‡æ ·å¤±è´¥: {e}")
                continue
        
        return collected_batches

def load_config_from_yaml(config_path: str, overrides: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®ï¼Œæ”¯æŒOmegaConfå’ŒåŸºç¡€YAML
    
    Args:
        config_path: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
        overrides: é…ç½®è¦†ç›–åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ ["training.lr=1e-5", "algo.batch_size=32"]
        
    Returns:
        é…ç½®å­—å…¸
    """
    config_path = Path(config_path)
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    print(f"ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    if OMEGACONF_AVAILABLE:
        # ä½¿ç”¨OmegaConfåŠ è½½é…ç½®
        config = OmegaConf.load(config_path)
        
        # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
        if overrides:
            print(f"ğŸ”§ åº”ç”¨é…ç½®è¦†ç›–:")
            for override in overrides:
                print(f"  {override}")
                key, value = override.split('=', 1)
                # å°è¯•è½¬æ¢ä¸ºæ­£ç¡®çš„ç±»å‹
                try:
                    # å°è¯•æ•°å­—è½¬æ¢
                    if '.' in value and value.replace('.', '').replace('-', '').isdigit():
                        value = float(value)
                    elif value.replace('-', '').isdigit():
                        value = int(value)
                    elif value.lower() in ['true', 'false']:
                        value = value.lower() == 'true'
                except:
                    pass  # ä¿æŒå­—ç¬¦ä¸²ç±»å‹
                
                # ä½¿ç”¨OmegaConfçš„æ­£ç¡®æ–¹æ³•è®¾ç½®åµŒå¥—é”®
                keys = key.split('.')
                target = config
                for k in keys[:-1]:
                    if k not in target:
                        target[k] = {}
                    target = target[k]
                target[keys[-1]] = value
        
        # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
        config_dict = OmegaConf.to_container(config, resolve=True)
        
    else:
        # ä½¿ç”¨åŸºç¡€YAMLåŠ è½½
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        
        # åº”ç”¨ç®€å•çš„é…ç½®è¦†ç›–
        if overrides:
            print(f"ğŸ”§ åº”ç”¨é…ç½®è¦†ç›– (åŸºç¡€æ¨¡å¼):")
            for override in overrides:
                print(f"  {override}")
                key, value = override.split('=', 1)
                # ç®€å•çš„åµŒå¥—é”®å¤„ç†
                keys = key.split('.')
                target = config_dict
                for k in keys[:-1]:
                    if k not in target:
                        target[k] = {}
                    target = target[k]
                
                # ç±»å‹è½¬æ¢
                try:
                    if '.' in value and value.replace('.', '').replace('-', '').isdigit():
                        value = float(value)
                    elif value.replace('-', '').isdigit():
                        value = int(value)
                    elif value.lower() in ['true', 'false']:
                        value = value.lower() == 'true'
                except:
                    pass
                
                target[keys[-1]] = value
    
    print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    return config_dict

def validate_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    éªŒè¯å’Œè¡¥å……é…ç½®å‚æ•°
    
    Args:
        config: åŸå§‹é…ç½®å­—å…¸
        
    Returns:
        éªŒè¯åçš„é…ç½®å­—å…¸
    """
    print("ğŸ” éªŒè¯é…ç½®å‚æ•°...")
    
    # è®¾ç½®é»˜è®¤å€¼
    defaults = {
        # è·¯å¾„é…ç½®
        'policy_path': "/zhaohan/ZJH/openpi_pytorch/checkpoints/pi0_libero_pytorch",
        'norm_stats_path': "/zhaohan/ZJH/openpi_pytorch/lerobot_dataset/norm_stats.json",
        'output_dir': "./pi0/ript/output",
        
        # ä»»åŠ¡å’Œç¯å¢ƒ
        'task': {
            'benchmark_name': "libero_spatial",
            'task_name': None,
            'num_parallel_envs': 1,
            'max_episode_length': 200,
        },
        
        # ç®—æ³•é…ç½®
        'algo': {
            'rloo_batch_size': 4,
            'num_epochs': 4,
            'data_batch_size': 2,
            'gradient_accumulation_steps': 1,
            'lr': 1e-5,
            'grad_norm_clip': 1.0,
            'enable_dynamic_sampling': True,
            'enable_rollout_stats_tracking': True,
            'rollout_skip_threshold': 3,
            'rollout_stats_path': "./rollout_stats.json",
            'use_val_init': False,
        },
        
        # è®­ç»ƒé…ç½®
        'training': {
            'num_train_steps': 5,
            'seed': 42,
            'save_freq': 5,
            'save_best': True,
            'use_mixed_precision': False,
            'optimizer': {
                'type': "AdamW",
                'weight_decay': 0.01,
                'momentum': 0.9,
                'beta1': 0.9,
                'beta2': 0.999,
            }
        },
        
        # æ—¥å¿—é…ç½®
        'logging': {
            'use_wandb': False,
            'wandb_project': "ript-pi0",
            'wandb_entity': None,
            'wandb_mode': "offline",
            'log_freq': 1,
            'log_gradients': False,
        },
        
        # å¢å¼ºé‡‡æ ·é…ç½®
        'enhanced_sampling': {
            'enable_smart_filtering': True,
            'max_sampling_attempts': 20,
            'debug_sampling': True,
            'save_videos': True,
            'video_dir': "rollout_videos_config",
        }
    }
    
    # é€’å½’åˆå¹¶é»˜è®¤å€¼
    def merge_defaults(target: Dict, source: Dict) -> Dict:
        for key, value in source.items():
            if key not in target:
                target[key] = value
            elif isinstance(value, dict) and isinstance(target[key], dict):
                target[key] = merge_defaults(target[key], value)
        return target
    
    config = merge_defaults(config, defaults)
    
    # éªŒè¯å¿…éœ€çš„è·¯å¾„
    required_paths = ['policy_path', 'norm_stats_path']
    for path_key in required_paths:
        if path_key in config:
            path = Path(config[path_key])
            if not path.exists():
                print(f"âš ï¸ è­¦å‘Š: {path_key} è·¯å¾„ä¸å­˜åœ¨: {path}")
            else:
                print(f"âœ“ {path_key}: {path}")
    
    # éªŒè¯æ•°å€¼å‚æ•°èŒƒå›´
    if config.get('algo', {}).get('lr', 0) <= 0:
        raise ValueError("å­¦ä¹ ç‡å¿…é¡»å¤§äº0")
    
    if config.get('training', {}).get('num_train_steps', 0) <= 0:
        raise ValueError("è®­ç»ƒæ­¥æ•°å¿…é¡»å¤§äº0")
    
    print("âœ“ é…ç½®éªŒè¯é€šè¿‡")
    return config

class ConfigurableTrainingRunner:
    """åŸºäºé…ç½®æ–‡ä»¶çš„è®­ç»ƒè¿è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.setup_directories()
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„è¾“å‡ºç›®å½•"""
        output_dir = Path(self.config['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå®éªŒç‰¹å®šçš„è¾“å‡ºç›®å½•
        exp_name = self.config.get('exp_name', 'pi0_config_experiment')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.exp_output_dir = output_dir / f"{exp_name}_{timestamp}"
        self.exp_output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºè§†é¢‘ç›®å½•
        video_dir = self.config.get('enhanced_sampling', {}).get('video_dir', 'rollout_videos')
        self.video_dir = Path(video_dir)
        self.video_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ å®éªŒè¾“å‡ºç›®å½•: {self.exp_output_dir}")
        print(f"ğŸ“ è§†é¢‘ä¿å­˜ç›®å½•: {self.video_dir}")
    
    def create_optimizer(self, policy):
        """æ ¹æ®é…ç½®åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_config = self.config['training']['optimizer']
        lr = self.config['algo']['lr']
        
        print(f"\n=== åˆ›å»ºä¼˜åŒ–å™¨ ===")
        print(f"ä¼˜åŒ–å™¨ç±»å‹: {optimizer_config['type']}")
        print(f"å­¦ä¹ ç‡: {lr}")
        print(f"æƒé‡è¡°å‡: {optimizer_config.get('weight_decay', 0.0)}")
        
        if optimizer_config['type'] == "AdamW":
            optimizer = torch.optim.AdamW(
                policy.parameters(),
                lr=lr,
                weight_decay=optimizer_config.get('weight_decay', 0.01),
                betas=(optimizer_config.get('beta1', 0.9), optimizer_config.get('beta2', 0.999))
            )
        elif optimizer_config['type'] == "Adam":
            optimizer = torch.optim.Adam(
                policy.parameters(),
                lr=lr,
                weight_decay=0.0
            )
        elif optimizer_config['type'] == "SGD":
            optimizer = torch.optim.SGD(
                policy.parameters(),
                lr=lr,
                momentum=optimizer_config.get('momentum', 0.9),
                weight_decay=optimizer_config.get('weight_decay', 0.0)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_config['type']}")
        
        # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
        total_params = sum(p.numel() for p in policy.parameters())
        trainable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)
        
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params:.2%}")
        
        return optimizer
    
    def create_enhanced_sampler_config(self):
        """åˆ›å»ºå¢å¼ºé‡‡æ ·å™¨çš„é…ç½®å¯¹è±¡"""
        class SamplerConfig:
            def __init__(self, config_dict):
                # åŸºç¡€é…ç½®
                task_config = config_dict.get('task', {})
                algo_config = config_dict.get('algo', {})
                sampling_config = config_dict.get('enhanced_sampling', {})
                
                # ä»»åŠ¡é…ç½®
                self.benchmark_name = task_config.get('benchmark_name', 'libero_spatial')
                self.task_id = task_config.get('task_name', 8)  # ä½¿ç”¨task_nameä½œä¸ºtask_id
                self.num_parallel_envs = task_config.get('num_parallel_envs', 1)
                self.max_episode_length = task_config.get('max_episode_length', 200)
                
                # é‡‡æ ·é…ç½®
                self.rollouts_per_batch = algo_config.get('rloo_batch_size', 4)
                self.target_batches_per_iteration = algo_config.get('data_batch_size', 2)
                self.gradient_accumulation_steps = algo_config.get('gradient_accumulation_steps', 1)
                self.num_epochs = algo_config.get('num_epochs', 4)
                
                # å¢å¼ºé‡‡æ ·é…ç½®
                self.enable_smart_filtering = sampling_config.get('enable_smart_filtering', True)
                self.max_sampling_attempts = sampling_config.get('max_sampling_attempts', 20)
                self.debug_sampling = sampling_config.get('debug_sampling', True)
                self.save_videos = sampling_config.get('save_videos', True)
                self.video_dir = sampling_config.get('video_dir', 'rollout_videos_config')
                
                # æ™ºèƒ½çŠ¶æ€è·Ÿè¸ªé…ç½®
                self.enable_rollout_stats_tracking = algo_config.get('enable_rollout_stats_tracking', True)
                self.rollout_skip_threshold = algo_config.get('rollout_skip_threshold', 3)
                self.rollout_stats_path = algo_config.get('rollout_stats_path', './rollout_stats.json')
                self.state_history_window = 20
        
        return SamplerConfig(self.config)
    
    def run_training(self):
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("=== ç¬¬3é˜¶æ®µï¼šé…ç½®ç³»ç»Ÿå‡çº§è®­ç»ƒ ===")
        print("æ”¯æŒYAMLé…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°è¦†ç›–")
        print()
        
        # æ˜¾ç¤ºå…³é”®é…ç½®
        print(f"ğŸ¯ è®­ç»ƒé…ç½®æ‘˜è¦:")
        print(f"  æ¨¡å‹è·¯å¾„: {self.config['policy_path']}")
        print(f"  åŸºå‡†æµ‹è¯•: {self.config['task']['benchmark_name']}")
        print(f"  ä»»åŠ¡ID: {self.config['task'].get('task_name', 'auto')}")
        print(f"  è®­ç»ƒæ­¥æ•°: {self.config['training']['num_train_steps']}")
        print(f"  å­¦ä¹ ç‡: {self.config['algo']['lr']}")
        print(f"  æ‰¹æ¬¡å¤§å°: {self.config['algo']['rloo_batch_size']}")
        print(f"  æ¢¯åº¦ç´¯ç§¯: {self.config['algo']['gradient_accumulation_steps']}")
        print(f"  æ™ºèƒ½é‡‡æ ·: {self.config['algo']['enable_dynamic_sampling']}")
        print("")
        
        # åŠ è½½PI0æ¨¡å‹
        print(f"åŠ è½½PI0æ¨¡å‹: {self.config['policy_path']}")
        policy = PI0Policy.from_pretrained(self.config['policy_path'])
        device = policy.config.device
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = self.create_optimizer(policy)
        
        # åˆ›å»ºCFGé€‚é…å™¨
        print(f"\n=== åˆ›å»ºCFGé€‚é…å™¨ ===")
        os.environ["PI0_DEBUG_SAVE_VIDEO"] = "false"  # é¿å…é‡å¤è§†é¢‘
        cfg_adapter = PI0_CFG_Adapter(policy, norm_stats_path=self.config['norm_stats_path'])
        print("âœ“ CFGé€‚é…å™¨åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºç¯å¢ƒè¿è¡Œå™¨
        print("\n=== åˆå§‹åŒ–ç¯å¢ƒè¿è¡Œå™¨ ===")
        sampler_config = self.create_enhanced_sampler_config()
        
        # --- è·å–ä»»åŠ¡å (æ”¯æŒIDâ†’å­—ç¬¦ä¸²æ˜ å°„ï¼Œå…¼å®¹æ— liberoå®‰è£…) ---
        task_id_or_name = sampler_config.task_id
        if isinstance(task_id_or_name, int):
            try:
                # ä¼˜å…ˆä½¿ç”¨å®˜æ–¹æ˜ å°„
                from libero.benchmark import LIBERO_SPATIAL_TASKS
                task_name = LIBERO_SPATIAL_TASKS[task_id_or_name]
            except ImportError as e:
                print("âš ï¸  æœªæ£€æµ‹åˆ° libreo.benchmark, å°è¯•ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ task_names_to_use è¿›è¡Œå›é€€æ˜ å°„")
                fallback_list = self.config.get('task', {}).get('task_names_to_use', [])
                if task_id_or_name < len(fallback_list):
                    task_name = fallback_list[task_id_or_name]
                    print(f"   â†’ å›é€€æ˜ å°„: task_id {task_id_or_name} -> {task_name}")
                else:
                    raise ImportError(
                        "libero.benchmark æœªå®‰è£…ä¸”æ— æ³•ä» task_names_to_use æ¨æ–­ä»»åŠ¡å; "
                        "è¯·å®‰è£… libreo æˆ–ç›´æ¥åœ¨ YAML ä¸­æä¾›å­—ç¬¦ä¸² task_name"
                    ) from e
        else:
            task_name = task_id_or_name

        env_runner = LIBEROEnvRunner(
            policy=policy,
            benchmark_name=sampler_config.benchmark_name,
            rollouts_per_env=1,
            num_parallel_envs=sampler_config.num_parallel_envs,
            max_episode_length=sampler_config.max_episode_length,
            task_names_to_use=[task_name],      # â† æ”¹è¿™é‡Œ
            config=self.config,
            rank=0,
            world_size=1,
            norm_stats_path=self.config['norm_stats_path']
        )
        
        # åˆ›å»ºç»„ä»¶
        reward_function = BinarySuccessReward()
        init_dataset = self.create_initial_state_dataset()
        smart_sampler = EnhancedSmartSampler(sampler_config)
        
        # ğŸš€ ä¸»è®­ç»ƒå¾ªç¯å¼€å§‹
        print(f"\n=== å¼€å§‹é…ç½®åŒ–è®­ç»ƒå¾ªç¯ ===")
        training_iterations = self.config['training']['num_train_steps']
        print(f"å°†è¿›è¡Œ {training_iterations} è½®å®Œæ•´çš„è®­ç»ƒè¿­ä»£")
        
        all_training_metrics = []
        
        for iteration in range(training_iterations):
            print(f"\n" + "="*60)
            print(f"ğŸ”„ è®­ç»ƒè¿­ä»£ {iteration + 1}/{training_iterations}")
            print("="*60)
            
            # ç¬¬1æ­¥ï¼šå¢å¼ºæ™ºèƒ½é‡‡æ ·æ”¶é›†æ•°æ®
            print(f"ğŸ¯ å¼€å§‹é…ç½®åŒ–æ™ºèƒ½é‡‡æ ·æ•°æ®æ”¶é›†...")
            collected_batches = enhanced_collect_smart_batches(
                env_runner, reward_function, init_dataset, 
                smart_sampler, sampler_config, iteration
            )
            
            if not collected_batches:
                print("âš ï¸ æ™ºèƒ½é‡‡æ ·æœªæ”¶é›†åˆ°æœ‰ç”¨çš„æ•°æ®ï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒ")
                continue
            
            # ç¬¬2æ­¥ï¼šåˆå¹¶æ‰€æœ‰episodes
            all_episodes = []
            for batch in collected_batches:
                all_episodes.extend(batch)
            
            if not all_episodes:
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„episodesï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒ")
                continue
            
            # ç¬¬3æ­¥ï¼šè®¡ç®—ä¼˜åŠ¿å€¼
            print(f"\n--- ä¼˜åŠ¿è®¡ç®— ---")
            all_rewards = [ep['total_reward'] for ep in all_episodes]
            rewards_tensor = torch.tensor(all_rewards, dtype=torch.float32)
            
            # Leave-One-Outä¼˜åŠ¿è®¡ç®—
            advantages = []
            for i in range(len(all_rewards)):
                other_rewards = torch.cat([rewards_tensor[:i], rewards_tensor[i+1:]])
                baseline = other_rewards.mean() if len(other_rewards) > 0 else 0.0
                advantage = rewards_tensor[i] - baseline
                advantages.append(advantage.item())
            
            advantages = torch.tensor(advantages, dtype=torch.float32, device=device)
            
            print(f"Episodesæ€»æ•°: {len(all_episodes)}")
            print(f"å¹³å‡å¥–åŠ±: {rewards_tensor.mean().item():.4f}")
            print(f"æˆåŠŸç‡: {sum(ep['success'] for ep in all_episodes) / len(all_episodes):.2f}")
            print(f"ä¼˜åŠ¿æ–¹å·®: {advantages.var().item():.6f}")
            
            # ç¬¬4æ­¥ï¼šæ¢¯åº¦ç´¯ç§¯è®­ç»ƒ
            training_metrics = self.train_with_gradient_accumulation(
                policy, optimizer, cfg_adapter, all_episodes, advantages, sampler_config
            )
            
            # ç¬¬5æ­¥ï¼šè®°å½•è¿­ä»£æŒ‡æ ‡
            iteration_metrics = {
                'iteration': iteration + 1,
                'collected_batches': len(collected_batches),
                'total_episodes': len(all_episodes),
                'mean_reward': rewards_tensor.mean().item(),
                'success_rate': sum(ep['success'] for ep in all_episodes) / len(all_episodes),
                'advantage_variance': advantages.var().item(),
                **training_metrics
            }
            
            all_training_metrics.append(iteration_metrics)
            
            print(f"\nâœ“ è¿­ä»£ {iteration + 1} å®Œæˆ:")
            print(f"  æ”¶é›†batches: {iteration_metrics['collected_batches']}")
            print(f"  æ€»episodes: {iteration_metrics['total_episodes']}")
            print(f"  å¹³å‡å¥–åŠ±: {iteration_metrics['mean_reward']:.4f}")
            print(f"  æˆåŠŸç‡: {iteration_metrics['success_rate']:.2f}")
            print(f"  å¹³å‡æŸå¤±: {iteration_metrics['mean_loss']:.6f}")
            print(f"  è®­ç»ƒæ­¥æ•°: {iteration_metrics['total_steps']}")
        
        # è®­ç»ƒæ€»ç»“
        self.save_training_results(all_training_metrics)
        self.print_training_summary(all_training_metrics)
        
        return all_training_metrics
    
    def create_initial_state_dataset(self):
        """åˆ›å»ºåˆå§‹çŠ¶æ€æ•°æ®é›†ï¼ˆä¿æŒä¸ç¬¬8é˜¶æ®µå…¼å®¹ï¼‰"""
        class InitialStateDataset:
            def __init__(self, num_states=50, state_dim=8):
                self.states = []
                np.random.seed(None)
                
                for i in range(num_states):
                    base_state = np.zeros(state_dim, dtype=np.float32)
                    
                    if i < num_states // 3:
                        noise = np.random.normal(0, 0.05, state_dim).astype(np.float32)
                    elif i < 2 * num_states // 3:
                        noise = np.random.normal(0, 0.15, state_dim).astype(np.float32)
                    else:
                        noise = np.random.normal(0, 0.25, state_dim).astype(np.float32)
                    
                    state = base_state + noise
                    self.states.append(state)
                
                print(f"âœ“ åˆ›å»ºäº†åŒ…å« {len(self.states)} ä¸ªå¤šæ ·åŒ–åˆå§‹çŠ¶æ€çš„æ•°æ®é›†")
            
            def sample_batch(self, batch_size=1):
                if batch_size > len(self.states):
                    indices = np.random.choice(len(self.states), batch_size, replace=True)
                else:
                    indices = np.random.choice(len(self.states), batch_size, replace=False)
                
                sampled_states = np.array([self.states[i] for i in indices])
                return sampled_states
        
        return InitialStateDataset()
    
    def train_with_gradient_accumulation(self, policy, optimizer, cfg_adapter, all_episodes, advantages, config):
        """ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯è¿›è¡Œè®­ç»ƒï¼ˆä¸ç¬¬8é˜¶æ®µå…¼å®¹ï¼‰"""
        print(f"\n--- æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ ---")
        print(f"æ€»episodes: {len(all_episodes)}")
        print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.gradient_accumulation_steps}")
        print(f"è®­ç»ƒepochs: {config.num_epochs}")
        
        policy.train()
        
        total_steps = 0
        all_losses = []
        
        for epoch in range(config.num_epochs):
            print(f"\n=== Epoch {epoch + 1}/{config.num_epochs} ===")
            
            episode_batches = []
            batch_size = max(1, len(all_episodes) // config.gradient_accumulation_steps)
            
            for i in range(0, len(all_episodes), batch_size):
                batch_episodes = all_episodes[i:i + batch_size]
                batch_advantages = advantages[i:i + len(batch_episodes)]
                episode_batches.append((batch_episodes, batch_advantages))
            
            print(f"åˆ›å»ºäº† {len(episode_batches)} ä¸ªæ‰¹æ¬¡ï¼Œå¹³å‡æ¯æ‰¹ {batch_size} episodes")
            
            optimizer.zero_grad()
            epoch_losses = []
            
            for step, (batch_episodes, batch_advantages) in enumerate(episode_batches):
                try:
                    batch_loss = cfg_adapter.compute_weighted_loss(batch_episodes, batch_advantages)
                    normalized_loss = batch_loss / config.gradient_accumulation_steps
                    normalized_loss.backward()
                    
                    batch_loss_value = batch_loss.item()
                    epoch_losses.append(batch_loss_value)
                    
                    if (step + 1) % config.gradient_accumulation_steps == 0 or (step + 1) == len(episode_batches):
                        if self.config['algo']['grad_norm_clip'] > 0:
                            grad_norm = torch.nn.utils.clip_grad_norm_(policy.parameters(), 
                                                                     self.config['algo']['grad_norm_clip'])
                        
                        optimizer.step()
                        optimizer.zero_grad()
                        total_steps += 1
                        
                except Exception as e:
                    print(f"    âŒ Step {step + 1} è®­ç»ƒå‡ºé”™: {e}")
                    continue
            
            if epoch_losses:
                epoch_mean_loss = np.mean(epoch_losses)
                all_losses.extend(epoch_losses)
                print(f"âœ“ Epoch {epoch + 1} å¹³å‡æŸå¤±: {epoch_mean_loss:.6f}")
            else:
                print(f"âš ï¸ Epoch {epoch + 1} æ²¡æœ‰æœ‰æ•ˆæŸå¤±")
        
        return {
            'total_steps': total_steps,
            'total_losses': all_losses,
            'mean_loss': np.mean(all_losses) if all_losses else 0.0
        }
    
    def save_training_results(self, all_training_metrics):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        results_file = self.exp_output_dir / "training_results.json"
        
        results = {
            'config': self.config,
            'training_metrics': all_training_metrics,
            'experiment_info': {
                'exp_output_dir': str(self.exp_output_dir),
                'video_dir': str(self.video_dir),
                'timestamp': datetime.now().isoformat(),
                'total_iterations': len(all_training_metrics),
                'total_episodes': sum(m['total_episodes'] for m in all_training_metrics),
                'total_training_steps': sum(m['total_steps'] for m in all_training_metrics),
                'final_success_rate': all_training_metrics[-1]['success_rate'] if all_training_metrics else 0.0
            }
        }
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ å®Œæ•´è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    def print_training_summary(self, all_training_metrics):
        """æ‰“å°è®­ç»ƒæ‘˜è¦"""
        print(f"\n" + "="*60)
        print("ğŸ‰ é…ç½®åŒ–è®­ç»ƒå¾ªç¯ç»“æŸï¼")
        print("="*60)
        
        print("\nâœ… æˆåŠŸå®ç°çš„é…ç½®åŒ–åŠŸèƒ½:")
        print("1. âœ“ YAMLé…ç½®æ–‡ä»¶æ”¯æŒ")
        print("2. âœ“ å‘½ä»¤è¡Œå‚æ•°è¦†ç›–")
        print("3. âœ“ é…ç½®éªŒè¯å’Œé»˜è®¤å€¼")
        print("4. âœ“ å¤šç¯å¢ƒé…ç½®æ”¯æŒ")
        print("5. âœ“ å¢å¼ºæ™ºèƒ½é‡‡æ ·é›†æˆ")
        print("6. âœ“ å‘åå…¼å®¹æ€§ä¿æŒ")
        
        if all_training_metrics:
            print(f"\nğŸ“Š è®­ç»ƒè½¨è¿¹æ€»ç»“:")
            print("Iter | Batches | Episodes | Reward | Success | Loss    | Steps")
            print("-----|---------|----------|--------|---------|---------|------")
            for m in all_training_metrics:
                print(f"{m['iteration']:4d} | {m['collected_batches']:7d} | "
                      f"{m['total_episodes']:8d} | {m['mean_reward']:6.3f} | "
                      f"{m['success_rate']:7.2f} | {m['mean_loss']:7.4f} | {m['total_steps']:5d}")
            
            first_loss = all_training_metrics[0]['mean_loss']
            last_loss = all_training_metrics[-1]['mean_loss']
            if first_loss > 0:
                loss_change = ((last_loss - first_loss) / first_loss) * 100
                print(f"\nğŸ“ˆ è®­ç»ƒè¶‹åŠ¿:")
                print(f"  åˆå§‹æŸå¤±: {first_loss:.6f}")
                print(f"  æœ€ç»ˆæŸå¤±: {last_loss:.6f}")
                print(f"  æŸå¤±å˜åŒ–: {loss_change:+.2f}%")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="PI0 RIPTé…ç½®åŒ–è®­ç»ƒ - ç¬¬3é˜¶æ®µ")
    parser.add_argument("--config_path", type=str, required=True,
                        help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--override", action='append', dest='overrides',
                        help="è¦†ç›–é…ç½®å‚æ•°ï¼Œæ ¼å¼: key=value")
    
    # ä¿æŒä¸ç¬¬8é˜¶æ®µçš„å…¼å®¹æ€§
    parser.add_argument("--training_iterations", type=int, default=None,
                        help="è®­ç»ƒè¿­ä»£æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--lr", type=float, default=None,
                        help="å­¦ä¹ ç‡ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--debug_sampling", action='store_true',
                        help="å¯ç”¨é‡‡æ ·è°ƒè¯•")
    
    return parser.parse_args()

def main():
    print("=== ç¬¬3é˜¶æ®µï¼šé…ç½®ç³»ç»Ÿå‡çº§è®­ç»ƒ ===")
    print("ä»ç¡¬ç¼–ç é…ç½®å‡çº§ä¸ºYAMLé…ç½®ç³»ç»Ÿ")
    print()
    
    # è§£æå‚æ•°
    args = parse_args()
    
    # æ„å»ºé…ç½®è¦†ç›–åˆ—è¡¨
    overrides = args.overrides or []
    
    # æ·»åŠ å…¼å®¹æ€§è¦†ç›–
    if args.training_iterations is not None:
        overrides.append(f"training.num_train_steps={args.training_iterations}")
    if args.lr is not None:
        overrides.append(f"algo.lr={args.lr}")
    if args.debug_sampling:
        overrides.append("enhanced_sampling.debug_sampling=true")
    
    # åŠ è½½å’ŒéªŒè¯é…ç½®
    try:
        config = load_config_from_yaml(args.config_path, overrides)
        config = validate_config(config)
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # åˆ›å»ºå¹¶è¿è¡Œè®­ç»ƒ
    try:
        trainer = ConfigurableTrainingRunner(config)
        training_metrics = trainer.run_training()
        
        print("\nğŸ¯ ç¬¬3é˜¶æ®µå®Œæˆï¼ç°åœ¨æ”¯æŒå®Œæ•´çš„YAMLé…ç½®ç³»ç»Ÿ")
        return 0
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())