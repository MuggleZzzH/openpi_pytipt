#!/usr/bin/env python3
"""
åŸºäºbackup runnerçš„å•GPU RIPTè®­ç»ƒè„šæœ¬
ä½¿ç”¨ç®€æ´ç‰ˆpi0_libero_runner_backup.pyçš„é€»è¾‘ï¼Œé¿å…å¤æ‚çš„åˆ†å¸ƒå¼ä»£ç 
"""

import os
import sys
import torch
import yaml
import numpy as np
from pathlib import Path
from datetime import datetime
import argparse

# è®¾ç½®ç¯å¢ƒ
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def setup_logging(output_dir, exp_name):
    """è®¾ç½®è¾“å‡ºç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = Path(output_dir) / f"{exp_name}_{timestamp}_rank0"
    run_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {run_dir}")
    return run_dir

def main():
    parser = argparse.ArgumentParser(description='å•GPU RIPTè®­ç»ƒ')
    parser.add_argument('--config_path', type=str, 
                       default='pi0/ript/config/single_gpu_test.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    args = parser.parse_args()
    
    print("ğŸš€ å•GPU RIPTè®­ç»ƒ (åŸºäºç®€æ´backupé€»è¾‘)")
    print("=" * 50)
    
    # 1. åŠ è½½é…ç½®
    print("ğŸ“‹ åŠ è½½é…ç½®...")
    config = load_config(args.config_path)
    print(f"   å®éªŒå: {config['exp_name']}")
    print(f"   ä»»åŠ¡æ•°é‡: {len(config['task']['task_names_to_use'])}")
    
    # 2. è®¾ç½®è¾“å‡ºç›®å½•
    run_dir = setup_logging(config['output_dir'], config['exp_name'])
    
    # 3. æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        raise RuntimeError("CUDAä¸å¯ç”¨")
    
    device = torch.device('cuda:0')
    print(f"ğŸ”§ è®¾å¤‡: {device}")
    
    # 4. å¯¼å…¥æ ¸å¿ƒç»„ä»¶ï¼ˆç¡®ä¿ä½¿ç”¨backupç‰ˆæœ¬ï¼‰
    print("ğŸ“¦ å¯¼å…¥æ ¸å¿ƒç»„ä»¶...")
    from pi0 import PI0Policy
    from pi0.ript.env.pi0_libero_runner import LIBEROEnvRunner  # è¿™åº”è¯¥æ˜¯backupç‰ˆæœ¬
    from pi0.ript.reward_function import BinarySuccessReward
    from pi0.ript.algos.rl_optimizers.pi0_cfg_interface import PI0_CFG_Adapter
    
    # 5. åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½PI0æ¨¡å‹...")
    policy = PI0Policy.from_pretrained(config['policy_path'])
    policy = policy.to(device)
    print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in policy.parameters()) / 1e6:.1f}M")
    
    # 6. åˆ›å»ºæ¨¡å‹é€‚é…å™¨
    model_adapter = PI0_CFG_Adapter(
        policy, 
        norm_stats_path=config.get('norm_stats_path'),
        device=device
    )
    
    # 7. åˆ›å»ºç¯å¢ƒè¿è¡Œå™¨
    print("ğŸŒ åˆ›å»ºç¯å¢ƒè¿è¡Œå™¨...")
    env_runner = LIBEROEnvRunner(
        policy=policy,
        task_names_to_use=config['task']['task_names_to_use'],
        benchmark_name=config['task']['benchmark_name'],
        num_parallel_envs=config['task']['num_parallel_envs'],
        max_episode_length=config['task']['max_episode_length'],
        rank=0,
        world_size=1
    )
    
    # 8. åˆ›å»ºå¥–åŠ±å‡½æ•°
    reward_fn = BinarySuccessReward()
    
    # 9. ç®€å•çš„è®­ç»ƒå¾ªç¯
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒå¾ªç¯")
    print("-" * 30)
    
    num_train_steps = config['training']['num_train_steps']
    batch_size = config['algo']['data_batch_size']
    
    for step in range(num_train_steps):
        print(f"\nğŸ“Š è®­ç»ƒæ­¥éª¤ {step + 1}/{num_train_steps}")
        
        # é€‰æ‹©å½“å‰ä»»åŠ¡ 
        task_name = config['task']['task_names_to_use'][step % len(config['task']['task_names_to_use'])]
        print(f"ğŸ¯ å½“å‰ä»»åŠ¡: {task_name.split('_')[-1]}")
        
        try:
            # æ•°æ®æ”¶é›†
            print("ğŸ“Š æ”¶é›†æ•°æ®...")
            init_states = np.random.randn(batch_size, 10).astype(np.float32) * 0.1
            
            episodes = []
            rollout_results = env_runner.run_policy_in_env(
                env_name=task_name,
                all_init_states=init_states,
                debug_save_video=(step == 0)  # åªåœ¨ç¬¬ä¸€æ­¥ä¿å­˜è§†é¢‘
            )
            
            # æ”¶é›†episodes
            for success, total_reward, episode_data in rollout_results:
                episodes.append(episode_data)
                print(f"   Episode: æˆåŠŸ={success}, å¥–åŠ±={total_reward:.3f}")
            
            # è®¡ç®—ä¼˜åŠ¿ (ç®€åŒ–ç‰ˆæœ¬)
            advantages = []
            for episode in episodes:
                total_reward = sum(episode['rewards'])
                # Leave-One-Outä¼˜åŠ¿: ç›¸å¯¹äºæ‰¹æ¬¡å¹³å‡å€¼
                batch_avg_reward = np.mean([sum(ep['rewards']) for ep in episodes])
                advantage = total_reward - batch_avg_reward
                advantages.append(advantage)
            
            advantages = torch.tensor(advantages, device=device)
            print(f"   ä¼˜åŠ¿: å‡å€¼={advantages.mean():.3f}, æ ‡å‡†å·®={advantages.std():.3f}")
            
            # æ¨¡å‹æ›´æ–° 
            if len(episodes) > 0:
                print("ğŸ”„ æ›´æ–°æ¨¡å‹...")
                loss = model_adapter.compute_weighted_loss(episodes, advantages, device)
                
                print(f"   æŸå¤±: {loss.item():.6f}")
                
                # åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                if loss.requires_grad:
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(
                        model_adapter.get_policy_model().parameters(), 
                        config['algo']['grad_norm_clip']
                    )
                    
                    # è¿™é‡Œåº”è¯¥æ·»åŠ ä¼˜åŒ–å™¨æ­¥éª¤ï¼Œä½†ä¸ºäº†æµ‹è¯•å…ˆè·³è¿‡
                    print("   âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ")
                else:
                    print("   âš ï¸ æŸå¤±ä¸éœ€è¦æ¢¯åº¦ï¼Œè·³è¿‡æ›´æ–°")
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            if (step + 1) % config['training']['save_freq'] == 0:
                result_file = run_dir / f"step_{step+1}_results.json"
                results = {
                    'step': step + 1,
                    'task_name': task_name,
                    'num_episodes': len(episodes),
                    'success_rate': sum(1 for ep in episodes if sum(ep['rewards']) > 0.5) / len(episodes),
                    'avg_reward': np.mean([sum(ep['rewards']) for ep in episodes]),
                    'loss': loss.item() if len(episodes) > 0 else 0.0
                }
                
                import json
                with open(result_file, 'w') as f:
                    json.dump(results, f, indent=2)
                print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
        
        except Exception as e:
            print(f"âŒ æ­¥éª¤ {step + 1} å¤±è´¥: {e}")
            if args.debug:
                import traceback
                traceback.print_exc()
            continue
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {run_dir}")

if __name__ == "__main__":
    main()