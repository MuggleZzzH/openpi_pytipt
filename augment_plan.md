Based on our analysis, I'll provide a comprehensive modification plan that addresses your specifications. Let me start by verifying my understanding of the complete training pipeline.

# å®Œæ•´è®­ç»ƒæµæ°´çº¿ä¿®æ”¹è®¡åˆ’

## 1. æ¡†æ¶ç†è§£éªŒè¯

### 1.1 è®­ç»ƒæ¶æ„ç†è§£ç¡®è®¤

è®©æˆ‘é¦–å…ˆæ£€æŸ¥so100_train.pyçš„æ•°æ®å¤„ç†é€»è¾‘æ¥ç¡®ä¿ç†è§£æ­£ç¡®ï¼š

### 1.2 å®Œæ•´è®­ç»ƒæµæ°´çº¿ç†è§£

**æˆ‘çš„ç†è§£ç¡®è®¤**ï¼š

1. **è®­ç»ƒæ¶æ„**ï¼š
   - æ¯ä¸ªè®­ç»ƒæ­¥éª¤ï¼šå¤„ç† m ä¸ªdemosï¼ˆä¸åŒåˆå§‹çŠ¶æ€ï¼‰
   - æ¯ä¸ªdemoï¼šæ”¶é›† n æ¡RLOOè½¨è¿¹ï¼ˆç›¸åŒåˆå§‹çŠ¶æ€ï¼‰
   - æ€»è½¨è¿¹æ•°ï¼šE = m Ã— n

2. **æ•°æ®å¤„ç†**ï¼š
   - æ¯æ¡é•¿åº¦Lçš„è½¨è¿¹ â†’ L-50+1ä¸ªè®­ç»ƒæ ·æœ¬
   - æ ·æœ¬æ ¼å¼ï¼šobs[t] â†’ action[t:t+50]ï¼ˆ50æ­¥åŠ¨ä½œå—ï¼‰
   - ç›¸å¯¹åŠ¨ä½œï¼šaction - current_state
   - æ•°æ®åˆ©ç”¨ç‡æå‡ï¼š50-150å€

3. **ä¼˜åŠ¿è®¡ç®—**ï¼š
   - Episodeçº§åˆ«RLOOä¼˜åŠ¿è®¡ç®—ï¼ˆä¿æŒRIPTæ•°å­¦æ­£ç¡®æ€§ï¼‰
   - ä¼˜åŠ¿æ˜ å°„åˆ°æ ·æœ¬çº§åˆ«ï¼ˆç”¨äºCFGè®­ç»ƒï¼‰
   - äºŒå€¼åŒ–ä¼˜åŠ¿æƒé‡ï¼ˆCFGRL-RIPTèåˆï¼‰

**è¿™ä¸ªç†è§£æ˜¯å¦æ­£ç¡®ï¼Ÿ**

## 2. å®Œæ•´æ•°æ®æµç¨‹å›¾

## 3. è¯¦ç»†ä¿®æ”¹è®¡åˆ’

### 3.1 éœ€è¦ä¿®æ”¹çš„æ ¸å¿ƒæ–‡ä»¶

```
openpi_pytipt/
â”œâ”€â”€ 11_train_ript_vla_style.py                    # ä¸»è®­ç»ƒè„šæœ¬ - é‡å¤§ä¿®æ”¹
â”œâ”€â”€ pi0/ript/algos/rl_optimizers/
â”‚   â””â”€â”€ pi0_cfg_interface.py                      # CFGé€‚é…å™¨ - é‡å¤§ä¿®æ”¹
â”œâ”€â”€ pi0/ript/env/pi0_libero_runner.py             # ç¯å¢ƒè¿è¡Œå™¨ - ä¸­ç­‰ä¿®æ”¹
â”œâ”€â”€ pi0/ript/data/                                # æ–°å»ºç›®å½•
â”‚   â”œâ”€â”€ __init__.py                               # æ–°å»º
â”‚   â”œâ”€â”€ so100_style_processor.py                 # æ–°å»º - æ ¸å¿ƒæ•°æ®å¤„ç†
â”‚   â””â”€â”€ sample_generator.py                      # æ–°å»º - æ ·æœ¬ç”Ÿæˆå™¨
â””â”€â”€ pi0/ript/config/optimized_config.yaml        # æ–°å»ºé…ç½®æ–‡ä»¶
```

### 3.2 åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

#### **é˜¶æ®µ1: æ•°æ®å¤„ç†æ ¸å¿ƒé‡æ„ (2-3å¤©)**

**ç›®æ ‡**: å®ç°so100_train.pyé£æ ¼çš„æ•°æ®å¤„ç†

**1.1 åˆ›å»ºSO100é£æ ¼æ•°æ®å¤„ç†å™¨**

```python
# pi0/ript/data/so100_style_processor.py
class SO100StyleProcessor:
    """åŸºäºso100_train.pyçš„æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.action_chunk_size = 50  # å›ºå®š50æ­¥åŠ¨ä½œå—
        self.normalizer = self._create_normalizer()
    
    def process_trajectory_to_samples(self, trajectory):
        """å°†å•æ¡è½¨è¿¹è½¬æ¢ä¸ºå¤šä¸ªè®­ç»ƒæ ·æœ¬"""
        obs_sequence = trajectory['observations']
        action_sequence = trajectory['actions']
        state_sequence = trajectory['states']
        
        samples = []
        trajectory_length = len(obs_sequence)
        
        # ç”Ÿæˆ L-50+1 ä¸ªæ ·æœ¬
        for t in range(trajectory_length - self.action_chunk_size + 1):
            # å½“å‰è§‚æµ‹
            current_obs = obs_sequence[t]
            current_state = state_sequence[t]
            
            # 50æ­¥åŠ¨ä½œå—
            action_chunk = action_sequence[t:t + self.action_chunk_size]
            
            # è½¬æ¢ä¸ºç›¸å¯¹åŠ¨ä½œ (å…³é”®ï¼šéµå¾ªso100_train.pyé€»è¾‘)
            relative_actions = []
            for i, action in enumerate(action_chunk):
                # action - current_state (so100_train.pyç¬¬65è¡Œé€»è¾‘)
                relative_action = action - current_state
                relative_actions.append(relative_action)
            
            # æ„å»ºæ ·æœ¬
            sample = {
                'observation': current_obs,
                'state': current_state,
                'action': np.array(relative_actions),  # shape: (50, action_dim)
                'action_is_pad': self._create_padding_mask(len(action_chunk)),
                'trajectory_id': trajectory['id'],
                'sample_index': t
            }
            
            samples.append(sample)
        
        return samples
    
    def _create_padding_mask(self, actual_length):
        """åˆ›å»ºpaddingæ©ç """
        mask = np.zeros(self.action_chunk_size, dtype=bool)
        if actual_length < self.action_chunk_size:
            mask[actual_length:] = True  # Trueè¡¨ç¤ºpadding
        return mask
    
    def convert_to_openpi_format(self, sample):
        """è½¬æ¢ä¸ºOpenPIæœŸæœ›çš„æ ¼å¼"""
        # å®Œå…¨æŒ‰ç…§so100_train.pyçš„æ ¼å¼
        return {
            "image": {
                "base_0_rgb": sample['observation']['base_0_rgb'],
                "left_wrist_0_rgb": sample['observation']['left_wrist_0_rgb']
            },
            "state": sample['state'],
            "action": sample['action'],
            "action_is_pad": sample['action_is_pad'],
            "prompt": sample['observation'].get('prompt', [''])
        }
```

**1.2 åˆ›å»ºæ ·æœ¬ç”Ÿæˆå™¨**

```python
# pi0/ript/data/sample_generator.py
class TrajectoryToSampleGenerator:
    """è½¨è¿¹åˆ°æ ·æœ¬çš„ç”Ÿæˆå™¨"""
    
    def __init__(self, processor):
        self.processor = processor
    
    def generate_samples_from_episodes(self, episodes):
        """ä»episodesç”Ÿæˆæ‰€æœ‰è®­ç»ƒæ ·æœ¬"""
        all_samples = []
        episode_to_samples_map = {}  # ç”¨äºä¼˜åŠ¿æ˜ å°„
        
        for ep_idx, episode in enumerate(episodes):
            # ä¸ºæ¯ä¸ªepisodeç”Ÿæˆæ ·æœ¬
            episode_samples = self.processor.process_trajectory_to_samples(episode)
            
            # è®°å½•æ˜ å°„å…³ç³»
            start_idx = len(all_samples)
            end_idx = start_idx + len(episode_samples)
            episode_to_samples_map[ep_idx] = list(range(start_idx, end_idx))
            
            all_samples.extend(episode_samples)
        
        return all_samples, episode_to_samples_map
    
    def map_episode_advantages_to_samples(self, episode_advantages, episode_to_samples_map):
        """å°†episodeçº§åˆ«ä¼˜åŠ¿æ˜ å°„åˆ°æ ·æœ¬çº§åˆ«"""
        sample_advantages = []
        
        for ep_idx, advantage in enumerate(episode_advantages):
            sample_indices = episode_to_samples_map[ep_idx]
            
            # è¯¥episodeçš„æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒçš„ä¼˜åŠ¿å€¼
            for _ in sample_indices:
                sample_advantages.append(advantage)
        
        return torch.tensor(sample_advantages)
```

#### **é˜¶æ®µ2: CFGé€‚é…å™¨é‡æ„ (2-3å¤©)**

**ç›®æ ‡**: ä¿®æ”¹CFGé€‚é…å™¨ä»¥å¤„ç†æ ·æœ¬çº§åˆ«æ•°æ®

**2.1 ä¿®æ”¹PI0_CFG_Adapter**

```python
# pi0/ript/algos/rl_optimizers/pi0_cfg_interface.py (é‡å¤§ä¿®æ”¹)
class PI0_CFG_Adapter(RLModelInterface):
    def __init__(self, policy, config):
        super().__init__(model=policy)
        self.policy = policy
        self.config = config
        
        # ç§»é™¤çª—å£åŒ–ç›¸å…³ä»£ç 
        # self.windowing_mode = ...  # åˆ é™¤
        # self.window_stride = ...   # åˆ é™¤
        
        # æ–°å¢æ ·æœ¬å¤„ç†å™¨
        from pi0.ript.data.so100_style_processor import SO100StyleProcessor
        from pi0.ript.data.sample_generator import TrajectoryToSampleGenerator
        
        self.sample_processor = SO100StyleProcessor(config)
        self.sample_generator = TrajectoryToSampleGenerator(self.sample_processor)
    
    def process_episodes_to_samples(self, episodes, device):
        """æ–°æ–¹æ³•ï¼šå°†episodesè½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬"""
        # 1. ç”Ÿæˆæ‰€æœ‰æ ·æœ¬
        all_samples, episode_to_samples_map = self.sample_generator.generate_samples_from_episodes(episodes)
        
        # 2. è½¬æ¢ä¸ºOpenPIæ ¼å¼
        openpi_samples = []
        for sample in all_samples:
            openpi_sample = self.sample_processor.convert_to_openpi_format(sample)
            openpi_samples.append(openpi_sample)
        
        # 3. æ‰¹æ¬¡åŒ–å¤„ç†
        batch = self._collate_samples(openpi_samples, device)
        
        return batch, episode_to_samples_map
    
    def _collate_samples(self, samples, device):
        """æ•´ç†æ ·æœ¬ä¸ºæ‰¹æ¬¡"""
        batch_size = len(samples)
        
        # æå–å„ä¸ªå­—æ®µ
        images_base = torch.stack([s['image']['base_0_rgb'] for s in samples]).to(device)
        images_wrist = torch.stack([s['image']['left_wrist_0_rgb'] for s in samples]).to(device)
        states = torch.stack([torch.tensor(s['state']) for s in samples]).to(device)
        actions = torch.stack([torch.tensor(s['action']) for s in samples]).to(device)
        action_is_pad = torch.stack([torch.tensor(s['action_is_pad']) for s in samples]).to(device)
        
        return {
            'image': {
                'base_0_rgb': images_base,
                'left_wrist_0_rgb': images_wrist
            },
            'state': states,
            'action': actions,
            'action_is_pad': action_is_pad,
            'batch_size': batch_size
        }
    
    def compute_weighted_loss_samples(self, episodes, episode_advantages, device):
        """æ–°çš„æŸå¤±è®¡ç®—æ–¹æ³•ï¼šåŸºäºæ ·æœ¬è€Œéçª—å£"""
        # 1. è½¬æ¢ä¸ºæ ·æœ¬
        sample_batch, episode_to_samples_map = self.process_episodes_to_samples(episodes, device)
        
        # 2. æ˜ å°„ä¼˜åŠ¿åˆ°æ ·æœ¬çº§åˆ«
        sample_advantages = self.sample_generator.map_episode_advantages_to_samples(
            episode_advantages, episode_to_samples_map
        )
        
        # 3. äºŒå€¼åŒ–ä¼˜åŠ¿
        binary_advantages = (sample_advantages > 0).float().to(device)
        
        # 4. CFGæŸå¤±è®¡ç®—
        return self._compute_cfg_loss_for_samples(sample_batch, binary_advantages, device)
    
    def _compute_cfg_loss_for_samples(self, sample_batch, binary_advantages, device):
        """ä¸ºæ ·æœ¬è®¡ç®—CFGæŸå¤±"""
        batch_size = sample_batch['batch_size']
        
        # ç”Ÿæˆå…±äº«çš„noiseå’Œtime
        noise = torch.randn(batch_size, self.config.n_action_steps, self.config.max_action_dim, device=device)
        time = torch.rand(batch_size, device=device)
        
        # æ¡ä»¶åˆ†æ”¯
        pos_batch = sample_batch.copy()
        pos_batch['is_positive'] = torch.ones(batch_size, device=device, dtype=torch.long)
        pos_batch['noise'] = noise
        pos_batch['time'] = time
        
        pos_output = self.policy.forward(pos_batch)
        pos_losses = pos_output['losses'].mean(dim=-1)  # (B, T)
        
        # æ— æ¡ä»¶åˆ†æ”¯
        uncond_batch = sample_batch.copy()
        uncond_batch['is_positive'] = torch.zeros(batch_size, device=device, dtype=torch.long)
        uncond_batch['noise'] = noise  # å…±äº«ç›¸åŒnoise
        uncond_batch['time'] = time    # å…±äº«ç›¸åŒtime
        
        uncond_output = self.policy.forward(uncond_batch)
        uncond_losses = uncond_output['losses'].mean(dim=-1)  # (B, T)
        
        # CFGç»„åˆæŸå¤±
        cfg_weight = self.config.get('cfg_uncond_weight', 0.1)
        
        # å¤„ç†padding mask
        valid_mask = (~sample_batch['action_is_pad']).float()  # (B, T)
        
        # åº”ç”¨äºŒå€¼ä¼˜åŠ¿æƒé‡
        binary_advantages_expanded = binary_advantages.unsqueeze(1).expand_as(valid_mask)  # (B, T)
        
        combined_losses = binary_advantages_expanded * pos_losses + cfg_weight * uncond_losses
        
        # åªè®¡ç®—æœ‰æ•ˆæ­¥éª¤çš„æŸå¤±
        masked_losses = combined_losses * valid_mask
        valid_steps = valid_mask.sum(dim=1)  # (B,)
        
        # æ¯ä¸ªæ ·æœ¬çš„å¹³å‡æŸå¤±
        sample_losses = masked_losses.sum(dim=1) / (valid_steps + 1e-8)  # (B,)
        
        # æœ€ç»ˆæŸå¤±
        final_loss = sample_losses.mean()
        
        return final_loss
```

#### **é˜¶æ®µ3: ä¸»è®­ç»ƒè„šæœ¬é‡æ„ (2-3å¤©)**

**ç›®æ ‡**: ä¿®æ”¹ä¸»è®­ç»ƒå¾ªç¯ä»¥æ”¯æŒæ–°çš„æ•°æ®æµç¨‹

**3.1 ä¿®æ”¹ä¸»è®­ç»ƒè„šæœ¬**

```python
# 11_train_ript_vla_style.py (é‡å¤§ä¿®æ”¹)
def collect_rollouts_multi_demo(env_runner, config):
    """æ”¶é›†å¤šdemoæ•°æ®ï¼šmä¸ªdemosï¼Œæ¯ä¸ªdemo næ¡è½¨è¿¹"""
    m_demos = config['algo']['demos_per_step']  # ä¾‹å¦‚ï¼š4
    n_trajectories = config['algo']['rloo_batch_size']  # ä¾‹å¦‚ï¼š8
    
    all_episodes = []
    demo_info = []
    
    for demo_idx in range(m_demos):
        # é€‰æ‹©ä¸åŒçš„åˆå§‹çŠ¶æ€
        init_state = env_runner.sample_init_state(demo_idx)
        
        # ä¸ºè¿™ä¸ªåˆå§‹çŠ¶æ€æ”¶é›†næ¡è½¨è¿¹
        demo_episodes = env_runner.collect_trajectories_for_init_state(
            init_state, n_trajectories
        )
        
        # è®°å½•demoä¿¡æ¯
        demo_info.append({
            'demo_id': demo_idx,
            'init_state': init_state,
            'episode_count': len(demo_episodes),
            'episode_indices': list(range(len(all_episodes), len(all_episodes) + len(demo_episodes)))
        })
        
        all_episodes.extend(demo_episodes)
    
    print(f"âœ“ æ”¶é›†å®Œæˆ: {m_demos} demos Ã— {n_trajectories} trajectories = {len(all_episodes)} episodes")
    return all_episodes, demo_info

def compute_rloo_advantages_multi_demo(episodes, demo_info, rloo_batch_size):
    """è®¡ç®—å¤šdemoçš„RLOOä¼˜åŠ¿"""
    all_advantages = []
    
    for demo in demo_info:
        # æå–è¯¥demoçš„episodes
        demo_episodes = [episodes[i] for i in demo['episode_indices']]
        
        # ä¸ºè¯¥demoè®¡ç®—RLOOä¼˜åŠ¿
        demo_rewards = torch.tensor([ep['total_reward'] for ep in demo_episodes])
        
        # RLOOè®¡ç®— (ä¿æŒRIPTæ•°å­¦æ­£ç¡®æ€§)
        baseline = (demo_rewards.sum() - demo_rewards) / (rloo_batch_size - 1)
        demo_advantages = demo_rewards - baseline
        
        all_advantages.extend(demo_advantages.tolist())
    
    return torch.tensor(all_advantages)

def training_step_optimized(policy, optimizer, cfg_adapter, env_runner, config):
    """ä¼˜åŒ–åçš„è®­ç»ƒæ­¥éª¤"""
    
    # 1. æ”¶é›†å¤šdemoæ•°æ®
    episodes, demo_info = collect_rollouts_multi_demo(env_runner, config)
    
    # 2. è®¡ç®—RLOOä¼˜åŠ¿
    episode_advantages = compute_rloo_advantages_multi_demo(
        episodes, demo_info, config['algo']['rloo_batch_size']
    )
    
    # 3. ä½¿ç”¨æ–°çš„æ ·æœ¬çº§åˆ«CFGè®­ç»ƒ
    device = next(policy.parameters()).device
    loss = cfg_adapter.compute_weighted_loss_samples(episodes, episode_advantages, device)
    
    # 4. ä¼˜åŒ–å™¨æ›´æ–°
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
    optimizer.step()
    
    # 5. ç»Ÿè®¡ä¿¡æ¯
    total_samples = sum(len(ep['observations']) - 50 + 1 for ep in episodes if len(ep['observations']) >= 50)
    
    return {
        'loss': loss.item(),
        'episodes_collected': len(episodes),
        'samples_generated': total_samples,
        'data_utilization_ratio': total_samples / len(episodes)
    }
```

#### **é˜¶æ®µ4: ç¯å¢ƒè¿è¡Œå™¨é€‚é… (1-2å¤©)**

**ç›®æ ‡**: ä¿®æ”¹ç¯å¢ƒè¿è¡Œå™¨ä»¥æ”¯æŒå¤šdemoæ”¶é›†

**4.1 ä¿®æ”¹ç¯å¢ƒè¿è¡Œå™¨**

```python
# pi0/ript/env/pi0_libero_runner.py (ä¸­ç­‰ä¿®æ”¹)
class LIBEROEnvRunner:
    def sample_init_state(self, demo_idx):
        """ä¸ºdemoé€‰æ‹©åˆå§‹çŠ¶æ€"""
        if hasattr(self, 'init_state_pool'):
            # ç¡®ä¿ä¸åŒdemoä½¿ç”¨ä¸åŒåˆå§‹çŠ¶æ€
            return self.init_state_pool[demo_idx % len(self.init_state_pool)]
        else:
            # ç”Ÿæˆéšæœºåˆå§‹çŠ¶æ€
            return self._generate_random_init_state(demo_idx)
    
    def collect_trajectories_for_init_state(self, init_state, num_trajectories):
        """ä¸ºå•ä¸ªåˆå§‹çŠ¶æ€æ”¶é›†å¤šæ¡è½¨è¿¹"""
        episodes = []
        
        # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
        vec_env = self._create_vectorized_env(num_trajectories)
        
        # æ‰€æœ‰ç¯å¢ƒä½¿ç”¨ç›¸åŒåˆå§‹çŠ¶æ€
        observations = vec_env.reset(init_states=[init_state] * num_trajectories)
        
        # å¹¶è¡Œæ”¶é›†è½¨è¿¹
        for traj_idx in range(num_trajectories):
            episode = self._collect_single_trajectory(vec_env, traj_idx, init_state)
            episodes.append(episode)
        
        vec_env.close()
        return episodes
```

## 4. å…³é”®ä»£ç ç¤ºä¾‹å’Œå®ç°ç»†èŠ‚

### 4.1 æ ¸å¿ƒæ•°æ®è½¬æ¢é€»è¾‘

è®©æˆ‘æ£€æŸ¥so100_train.pyçš„å…·ä½“å®ç°ç»†èŠ‚ï¼š

### 4.2 å®Œæ•´çš„ç›¸å¯¹åŠ¨ä½œå¤„ç†å®ç°

```python
# pi0/ript/data/so100_style_processor.py (å®Œæ•´å®ç°)
class SO100StyleProcessor:
    """å®Œå…¨åŸºäºso100_train.pyç¬¬65è¡Œé€»è¾‘çš„æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.action_chunk_size = 50
        self.normalizer = self._create_normalizer()
        
    def _create_normalizer(self):
        """åˆ›å»ºä¸so100_train.pyä¸€è‡´çš„å½’ä¸€åŒ–å™¨"""
        from lerobot.common.datasets.normalize import Normalizer
        
        normalizer = Normalizer(
            config={
                "observation.state": {"mean": self.config.state_mean, "std": self.config.state_std},
                "action": {"mean": self.config.action_mean, "std": self.config.action_std},
            },
            mode="mean_std"
        )
        return normalizer
    
    def process_trajectory_to_samples(self, trajectory):
        """å°†è½¨è¿¹è½¬æ¢ä¸ºæ ·æœ¬ï¼Œå®Œå…¨éµå¾ªso100_train.pyé€»è¾‘"""
        obs_sequence = trajectory['processed_observations']  # å·²é¢„å¤„ç†çš„è§‚æµ‹
        action_sequence = trajectory['actions']              # åŸå§‹åŠ¨ä½œ
        state_sequence = trajectory['states']                # çŠ¶æ€åºåˆ—
        
        samples = []
        trajectory_length = len(obs_sequence)
        
        # ç¡®ä¿è½¨è¿¹é•¿åº¦è¶³å¤Ÿ
        if trajectory_length < self.action_chunk_size:
            return samples  # è·³è¿‡å¤ªçŸ­çš„è½¨è¿¹
        
        # ç”Ÿæˆ L-50+1 ä¸ªæ ·æœ¬
        for t in range(trajectory_length - self.action_chunk_size + 1):
            # å½“å‰æ—¶åˆ»çš„è§‚æµ‹å’ŒçŠ¶æ€
            current_obs = obs_sequence[t]
            current_state = state_sequence[t]  # shape: (state_dim,)
            
            # æå–50æ­¥åŠ¨ä½œå—
            action_chunk = action_sequence[t:t + self.action_chunk_size]  # shape: (50, action_dim)
            
            # å…³é”®ï¼šè®¡ç®—ç›¸å¯¹åŠ¨ä½œ (so100_train.pyç¬¬65è¡Œ)
            # item["action"] = item["action"] - item["observation.state"]
            relative_actions = action_chunk - current_state[None, :]  # å¹¿æ’­å‡æ³•
            
            # åˆ›å»ºpadding mask
            actual_length = len(action_chunk)
            action_is_pad = np.zeros(self.action_chunk_size, dtype=bool)
            if actual_length < self.action_chunk_size:
                # å¦‚æœä¸è¶³50æ­¥ï¼Œè¿›è¡Œpadding
                padded_actions = np.zeros((self.action_chunk_size, action_chunk.shape[1]))
                padded_actions[:actual_length] = relative_actions
                relative_actions = padded_actions
                action_is_pad[actual_length:] = True
            
            # æ„å»ºæ ·æœ¬ (ä¸so100_train.pyæ ¼å¼å®Œå…¨ä¸€è‡´)
            sample = {
                'observation': {
                    'images': {
                        'base': current_obs['image']['base_0_rgb'],
                        'wrist': current_obs['image']['left_wrist_0_rgb']
                    },
                    'state': current_state
                },
                'action': relative_actions,
                'action_is_pad': action_is_pad,
                'trajectory_id': trajectory['id'],
                'sample_index': t
            }
            
            samples.append(sample)
        
        return samples
    
    def normalize_sample(self, sample):
        """å½’ä¸€åŒ–æ ·æœ¬ï¼Œéµå¾ªso100_train.pyç¬¬66è¡Œé€»è¾‘"""
        # æ„å»ºå½’ä¸€åŒ–è¾“å…¥
        norm_input = {
            'observation.state': sample['observation']['state'][None, :],  # æ·»åŠ batchç»´åº¦
            'action': sample['action'],
            'action_is_pad': sample['action_is_pad']
        }
        
        # åº”ç”¨å½’ä¸€åŒ–
        normalized = self.normalizer.normalize(norm_input)
        
        # å¤„ç†å›¾åƒ (so100_train.pyç¬¬67-70è¡Œ)
        base_image = (sample['observation']['images']['base'] * 255).to(torch.uint8)
        wrist_image = (sample['observation']['images']['wrist'] * 255).to(torch.uint8)
        
        # è¿”å›so100_train.pyç¬¬71-75è¡Œçš„æ ¼å¼
        return {
            "image": {
                "base_0_rgb": base_image,
                "left_wrist_0_rgb": wrist_image
            },
            "state": normalized["observation.state"][0],  # ç§»é™¤batchç»´åº¦
            "action": normalized["action"],
            "action_is_pad": normalized["action_is_pad"],
        }
```

### 4.3 RIPTæ•°å­¦æ­£ç¡®æ€§ä¿è¯

```python
# pi0/ript/algos/rl_optimizers/pi0_cfg_interface.py (å…³é”®ä¿®æ”¹)
class RLOOAdvantageComputer:
    """ç¡®ä¿RIPTæ•°å­¦æ­£ç¡®æ€§çš„ä¼˜åŠ¿è®¡ç®—å™¨"""
    
    def compute_multi_demo_advantages(self, episodes, demo_info, rloo_batch_size):
        """å¤šdemo RLOOä¼˜åŠ¿è®¡ç®—ï¼Œä¿æŒæ•°å­¦æ­£ç¡®æ€§"""
        all_advantages = []
        
        for demo in demo_info:
            # æå–è¯¥demoçš„episodes
            demo_episodes = [episodes[i] for i in demo['episode_indices']]
            
            # æå–å¥–åŠ±
            demo_rewards = torch.tensor([ep['total_reward'] for ep in demo_episodes], dtype=torch.float32)
            
            # RLOOè®¡ç®— (ä¸åŸå§‹RIPTå®Œå…¨ä¸€è‡´)
            # baseline = (sum - current) / (n - 1)
            total_reward = demo_rewards.sum()
            baseline = (total_reward - demo_rewards) / (rloo_batch_size - 1)
            
            # advantage = reward - baseline
            demo_advantages = demo_rewards - baseline
            
            all_advantages.extend(demo_advantages.tolist())
            
            # éªŒè¯æ•°å­¦æ­£ç¡®æ€§
            assert len(demo_advantages) == rloo_batch_size, f"Demo {demo['demo_id']}: expected {rloo_batch_size} advantages, got {len(demo_advantages)}"
        
        return torch.tensor(all_advantages, dtype=torch.float32)
    
    def map_episode_advantages_to_samples(self, episode_advantages, episode_to_samples_map):
        """å°†episodeä¼˜åŠ¿æ˜ å°„åˆ°æ ·æœ¬ï¼Œä¿æŒä¸€è‡´æ€§"""
        sample_advantages = []
        
        for ep_idx, advantage in enumerate(episode_advantages):
            if ep_idx in episode_to_samples_map:
                sample_indices = episode_to_samples_map[ep_idx]
                
                # è¯¥episodeçš„æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒçš„ä¼˜åŠ¿å€¼
                for _ in sample_indices:
                    sample_advantages.append(advantage.item())
        
        return torch.tensor(sample_advantages, dtype=torch.float32)
```

### 4.4 CFGè®­ç»ƒçš„å®Œæ•´å®ç°

```python
# pi0/ript/algos/rl_optimizers/pi0_cfg_interface.py (CFGè®­ç»ƒéƒ¨åˆ†)
def compute_cfg_loss_with_samples(self, sample_batch, sample_advantages, device):
    """åŸºäºæ ·æœ¬çš„CFGæŸå¤±è®¡ç®—"""
    batch_size = sample_batch['batch_size']
    
    # äºŒå€¼åŒ–ä¼˜åŠ¿
    binary_advantages = (sample_advantages > 0).float().to(device)
    
    # ç”Ÿæˆå…±äº«çš„noiseå’Œtime (å…³é”®ï¼šç¡®ä¿æ¡ä»¶å’Œæ— æ¡ä»¶åˆ†æ”¯ä½¿ç”¨ç›¸åŒçš„éšæœºæ€§)
    noise_shape = (batch_size, self.action_chunk_size, self.config.action_dim)
    shared_noise = torch.randn(noise_shape, device=device, dtype=torch.float32)
    shared_time = torch.rand(batch_size, device=device, dtype=torch.float32)
    
    # æ¡ä»¶åˆ†æ”¯ (is_positive=1)
    pos_batch = {
        'image': sample_batch['image'],
        'state': sample_batch['state'],
        'action': sample_batch['action'],
        'action_is_pad': sample_batch['action_is_pad'],
        'is_positive': torch.ones(batch_size, device=device, dtype=torch.long),
        'noise': shared_noise,
        'time': shared_time
    }
    
    pos_output = self.policy.forward(pos_batch)
    pos_losses = pos_output['losses']  # shape: (B, T)
    
    # æ— æ¡ä»¶åˆ†æ”¯ (is_positive=0)
    uncond_batch = {
        'image': sample_batch['image'],
        'state': sample_batch['state'],
        'action': sample_batch['action'],
        'action_is_pad': sample_batch['action_is_pad'],
        'is_positive': torch.zeros(batch_size, device=device, dtype=torch.long),
        'noise': shared_noise,  # å…±äº«ç›¸åŒçš„noise
        'time': shared_time     # å…±äº«ç›¸åŒçš„time
    }
    
    uncond_output = self.policy.forward(uncond_batch)
    uncond_losses = uncond_output['losses']  # shape: (B, T)
    
    # CFGç»„åˆæŸå¤±
    cfg_weight = self.config.get('cfg_uncond_weight', 0.1)
    
    # å¤„ç†padding mask
    valid_mask = (~sample_batch['action_is_pad']).float()  # shape: (B, T)
    
    # åº”ç”¨äºŒå€¼ä¼˜åŠ¿æƒé‡
    binary_advantages_expanded = binary_advantages.unsqueeze(1)  # shape: (B, 1)
    
    # CFGæŸå¤±å…¬å¼: w_pos * L_pos + w_uncond * L_uncond
    combined_losses = binary_advantages_expanded * pos_losses + cfg_weight * uncond_losses
    
    # åªè®¡ç®—æœ‰æ•ˆæ­¥éª¤çš„æŸå¤±
    masked_losses = combined_losses * valid_mask
    valid_steps = valid_mask.sum(dim=1)  # shape: (B,)
    
    # æ¯ä¸ªæ ·æœ¬çš„å¹³å‡æŸå¤±
    sample_losses = masked_losses.sum(dim=1) / (valid_steps + 1e-8)  # shape: (B,)
    
    # æœ€ç»ˆæŸå¤±
    final_loss = sample_losses.mean()
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'positive_samples': (binary_advantages > 0).sum().item(),
        'negative_samples': (binary_advantages <= 0).sum().item(),
        'avg_valid_steps': valid_steps.mean().item(),
        'pos_loss_mean': pos_losses.mean().item(),
        'uncond_loss_mean': uncond_losses.mean().item()
    }
    
    return final_loss, stats
```

### 4.5 æ•°æ®åˆ©ç”¨ç‡éªŒè¯

```python
# éªŒè¯æ•°æ®åˆ©ç”¨ç‡æå‡
def calculate_data_utilization_improvement(episodes, action_chunk_size=50):
    """è®¡ç®—æ•°æ®åˆ©ç”¨ç‡æå‡"""
    
    # å½“å‰çª—å£åŒ–æ–¹æ³•çš„æ ·æœ¬æ•° (å‡è®¾)
    current_samples = len(episodes)  # æ¯ä¸ªepisodeåªäº§ç”Ÿ1ä¸ªæ ·æœ¬
    
    # æ–°æ–¹æ³•çš„æ ·æœ¬æ•°
    new_samples = 0
    for episode in episodes:
        episode_length = len(episode['observations'])
        if episode_length >= action_chunk_size:
            new_samples += episode_length - action_chunk_size + 1
    
    # è®¡ç®—æå‡å€æ•°
    improvement_ratio = new_samples / current_samples if current_samples > 0 else 0
    
    print(f"æ•°æ®åˆ©ç”¨ç‡åˆ†æ:")
    print(f"  å½“å‰æ–¹æ³•æ ·æœ¬æ•°: {current_samples}")
    print(f"  æ–°æ–¹æ³•æ ·æœ¬æ•°: {new_samples}")
    print(f"  æå‡å€æ•°: {improvement_ratio:.1f}x")
    
    return improvement_ratio
```

## 5. å®æ–½éªŒè¯å’Œå…¼å®¹æ€§ä¿è¯

### 5.1 åˆ†æ­¥éªŒè¯ç­–ç•¥

```python
# scripts/validate_modifications.py
class ModificationValidator:
    """ä¿®æ”¹éªŒè¯å™¨"""
    
    def validate_data_processing_compatibility(self):
        """éªŒè¯æ•°æ®å¤„ç†ä¸so100_train.pyçš„å…¼å®¹æ€§"""
        
        # 1. åŠ è½½æµ‹è¯•è½¨è¿¹
        test_trajectory = self._create_test_trajectory()
        
        # 2. ä½¿ç”¨æ–°å¤„ç†å™¨å¤„ç†
        processor = SO100StyleProcessor(self.config)
        samples = processor.process_trajectory_to_samples(test_trajectory)
        
        # 3. éªŒè¯æ ·æœ¬æ ¼å¼
        for sample in samples[:5]:  # æ£€æŸ¥å‰5ä¸ªæ ·æœ¬
            # éªŒè¯ç›¸å¯¹åŠ¨ä½œè®¡ç®—
            assert 'action' in sample
            assert sample['action'].shape[0] == 50  # 50æ­¥åŠ¨ä½œå—
            
            # éªŒè¯padding mask
            assert 'action_is_pad' in sample
            assert sample['action_is_pad'].shape[0] == 50
            
            # éªŒè¯è§‚æµ‹æ ¼å¼
            assert 'observation' in sample
            assert 'images' in sample['observation']
            
        print(f"âœ… æ•°æ®å¤„ç†éªŒè¯é€šè¿‡: ç”Ÿæˆäº† {len(samples)} ä¸ªæ ·æœ¬")
    
    def validate_rloo_mathematics(self):
        """éªŒè¯RLOOæ•°å­¦æ­£ç¡®æ€§"""
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_rewards = torch.tensor([0.8, 0.2, 0.9, 0.1, 0.7, 0.3, 0.6, 0.4])
        rloo_batch_size = 8
        
        # è®¡ç®—RLOOä¼˜åŠ¿
        total_reward = test_rewards.sum()
        baseline = (total_reward - test_rewards) / (rloo_batch_size - 1)
        advantages = test_rewards - baseline
        
        # éªŒè¯æ•°å­¦æ€§è´¨
        assert abs(advantages.sum().item()) < 1e-6, "RLOOä¼˜åŠ¿å’Œåº”è¯¥æ¥è¿‘0"
        assert len(advantages) == rloo_batch_size, "ä¼˜åŠ¿æ•°é‡åº”è¯¥ç­‰äºæ‰¹æ¬¡å¤§å°"
        
        print("âœ… RLOOæ•°å­¦éªŒè¯é€šè¿‡")
    
    def validate_cfg_loss_computation(self):
        """éªŒè¯CFGæŸå¤±è®¡ç®—"""
        
        # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
        batch_size = 4
        test_batch = self._create_test_batch(batch_size)
        test_advantages = torch.tensor([1.0, -0.5, 0.8, -0.2])
        
        # è®¡ç®—CFGæŸå¤±
        cfg_adapter = PI0_CFG_Adapter(self.policy, self.config)
        loss, stats = cfg_adapter.compute_cfg_loss_with_samples(
            test_batch, test_advantages, self.device
        )
        
        # éªŒè¯æŸå¤±åˆç†æ€§
        assert torch.isfinite(loss), "æŸå¤±åº”è¯¥æ˜¯æœ‰é™çš„"
        assert loss.item() > 0, "æŸå¤±åº”è¯¥æ˜¯æ­£æ•°"
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        assert stats['positive_samples'] + stats['negative_samples'] == batch_size
        
        print("âœ… CFGæŸå¤±è®¡ç®—éªŒè¯é€šè¿‡")
```

### 5.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
# scripts/benchmark_performance.py
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def benchmark_data_utilization(self):
        """åŸºå‡†æµ‹è¯•æ•°æ®åˆ©ç”¨ç‡"""
        
        # æ¨¡æ‹Ÿæ”¶é›†episodes
        episodes = self._simulate_episode_collection(num_episodes=32, avg_length=150)
        
        # è®¡ç®—å½“å‰æ–¹æ³•çš„æ ·æœ¬æ•°
        current_samples = len(episodes)
        
        # è®¡ç®—æ–°æ–¹æ³•çš„æ ·æœ¬æ•°
        processor = SO100StyleProcessor(self.config)
        total_new_samples = 0
        
        for episode in episodes:
            samples = processor.process_trajectory_to_samples(episode)
            total_new_samples += len(samples)
        
        # è®¡ç®—æå‡
        improvement = total_new_samples / current_samples
        
        print(f"æ•°æ®åˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•:")
        print(f"  Episodes: {len(episodes)}")
        print(f"  å¹³å‡é•¿åº¦: {sum(len(ep['observations']) for ep in episodes) / len(episodes):.1f}")
        print(f"  å½“å‰æ ·æœ¬æ•°: {current_samples}")
        print(f"  æ–°æ ·æœ¬æ•°: {total_new_samples}")
        print(f"  æå‡å€æ•°: {improvement:.1f}x")
        
        return improvement
    
    def benchmark_training_speed(self):
        """åŸºå‡†æµ‹è¯•è®­ç»ƒé€Ÿåº¦"""
        import time
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        episodes = self._simulate_episode_collection(num_episodes=16, avg_length=120)
        advantages = torch.randn(len(episodes))
        
        # æµ‹è¯•å½“å‰æ–¹æ³•
        start_time = time.time()
        # current_loss = self._compute_current_method_loss(episodes, advantages)
        current_time = time.time() - start_time
        
        # æµ‹è¯•æ–°æ–¹æ³•
        start_time = time.time()
        cfg_adapter = PI0_CFG_Adapter(self.policy, self.config)
        new_loss = cfg_adapter.compute_weighted_loss_samples(episodes, advantages, self.device)
        new_time = time.time() - start_time
        
        print(f"è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•:")
        print(f"  å½“å‰æ–¹æ³•æ—¶é—´: {current_time:.3f}s")
        print(f"  æ–°æ–¹æ³•æ—¶é—´: {new_time:.3f}s")
        print(f"  é€Ÿåº¦æ¯”: {current_time / new_time:.2f}x")
        
        return new_time / current_time
```

### 5.3 å®Œæ•´å®æ–½æ£€æŸ¥æ¸…å•

```markdown
## å®æ–½æ£€æŸ¥æ¸…å•

### é˜¶æ®µ1: æ•°æ®å¤„ç†é‡æ„
- [ ] åˆ›å»º `pi0/ript/data/` ç›®å½•
- [ ] å®ç° `SO100StyleProcessor` ç±»
- [ ] å®ç° `TrajectoryToSampleGenerator` ç±»
- [ ] éªŒè¯ç›¸å¯¹åŠ¨ä½œè®¡ç®—æ­£ç¡®æ€§
- [ ] éªŒè¯æ ·æœ¬æ ¼å¼ä¸so100_train.pyä¸€è‡´
- [ ] æµ‹è¯•æ•°æ®åˆ©ç”¨ç‡æå‡ (ç›®æ ‡: 50-150x)

### é˜¶æ®µ2: CFGé€‚é…å™¨ä¿®æ”¹
- [ ] ä¿®æ”¹ `PI0_CFG_Adapter` ç±»
- [ ] ç§»é™¤çª—å£åŒ–ç›¸å…³ä»£ç 
- [ ] å®ç°æ ·æœ¬çº§åˆ«CFGæŸå¤±è®¡ç®—
- [ ] éªŒè¯äºŒå€¼ä¼˜åŠ¿æ˜ å°„æ­£ç¡®æ€§
- [ ] æµ‹è¯•CFGæŸå¤±è®¡ç®—ç¨³å®šæ€§

### é˜¶æ®µ3: ä¸»è®­ç»ƒè„šæœ¬é‡æ„
- [ ] ä¿®æ”¹ `11_train_ript_vla_style.py`
- [ ] å®ç°å¤šdemoæ”¶é›†é€»è¾‘
- [ ] å®ç°RLOOä¼˜åŠ¿è®¡ç®—
- [ ] é›†æˆæ–°çš„CFGè®­ç»ƒæµç¨‹
- [ ] éªŒè¯ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹

### é˜¶æ®µ4: ç¯å¢ƒè¿è¡Œå™¨é€‚é…
- [ ] ä¿®æ”¹ `pi0_libero_runner.py`
- [ ] å®ç°å¤šè½¨è¿¹æ”¶é›†åŠŸèƒ½
- [ ] ä¼˜åŒ–å¹¶è¡Œç¯å¢ƒç®¡ç†
- [ ] æµ‹è¯•ç¯å¢ƒç¨³å®šæ€§

### éªŒè¯å’Œæµ‹è¯•
- [ ] æ•°æ®å¤„ç†å…¼å®¹æ€§æµ‹è¯•
- [ ] RLOOæ•°å­¦æ­£ç¡®æ€§éªŒè¯
- [ ] CFGæŸå¤±è®¡ç®—éªŒè¯
- [ ] ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] å†…å­˜ä½¿ç”¨åˆ†æ

### é…ç½®å’Œæ–‡æ¡£
- [ ] åˆ›å»ºä¼˜åŒ–é…ç½®æ–‡ä»¶
- [ ] æ›´æ–°å‚æ•°è¯´æ˜æ–‡æ¡£
- [ ] æ·»åŠ ä½¿ç”¨ç¤ºä¾‹
- [ ] åˆ›å»ºæ•…éšœæ’é™¤æŒ‡å—
```

## æ€»ç»“

### âœ… **æ¡†æ¶ç†è§£ç¡®è®¤**

æˆ‘å¯¹æ‚¨çš„è®­ç»ƒæµæ°´çº¿æœ‰å®Œæ•´æ­£ç¡®çš„ç†è§£ï¼š
- **è®­ç»ƒæ¶æ„**: m demos Ã— n trajectories = E total trajectories
- **æ•°æ®å¤„ç†**: éµå¾ªso100_train.pyï¼ŒL-50+1æ ·æœ¬ï¼Œç›¸å¯¹åŠ¨ä½œ
- **ä¼˜åŠ¿è®¡ç®—**: Episodeçº§åˆ«RLOO â†’ æ ·æœ¬çº§åˆ«æ˜ å°„
- **CFGè®­ç»ƒ**: äºŒå€¼ä¼˜åŠ¿æƒé‡ï¼Œæ¡ä»¶/æ— æ¡ä»¶åˆ†æ”¯

### ğŸ¯ **å…³é”®åˆ›æ–°ç‚¹**

1. **æ•°æ®åˆ©ç”¨ç‡æœ€å¤§åŒ–**: 50-150å€æå‡
2. **RIPTæ•°å­¦æ­£ç¡®æ€§**: ä¿æŒepisodeçº§åˆ«ä¼˜åŠ¿è®¡ç®—
3. **OpenPIå®Œå…¨å…¼å®¹**: ä¸¥æ ¼éµå¾ªso100_train.pyæ ¼å¼
4. **CFGRL-RIPTèåˆ**: ç†è®ºåˆç†çš„åˆ›æ–°ç»“åˆ

### ğŸš€ **å®æ–½å°±ç»ª**

æä¾›äº†å®Œæ•´çš„ï¼š
- è¯¦ç»†ä¿®æ”¹è®¡åˆ’ (4ä¸ªé˜¶æ®µ)
- å…³é”®ä»£ç å®ç° (æ ¸å¿ƒç±»å’Œå‡½æ•°)
- éªŒè¯ç­–ç•¥ (æ•°å­¦æ­£ç¡®æ€§ã€å…¼å®¹æ€§ã€æ€§èƒ½)
- å®æ–½æ£€æŸ¥æ¸…å• (ç¡®ä¿æ— é—æ¼)

**æˆ‘ä»¬ç°åœ¨å¯ä»¥å¼€å§‹å…·ä½“çš„ä»£ç ä¿®æ”¹å®æ–½ï¼** æ‚¨å¸Œæœ›ä»å“ªä¸ªé˜¶æ®µå¼€å§‹ï¼Œè¿˜æ˜¯éœ€è¦æˆ‘è¿›ä¸€æ­¥æ¾„æ¸…ä»»ä½•æŠ€æœ¯ç»†èŠ‚ï¼Ÿ
